{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Bayesian Optimization\n",
    "# Optimizing the SGD Learning Rate when Training a Neural Network\n",
    "\n",
    "### Roger Garrida\n",
    "### Akhil Lohia\n",
    "### Daniel Velasquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "\n",
    "Training a neural network can be a difficult task. In particular, due to the large number of hyperparameters that need to be tunned, e.g. number of layers, number of hidden units, batch size among other. In this project, we focus on one particular hyperparameter that influence directly the success of the learning procedure: The stochastic gradient descent **learning rate **. We use bayesian optimization to tune the learning rate and compare the results with a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import pandas as pd\n",
    "import math as mat\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm #add progress bar to for loops\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1,x2,noise,length): #Generate the kernel (cov) of the Gaussian Process\n",
    "    n1 = x1.shape[0]\n",
    "    n2 = x2.shape[0]\n",
    "    kernel = np.zeros((n1,n2))\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            kernel[i,j] = noise**2*mat.exp(-0.5*((x1[i]-x2[j])/length)**2)\n",
    "    return kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LLH_GP(x,y,m,noise,length, sf = 0): #Compute the likelihood of the data (add sf if consider noise)\n",
    "    ker = gaussian_kernel(x,x, noise, length)\n",
    "    ker = ker+np.diag([sf]*len(x))\n",
    "    return 1/2*(mat.log(np.linalg.det(ker))+np.dot(np.dot(np.transpose(y-m),\n",
    "                                                       np.linalg.inv(ker)),(y-m)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_hyparams(x,y): #Find the hyperparameters that optimize LLH without noise\n",
    "    ini = np.array([0,1,1])\n",
    "    opt = optimize.minimize(lambda params: LLH_GP(x, y, params[0], params[1], params[2]),\n",
    "                            ini)\n",
    "    params = opt.x\n",
    "    m = params[0]\n",
    "    noise = abs(params[1])\n",
    "    length = abs(params[2])\n",
    "    sf = 0.001\n",
    "    return m, noise, length, sf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_hyparams_noise(x,y): #Find the hyperparameters that optimize LLH with noise\n",
    "    ini = np.array([0,1,1,1])\n",
    "    opt = optimize.minimize(lambda params: LLH_GP(x, y, params[0], \n",
    "                                                  params[1], params[2], params[3]),ini)\n",
    "    params = opt.x\n",
    "    m = params[0]\n",
    "    noise = abs(params[1])\n",
    "    length = abs(params[2])\n",
    "    sf = abs(params[3])\n",
    "    return m, noise, length, sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generates the mean and covariance of the posterior distribution in the given grid (xn) \n",
    "#from data (x,y) and the optimized parameters (mean->m, noise, length, noise in y -> sf)\n",
    "def gp_posterior(x, y, xn, m, noise, length, sf = 0): \n",
    "    kxx = gaussian_kernel(x, x, noise = noise, length = length)\n",
    "    kxxn = gaussian_kernel(x, xn, noise = noise, length = length)\n",
    "    kxnx = gaussian_kernel(xn, x, noise = noise, length = length)\n",
    "    kxnxn = gaussian_kernel(xn, xn, noise = noise, length = length)\n",
    "    core = np.linalg.inv(kxx + np.diag([sf]*len(x)))\n",
    "    En = np.dot(np.dot(kxnx, core), y)\n",
    "    covn = kxnxn - np.dot(np.dot(kxnx, core), kxxn)  \n",
    "    \n",
    "    return En, covn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_posterior(x, E, cov):\n",
    "    data = pd.DataFrame({'x': x})\n",
    "    data['Mean'] = E\n",
    "    data['StdDev'] = np.sqrt(np.diag(cov))\n",
    "    #Generate the 5 samples as multivariate normals with 0 mean and covariance sigma\n",
    "    for i in range(5):\n",
    "        data['y'+str(i)] = np.random.multivariate_normal(E, cov)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network Architecture and Dataset\n",
    "\n",
    "Our goal is to perform classification on the MNIST dataset. To do so, we build a neural network with one hidden layer and a fixed number of hidden units. We divide the dataset in training and test sample. We define a function that trains the network as a function of the learning rate and returns a measure of accuracy estimated using the test sample. The accuracy corresponds the number of correctly classified observations divided by the total number of observations within the test sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#MNIST dataset:\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, validation_size=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder:\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "##Variables:\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    #initial = tf.ones(shape)/10\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def neural_network(input, h_dim):\n",
    "    W0 = weight_variable([784, h_dim])\n",
    "    b0 = bias_variable([h_dim])\n",
    "    h = tf.nn.relu(tf.matmul(input, W0) + b0)\n",
    "\n",
    "    W = weight_variable([h_dim, 10])\n",
    "    b = bias_variable([10])\n",
    "\n",
    "    y = tf.nn.softmax(tf.matmul(h, W) + b)\n",
    "    return y\n",
    "\n",
    "#Network training:\n",
    "def nn_train(learning_rate, h_dim, minibatch = 100):\n",
    "    y = neural_network(x, h_dim)\n",
    "    #cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))*minibatch\n",
    "    #train_step = tf.train.MomentumOptimizer(learning_rate, 0.5).minimize(cross_entropy)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    #n_samples = len(mnist.train.images)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(10000+1):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(minibatch) #These variables are numpy arrays\n",
    "        \n",
    "        if (i%20 == 0): #Training accuracy update\n",
    "            a, c = sess.run([accuracy, cross_entropy], {x: batch_xs, y_: batch_ys})\n",
    "            print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "        \n",
    "        if (i%100 == 0): #Test accuracy update\n",
    "            a, c = sess.run([accuracy, cross_entropy], {x: mnist.test.images, y_: mnist.test.labels})\n",
    "            print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "\n",
    "    return(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "\n",
    "# Network training with variable learning rate\n",
    "\n",
    "def nn_train_var(min_lr, max_lr, h_dim, minibatch = 100):\n",
    "    \n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    \n",
    "    y = neural_network(x, h_dim)\n",
    "    #cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))*minibatch\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # training step, the learning rate is a placeholder\n",
    "    train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "    #train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(10000+1):\n",
    "        # learning rate decay\n",
    "        decay_speed = 2000.0\n",
    "        learning_rate = min_lr + (max_lr - min_lr) * math.exp(-i/decay_speed)\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(minibatch) #These variables are numpy arrays\n",
    "        \n",
    "        if (i%20 == 0):\n",
    "            a, c = sess.run([accuracy, cross_entropy], {x: batch_xs, y_: batch_ys})\n",
    "            print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "        \n",
    "        if (i%100 == 0):\n",
    "            a, c = sess.run([accuracy, cross_entropy], {x: mnist.test.images, y_: mnist.test.labels})\n",
    "            print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, lr:learning_rate})\n",
    "\n",
    "    return(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Adquisition Function\n",
    "\n",
    "We start with a gaussian prior on the hyperparameters. We define an adquisicion function that allows us to select a new learning rate to test. In particular we use *Expected Improvement*. With each new value, we train the network and use the output (i.e. the classification accuracy) to update the prior density. Notice that in this case, we want maximize the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adquisition function\n",
    "\n",
    "def acquisition_fun(x, y, xn, mean_vector, sigma_vector):\n",
    "    x_best = x[np.argmax(y)]\n",
    "    y_best = np.max(y)\n",
    "    ind_cand = np.array([not any(abs(xi-x)<=0.00001) for xi in xn]) #Indicator of candidates\n",
    "    gamma = (mean_vector[ind_cand] - y_best)/sigma_vector[ind_cand]\n",
    "    af = (mean_vector[ind_cand] - y_best)* norm.cdf(gamma) + sigma_vector[ind_cand]*norm.pdf(gamma)\n",
    "\n",
    "    x_next = xn[ind_cand][np.argmax(af)]\n",
    "    return x_next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algorithm\n",
    "\n",
    "\n",
    "Initially, we assume we only observe 2 potential learning rates and their corresponding accuracy. We implement an algorithm that, given some prior on the learning rate, at each iteration the acquisition function selects a new candidate learning rate, then trains the network and estimates the posterior density. The plots below shows inital observations, the mean and 1 standard deviation around the mean. \n",
    "\n",
    "When implementing the algorith we can assume that the observed accuracies contains noise or not. Given that in practice the classification accuracy of the neural network is not a deterministic function of the learning rate, we assume the observe values contain some noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.13 loss: 229.158 (lr:0.1)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1508 test loss: 229.574\n",
      "20: accuracy:0.08 loss: 238.114 (lr:0.1)\n",
      "40: accuracy:0.08 loss: 237.664 (lr:0.1)\n",
      "60: accuracy:0.46 loss: 200.742 (lr:0.1)\n",
      "80: accuracy:0.51 loss: 194.898 (lr:0.1)\n",
      "100: accuracy:0.55 loss: 191.714 (lr:0.1)\n",
      "100: ********* epoch 1 ********* test accuracy:0.5656 test loss: 188.776\n",
      "120: accuracy:0.71 loss: 175.5 (lr:0.1)\n",
      "140: accuracy:0.79 loss: 166.136 (lr:0.1)\n",
      "160: accuracy:0.83 loss: 164.376 (lr:0.1)\n",
      "180: accuracy:0.84 loss: 162.823 (lr:0.1)\n",
      "200: accuracy:0.8 loss: 164.411 (lr:0.1)\n",
      "200: ********* epoch 1 ********* test accuracy:0.8333 test loss: 162.753\n",
      "220: accuracy:0.85 loss: 160.31 (lr:0.1)\n",
      "240: accuracy:0.85 loss: 161.085 (lr:0.1)\n",
      "260: accuracy:0.9 loss: 156.509 (lr:0.1)\n",
      "280: accuracy:0.84 loss: 161.963 (lr:0.1)\n",
      "300: accuracy:0.86 loss: 159.243 (lr:0.1)\n",
      "300: ********* epoch 1 ********* test accuracy:0.9007 test loss: 156.126\n",
      "320: accuracy:0.92 loss: 155.069 (lr:0.1)\n",
      "340: accuracy:0.8 loss: 164.909 (lr:0.1)\n",
      "360: accuracy:0.83 loss: 162.972 (lr:0.1)\n",
      "380: accuracy:0.95 loss: 152.133 (lr:0.1)\n",
      "400: accuracy:0.92 loss: 153.628 (lr:0.1)\n",
      "400: ********* epoch 1 ********* test accuracy:0.8744 test loss: 158.679\n",
      "420: accuracy:0.88 loss: 157.882 (lr:0.1)\n",
      "440: accuracy:0.91 loss: 154.876 (lr:0.1)\n",
      "460: accuracy:0.91 loss: 155.055 (lr:0.1)\n",
      "480: accuracy:0.91 loss: 155.012 (lr:0.1)\n",
      "500: accuracy:0.95 loss: 151.421 (lr:0.1)\n",
      "500: ********* epoch 1 ********* test accuracy:0.9098 test loss: 155.119\n",
      "520: accuracy:0.92 loss: 153.174 (lr:0.1)\n",
      "540: accuracy:0.9 loss: 156.52 (lr:0.1)\n",
      "560: accuracy:0.89 loss: 157.699 (lr:0.1)\n",
      "580: accuracy:0.94 loss: 152.613 (lr:0.1)\n",
      "600: accuracy:0.88 loss: 157.755 (lr:0.1)\n",
      "600: ********* epoch 2 ********* test accuracy:0.899 test loss: 156.12\n",
      "620: accuracy:0.94 loss: 152.378 (lr:0.1)\n",
      "640: accuracy:0.93 loss: 153.196 (lr:0.1)\n",
      "660: accuracy:0.92 loss: 154.865 (lr:0.1)\n",
      "680: accuracy:0.91 loss: 155.545 (lr:0.1)\n",
      "700: accuracy:0.88 loss: 158.154 (lr:0.1)\n",
      "700: ********* epoch 2 ********* test accuracy:0.9225 test loss: 153.841\n",
      "720: accuracy:0.91 loss: 155.517 (lr:0.1)\n",
      "740: accuracy:0.9 loss: 155.811 (lr:0.1)\n",
      "760: accuracy:0.9 loss: 155.634 (lr:0.1)\n",
      "780: accuracy:0.88 loss: 157.89 (lr:0.1)\n",
      "800: accuracy:0.92 loss: 153.962 (lr:0.1)\n",
      "800: ********* epoch 2 ********* test accuracy:0.9217 test loss: 153.982\n",
      "820: accuracy:0.94 loss: 152.395 (lr:0.1)\n",
      "840: accuracy:0.88 loss: 157.936 (lr:0.1)\n",
      "860: accuracy:0.92 loss: 153.273 (lr:0.1)\n",
      "880: accuracy:0.94 loss: 152.412 (lr:0.1)\n",
      "900: accuracy:0.92 loss: 154.348 (lr:0.1)\n",
      "900: ********* epoch 2 ********* test accuracy:0.8984 test loss: 156.226\n",
      "920: accuracy:0.92 loss: 154.208 (lr:0.1)\n",
      "940: accuracy:0.92 loss: 154.664 (lr:0.1)\n",
      "960: accuracy:0.9 loss: 156.112 (lr:0.1)\n",
      "980: accuracy:0.87 loss: 159.104 (lr:0.1)\n",
      "1000: accuracy:0.94 loss: 152.226 (lr:0.1)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.933 test loss: 152.829\n",
      "1020: accuracy:0.94 loss: 151.985 (lr:0.1)\n",
      "1040: accuracy:0.91 loss: 154.873 (lr:0.1)\n",
      "1060: accuracy:0.88 loss: 157.409 (lr:0.1)\n",
      "1080: accuracy:0.9 loss: 156.136 (lr:0.1)\n",
      "1100: accuracy:0.92 loss: 154.111 (lr:0.1)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.902 test loss: 155.883\n",
      "1120: accuracy:0.92 loss: 153.598 (lr:0.1)\n",
      "1140: accuracy:0.89 loss: 156.46 (lr:0.1)\n",
      "1160: accuracy:0.88 loss: 158.165 (lr:0.1)\n",
      "1180: accuracy:0.95 loss: 151.338 (lr:0.1)\n",
      "1200: accuracy:0.91 loss: 155.074 (lr:0.1)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9062 test loss: 155.433\n",
      "1220: accuracy:0.94 loss: 152.171 (lr:0.1)\n",
      "1240: accuracy:0.95 loss: 151.101 (lr:0.1)\n",
      "1260: accuracy:0.92 loss: 153.874 (lr:0.1)\n",
      "1280: accuracy:0.92 loss: 154.051 (lr:0.1)\n",
      "1300: accuracy:0.8 loss: 165.327 (lr:0.1)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9037 test loss: 155.698\n",
      "1320: accuracy:0.94 loss: 152.234 (lr:0.1)\n",
      "1340: accuracy:0.94 loss: 152.398 (lr:0.1)\n",
      "1360: accuracy:0.91 loss: 155.046 (lr:0.1)\n",
      "1380: accuracy:0.95 loss: 151.707 (lr:0.1)\n",
      "1400: accuracy:0.91 loss: 155.279 (lr:0.1)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9278 test loss: 153.336\n",
      "1420: accuracy:0.91 loss: 154.624 (lr:0.1)\n",
      "1440: accuracy:0.96 loss: 150.156 (lr:0.1)\n",
      "1460: accuracy:0.91 loss: 155.369 (lr:0.1)\n",
      "1480: accuracy:0.89 loss: 157.093 (lr:0.1)\n",
      "1500: accuracy:0.9 loss: 156.311 (lr:0.1)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9193 test loss: 154.112\n",
      "1520: accuracy:0.93 loss: 152.814 (lr:0.1)\n",
      "1540: accuracy:0.91 loss: 154.962 (lr:0.1)\n",
      "1560: accuracy:0.93 loss: 153.024 (lr:0.1)\n",
      "1580: accuracy:0.91 loss: 155.121 (lr:0.1)\n",
      "1600: accuracy:0.9 loss: 155.214 (lr:0.1)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9233 test loss: 153.805\n",
      "1620: accuracy:0.9 loss: 156.229 (lr:0.1)\n",
      "1640: accuracy:0.94 loss: 152.135 (lr:0.1)\n",
      "1660: accuracy:0.92 loss: 153.788 (lr:0.1)\n",
      "1680: accuracy:0.95 loss: 151.199 (lr:0.1)\n",
      "1700: accuracy:0.9 loss: 156.419 (lr:0.1)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9339 test loss: 152.723\n",
      "1720: accuracy:0.93 loss: 153.511 (lr:0.1)\n",
      "1740: accuracy:0.89 loss: 157.209 (lr:0.1)\n",
      "1760: accuracy:0.91 loss: 155.053 (lr:0.1)\n",
      "1780: accuracy:0.93 loss: 153.046 (lr:0.1)\n",
      "1800: accuracy:0.92 loss: 154.037 (lr:0.1)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.906 test loss: 155.465\n",
      "1820: accuracy:0.93 loss: 153.147 (lr:0.1)\n",
      "1840: accuracy:0.95 loss: 151.085 (lr:0.1)\n",
      "1860: accuracy:0.92 loss: 153.868 (lr:0.1)\n",
      "1880: accuracy:0.97 loss: 149.154 (lr:0.1)\n",
      "1900: accuracy:0.94 loss: 151.79 (lr:0.1)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9319 test loss: 152.924\n",
      "1920: accuracy:0.89 loss: 157.121 (lr:0.1)\n",
      "1940: accuracy:0.91 loss: 154.864 (lr:0.1)\n",
      "1960: accuracy:0.92 loss: 153.537 (lr:0.1)\n",
      "1980: accuracy:0.94 loss: 151.667 (lr:0.1)\n",
      "2000: accuracy:0.94 loss: 152.122 (lr:0.1)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9191 test loss: 154.22\n",
      "2020: accuracy:0.91 loss: 155.115 (lr:0.1)\n",
      "2040: accuracy:0.95 loss: 151.155 (lr:0.1)\n",
      "2060: accuracy:0.95 loss: 151.152 (lr:0.1)\n",
      "2080: accuracy:0.93 loss: 153.55 (lr:0.1)\n",
      "2100: accuracy:0.91 loss: 155.477 (lr:0.1)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9132 test loss: 154.749\n",
      "2120: accuracy:0.92 loss: 154.094 (lr:0.1)\n",
      "2140: accuracy:0.93 loss: 152.512 (lr:0.1)\n",
      "2160: accuracy:0.91 loss: 154.645 (lr:0.1)\n",
      "2180: accuracy:0.95 loss: 151.287 (lr:0.1)\n",
      "2200: accuracy:0.92 loss: 154.061 (lr:0.1)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9408 test loss: 152.041\n",
      "2220: accuracy:0.89 loss: 156.913 (lr:0.1)\n",
      "2240: accuracy:0.92 loss: 154.114 (lr:0.1)\n",
      "2260: accuracy:0.91 loss: 155.096 (lr:0.1)\n",
      "2280: accuracy:0.95 loss: 151.141 (lr:0.1)\n",
      "2300: accuracy:0.92 loss: 154.141 (lr:0.1)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9295 test loss: 153.106\n",
      "2320: accuracy:0.95 loss: 151.163 (lr:0.1)\n",
      "2340: accuracy:0.92 loss: 154.137 (lr:0.1)\n",
      "2360: accuracy:0.91 loss: 155.099 (lr:0.1)\n",
      "2380: accuracy:0.93 loss: 153.188 (lr:0.1)\n",
      "2400: accuracy:0.98 loss: 148.509 (lr:0.1)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9389 test loss: 152.221\n",
      "2420: accuracy:0.95 loss: 150.843 (lr:0.1)\n",
      "2440: accuracy:0.92 loss: 154.108 (lr:0.1)\n",
      "2460: accuracy:0.9 loss: 156.258 (lr:0.1)\n",
      "2480: accuracy:0.93 loss: 153.143 (lr:0.1)\n",
      "2500: accuracy:0.91 loss: 155.139 (lr:0.1)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9368 test loss: 152.454\n",
      "2520: accuracy:0.92 loss: 153.711 (lr:0.1)\n",
      "2540: accuracy:0.93 loss: 153.263 (lr:0.1)\n",
      "2560: accuracy:0.89 loss: 157.111 (lr:0.1)\n",
      "2580: accuracy:0.95 loss: 151.295 (lr:0.1)\n",
      "2600: accuracy:0.92 loss: 153.549 (lr:0.1)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9206 test loss: 154.005\n",
      "2620: accuracy:0.85 loss: 161.26 (lr:0.1)\n",
      "2640: accuracy:0.94 loss: 152.075 (lr:0.1)\n",
      "2660: accuracy:0.9 loss: 156.098 (lr:0.1)\n",
      "2680: accuracy:0.93 loss: 153.163 (lr:0.1)\n",
      "2700: accuracy:0.93 loss: 153.217 (lr:0.1)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9361 test loss: 152.475\n",
      "2720: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "2740: accuracy:0.9 loss: 156.194 (lr:0.1)\n",
      "2760: accuracy:0.93 loss: 152.828 (lr:0.1)\n",
      "2780: accuracy:0.87 loss: 158.95 (lr:0.1)\n",
      "2800: accuracy:0.94 loss: 152.079 (lr:0.1)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9355 test loss: 152.55\n",
      "2820: accuracy:0.87 loss: 158.771 (lr:0.1)\n",
      "2840: accuracy:0.92 loss: 153.822 (lr:0.1)\n",
      "2860: accuracy:0.94 loss: 152.115 (lr:0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2880: accuracy:0.97 loss: 149.101 (lr:0.1)\n",
      "2900: accuracy:0.9 loss: 155.771 (lr:0.1)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9358 test loss: 152.554\n",
      "2920: accuracy:0.94 loss: 151.835 (lr:0.1)\n",
      "2940: accuracy:0.96 loss: 150.193 (lr:0.1)\n",
      "2960: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "2980: accuracy:0.94 loss: 152.113 (lr:0.1)\n",
      "3000: accuracy:0.91 loss: 155.124 (lr:0.1)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9189 test loss: 154.21\n",
      "3020: accuracy:0.9 loss: 155.694 (lr:0.1)\n",
      "3040: accuracy:0.97 loss: 149.697 (lr:0.1)\n",
      "3060: accuracy:0.91 loss: 154.532 (lr:0.1)\n",
      "3080: accuracy:0.92 loss: 154.117 (lr:0.1)\n",
      "3100: accuracy:0.94 loss: 152.203 (lr:0.1)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9398 test loss: 152.141\n",
      "3120: accuracy:0.94 loss: 151.841 (lr:0.1)\n",
      "3140: accuracy:0.9 loss: 156.201 (lr:0.1)\n",
      "3160: accuracy:0.96 loss: 150.613 (lr:0.1)\n",
      "3180: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "3200: accuracy:0.93 loss: 152.984 (lr:0.1)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.929 test loss: 153.187\n",
      "3220: accuracy:0.91 loss: 155.103 (lr:0.1)\n",
      "3240: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "3260: accuracy:0.97 loss: 149.163 (lr:0.1)\n",
      "3280: accuracy:0.98 loss: 148.414 (lr:0.1)\n",
      "3300: accuracy:0.91 loss: 155.033 (lr:0.1)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9447 test loss: 151.622\n",
      "3320: accuracy:0.95 loss: 150.863 (lr:0.1)\n",
      "3340: accuracy:0.93 loss: 153.073 (lr:0.1)\n",
      "3360: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "3380: accuracy:0.91 loss: 155.113 (lr:0.1)\n",
      "3400: accuracy:0.95 loss: 151.114 (lr:0.1)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9336 test loss: 152.698\n",
      "3420: accuracy:0.95 loss: 151.097 (lr:0.1)\n",
      "3440: accuracy:0.92 loss: 153.597 (lr:0.1)\n",
      "3460: accuracy:0.93 loss: 153.095 (lr:0.1)\n",
      "3480: accuracy:0.9 loss: 156.114 (lr:0.1)\n",
      "3500: accuracy:0.89 loss: 157.349 (lr:0.1)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9299 test loss: 153.122\n",
      "3520: accuracy:0.94 loss: 152.104 (lr:0.1)\n",
      "3540: accuracy:0.93 loss: 153.03 (lr:0.1)\n",
      "3560: accuracy:0.93 loss: 152.876 (lr:0.1)\n",
      "3580: accuracy:0.91 loss: 154.976 (lr:0.1)\n",
      "3600: accuracy:0.88 loss: 158.139 (lr:0.1)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9239 test loss: 153.734\n",
      "3620: accuracy:0.93 loss: 153.087 (lr:0.1)\n",
      "3640: accuracy:0.93 loss: 153.171 (lr:0.1)\n",
      "3660: accuracy:0.95 loss: 151.534 (lr:0.1)\n",
      "3680: accuracy:0.92 loss: 154.249 (lr:0.1)\n",
      "3700: accuracy:0.88 loss: 158.013 (lr:0.1)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9448 test loss: 151.648\n",
      "3720: accuracy:0.93 loss: 153.119 (lr:0.1)\n",
      "3740: accuracy:0.9 loss: 156.022 (lr:0.1)\n",
      "3760: accuracy:0.95 loss: 151.031 (lr:0.1)\n",
      "3780: accuracy:0.96 loss: 150.116 (lr:0.1)\n",
      "3800: accuracy:0.93 loss: 153.115 (lr:0.1)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9252 test loss: 153.558\n",
      "3820: accuracy:0.9 loss: 156.399 (lr:0.1)\n",
      "3840: accuracy:0.95 loss: 151.37 (lr:0.1)\n",
      "3860: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "3880: accuracy:0.85 loss: 160.69 (lr:0.1)\n",
      "3900: accuracy:0.94 loss: 151.994 (lr:0.1)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9376 test loss: 152.318\n",
      "3920: accuracy:0.93 loss: 152.748 (lr:0.1)\n",
      "3940: accuracy:0.92 loss: 154.206 (lr:0.1)\n",
      "3960: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "3980: accuracy:0.93 loss: 153.108 (lr:0.1)\n",
      "4000: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9441 test loss: 151.698\n",
      "4020: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "4040: accuracy:0.92 loss: 154.116 (lr:0.1)\n",
      "4060: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "4080: accuracy:0.91 loss: 155.112 (lr:0.1)\n",
      "4100: accuracy:0.96 loss: 150.06 (lr:0.1)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.942 test loss: 151.929\n",
      "4120: accuracy:0.99 loss: 147.116 (lr:0.1)\n",
      "4140: accuracy:0.96 loss: 150.036 (lr:0.1)\n",
      "4160: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "4180: accuracy:0.96 loss: 149.974 (lr:0.1)\n",
      "4200: accuracy:0.9 loss: 155.615 (lr:0.1)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9364 test loss: 152.475\n",
      "4220: accuracy:0.93 loss: 153.519 (lr:0.1)\n",
      "4240: accuracy:0.95 loss: 151.1 (lr:0.1)\n",
      "4260: accuracy:0.94 loss: 152.448 (lr:0.1)\n",
      "4280: accuracy:0.96 loss: 150.116 (lr:0.1)\n",
      "4300: accuracy:0.91 loss: 154.636 (lr:0.1)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.932 test loss: 152.825\n",
      "4320: accuracy:0.97 loss: 148.897 (lr:0.1)\n",
      "4340: accuracy:0.95 loss: 151.265 (lr:0.1)\n",
      "4360: accuracy:0.93 loss: 153.115 (lr:0.1)\n",
      "4380: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "4400: accuracy:0.92 loss: 154.087 (lr:0.1)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9366 test loss: 152.45\n",
      "4420: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "4440: accuracy:0.94 loss: 152.381 (lr:0.1)\n",
      "4460: accuracy:0.97 loss: 149.128 (lr:0.1)\n",
      "4480: accuracy:0.91 loss: 155.115 (lr:0.1)\n",
      "4500: accuracy:0.9 loss: 156.107 (lr:0.1)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9337 test loss: 152.734\n",
      "4520: accuracy:0.89 loss: 157.113 (lr:0.1)\n",
      "4540: accuracy:0.93 loss: 153.212 (lr:0.1)\n",
      "4560: accuracy:0.93 loss: 153.115 (lr:0.1)\n",
      "4580: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "4600: accuracy:0.93 loss: 153.356 (lr:0.1)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.938 test loss: 152.275\n",
      "4620: accuracy:0.95 loss: 151.159 (lr:0.1)\n",
      "4640: accuracy:0.94 loss: 152.119 (lr:0.1)\n",
      "4660: accuracy:0.94 loss: 151.9 (lr:0.1)\n",
      "4680: accuracy:0.91 loss: 155.131 (lr:0.1)\n",
      "4700: accuracy:0.94 loss: 152.116 (lr:0.1)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9299 test loss: 153.094\n",
      "4720: accuracy:0.9 loss: 156.103 (lr:0.1)\n",
      "4740: accuracy:0.9 loss: 156.096 (lr:0.1)\n",
      "4760: accuracy:0.91 loss: 155.082 (lr:0.1)\n",
      "4780: accuracy:0.89 loss: 157.113 (lr:0.1)\n",
      "4800: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9394 test loss: 152.158\n",
      "4820: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "4840: accuracy:0.95 loss: 151.111 (lr:0.1)\n",
      "4860: accuracy:0.94 loss: 151.875 (lr:0.1)\n",
      "4880: accuracy:0.91 loss: 155.082 (lr:0.1)\n",
      "4900: accuracy:0.94 loss: 152.11 (lr:0.1)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9406 test loss: 152.035\n",
      "4920: accuracy:0.98 loss: 148.115 (lr:0.1)\n",
      "4940: accuracy:0.93 loss: 153.077 (lr:0.1)\n",
      "4960: accuracy:0.97 loss: 149.121 (lr:0.1)\n",
      "4980: accuracy:0.97 loss: 149.124 (lr:0.1)\n",
      "5000: accuracy:0.98 loss: 148.115 (lr:0.1)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9457 test loss: 151.573\n",
      "5020: accuracy:0.91 loss: 155.166 (lr:0.1)\n",
      "5040: accuracy:0.92 loss: 154.113 (lr:0.1)\n",
      "5060: accuracy:0.96 loss: 150.123 (lr:0.1)\n",
      "5080: accuracy:0.91 loss: 154.898 (lr:0.1)\n",
      "5100: accuracy:0.92 loss: 154.042 (lr:0.1)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9462 test loss: 151.47\n",
      "5120: accuracy:0.96 loss: 150.096 (lr:0.1)\n",
      "5140: accuracy:0.92 loss: 154.113 (lr:0.1)\n",
      "5160: accuracy:0.93 loss: 153.112 (lr:0.1)\n",
      "5180: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "5200: accuracy:0.96 loss: 149.652 (lr:0.1)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9278 test loss: 153.332\n",
      "5220: accuracy:0.92 loss: 153.959 (lr:0.1)\n",
      "5240: accuracy:0.92 loss: 154.245 (lr:0.1)\n",
      "5260: accuracy:0.91 loss: 154.95 (lr:0.1)\n",
      "5280: accuracy:0.94 loss: 151.679 (lr:0.1)\n",
      "5300: accuracy:0.95 loss: 151.108 (lr:0.1)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.937 test loss: 152.349\n",
      "5320: accuracy:0.96 loss: 150.119 (lr:0.1)\n",
      "5340: accuracy:0.91 loss: 155.402 (lr:0.1)\n",
      "5360: accuracy:0.97 loss: 149.346 (lr:0.1)\n",
      "5380: accuracy:0.95 loss: 151.114 (lr:0.1)\n",
      "5400: accuracy:0.9 loss: 156.115 (lr:0.1)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.945 test loss: 151.612\n",
      "5420: accuracy:0.89 loss: 156.795 (lr:0.1)\n",
      "5440: accuracy:0.94 loss: 151.614 (lr:0.1)\n",
      "5460: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "5480: accuracy:0.93 loss: 153.114 (lr:0.1)\n",
      "5500: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9506 test loss: 151.077\n",
      "5520: accuracy:0.95 loss: 151.112 (lr:0.1)\n",
      "5540: accuracy:0.91 loss: 155.11 (lr:0.1)\n",
      "5560: accuracy:0.95 loss: 150.89 (lr:0.1)\n",
      "5580: accuracy:0.9 loss: 156.117 (lr:0.1)\n",
      "5600: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9333 test loss: 152.768\n",
      "5620: accuracy:0.92 loss: 154.257 (lr:0.1)\n",
      "5640: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "5660: accuracy:0.97 loss: 149.303 (lr:0.1)\n",
      "5680: accuracy:0.94 loss: 151.826 (lr:0.1)\n",
      "5700: accuracy:0.95 loss: 151.1 (lr:0.1)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9381 test loss: 152.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5720: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "5740: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "5760: accuracy:0.96 loss: 150.092 (lr:0.1)\n",
      "5780: accuracy:0.9 loss: 155.345 (lr:0.1)\n",
      "5800: accuracy:0.89 loss: 157.477 (lr:0.1)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9364 test loss: 152.478\n",
      "5820: accuracy:0.95 loss: 151.121 (lr:0.1)\n",
      "5840: accuracy:0.91 loss: 155.067 (lr:0.1)\n",
      "5860: accuracy:0.96 loss: 150.182 (lr:0.1)\n",
      "5880: accuracy:0.92 loss: 154.524 (lr:0.1)\n",
      "5900: accuracy:0.95 loss: 151.47 (lr:0.1)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9449 test loss: 151.628\n",
      "5920: accuracy:0.91 loss: 155.115 (lr:0.1)\n",
      "5940: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "5960: accuracy:0.94 loss: 152.48 (lr:0.1)\n",
      "5980: accuracy:0.93 loss: 153.156 (lr:0.1)\n",
      "6000: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9178 test loss: 154.327\n",
      "6020: accuracy:0.97 loss: 149.096 (lr:0.1)\n",
      "6040: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "6060: accuracy:0.97 loss: 149.326 (lr:0.1)\n",
      "6080: accuracy:0.96 loss: 150.342 (lr:0.1)\n",
      "6100: accuracy:0.96 loss: 149.977 (lr:0.1)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9446 test loss: 151.672\n",
      "6120: accuracy:0.9 loss: 156.344 (lr:0.1)\n",
      "6140: accuracy:0.93 loss: 153.011 (lr:0.1)\n",
      "6160: accuracy:0.96 loss: 150.118 (lr:0.1)\n",
      "6180: accuracy:0.91 loss: 155.034 (lr:0.1)\n",
      "6200: accuracy:0.9 loss: 155.975 (lr:0.1)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.949 test loss: 151.209\n",
      "6220: accuracy:0.96 loss: 150.202 (lr:0.1)\n",
      "6240: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "6260: accuracy:0.92 loss: 154.097 (lr:0.1)\n",
      "6280: accuracy:0.95 loss: 151.085 (lr:0.1)\n",
      "6300: accuracy:0.95 loss: 151.084 (lr:0.1)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.937 test loss: 152.408\n",
      "6320: accuracy:0.93 loss: 153.347 (lr:0.1)\n",
      "6340: accuracy:0.96 loss: 150.332 (lr:0.1)\n",
      "6360: accuracy:0.94 loss: 152.11 (lr:0.1)\n",
      "6380: accuracy:0.98 loss: 148.115 (lr:0.1)\n",
      "6400: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9433 test loss: 151.776\n",
      "6420: accuracy:0.99 loss: 147.191 (lr:0.1)\n",
      "6440: accuracy:0.96 loss: 150.263 (lr:0.1)\n",
      "6460: accuracy:0.94 loss: 152.1 (lr:0.1)\n",
      "6480: accuracy:0.93 loss: 153.118 (lr:0.1)\n",
      "6500: accuracy:0.96 loss: 150.176 (lr:0.1)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9465 test loss: 151.466\n",
      "6520: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "6540: accuracy:0.95 loss: 151.077 (lr:0.1)\n",
      "6560: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "6580: accuracy:0.95 loss: 150.686 (lr:0.1)\n",
      "6600: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9422 test loss: 151.877\n",
      "6620: accuracy:0.94 loss: 152.098 (lr:0.1)\n",
      "6640: accuracy:0.9 loss: 156.108 (lr:0.1)\n",
      "6660: accuracy:0.89 loss: 157.179 (lr:0.1)\n",
      "6680: accuracy:0.93 loss: 153.115 (lr:0.1)\n",
      "6700: accuracy:0.93 loss: 153.52 (lr:0.1)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9344 test loss: 152.689\n",
      "6720: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "6740: accuracy:0.92 loss: 154.119 (lr:0.1)\n",
      "6760: accuracy:0.96 loss: 150.114 (lr:0.1)\n",
      "6780: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "6800: accuracy:0.91 loss: 155.119 (lr:0.1)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9442 test loss: 151.687\n",
      "6820: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "6840: accuracy:0.88 loss: 157.853 (lr:0.1)\n",
      "6860: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "6880: accuracy:0.94 loss: 152.271 (lr:0.1)\n",
      "6900: accuracy:0.92 loss: 154.114 (lr:0.1)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9337 test loss: 152.754\n",
      "6920: accuracy:0.92 loss: 154.23 (lr:0.1)\n",
      "6940: accuracy:0.94 loss: 152.85 (lr:0.1)\n",
      "6960: accuracy:0.93 loss: 153.204 (lr:0.1)\n",
      "6980: accuracy:0.89 loss: 156.902 (lr:0.1)\n",
      "7000: accuracy:0.9 loss: 156.007 (lr:0.1)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9329 test loss: 152.826\n",
      "7020: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "7040: accuracy:0.93 loss: 153.067 (lr:0.1)\n",
      "7060: accuracy:0.91 loss: 154.95 (lr:0.1)\n",
      "7080: accuracy:0.94 loss: 152.108 (lr:0.1)\n",
      "7100: accuracy:0.91 loss: 155.109 (lr:0.1)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9424 test loss: 151.888\n",
      "7120: accuracy:0.92 loss: 154.096 (lr:0.1)\n",
      "7140: accuracy:0.94 loss: 152.13 (lr:0.1)\n",
      "7160: accuracy:0.92 loss: 153.82 (lr:0.1)\n",
      "7180: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "7200: accuracy:0.94 loss: 152.089 (lr:0.1)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9439 test loss: 151.719\n",
      "7220: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "7240: accuracy:0.95 loss: 150.677 (lr:0.1)\n",
      "7260: accuracy:0.96 loss: 149.697 (lr:0.1)\n",
      "7280: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "7300: accuracy:0.97 loss: 148.876 (lr:0.1)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.94 test loss: 152.114\n",
      "7320: accuracy:0.96 loss: 150.393 (lr:0.1)\n",
      "7340: accuracy:0.96 loss: 150.112 (lr:0.1)\n",
      "7360: accuracy:0.92 loss: 154.115 (lr:0.1)\n",
      "7380: accuracy:0.96 loss: 150.066 (lr:0.1)\n",
      "7400: accuracy:0.96 loss: 150.704 (lr:0.1)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9222 test loss: 153.865\n",
      "7420: accuracy:0.9 loss: 155.649 (lr:0.1)\n",
      "7440: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "7460: accuracy:0.97 loss: 149.11 (lr:0.1)\n",
      "7480: accuracy:0.95 loss: 151.042 (lr:0.1)\n",
      "7500: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.944 test loss: 151.712\n",
      "7520: accuracy:0.98 loss: 148.118 (lr:0.1)\n",
      "7540: accuracy:0.95 loss: 151.058 (lr:0.1)\n",
      "7560: accuracy:0.91 loss: 155.116 (lr:0.1)\n",
      "7580: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "7600: accuracy:0.97 loss: 148.744 (lr:0.1)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.937 test loss: 152.395\n",
      "7620: accuracy:0.93 loss: 153.115 (lr:0.1)\n",
      "7640: accuracy:0.94 loss: 152.116 (lr:0.1)\n",
      "7660: accuracy:0.96 loss: 150.114 (lr:0.1)\n",
      "7680: accuracy:0.92 loss: 153.956 (lr:0.1)\n",
      "7700: accuracy:0.96 loss: 150.576 (lr:0.1)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9211 test loss: 153.964\n",
      "7720: accuracy:0.94 loss: 152.1 (lr:0.1)\n",
      "7740: accuracy:0.94 loss: 151.755 (lr:0.1)\n",
      "7760: accuracy:0.92 loss: 154.115 (lr:0.1)\n",
      "7780: accuracy:0.96 loss: 150.181 (lr:0.1)\n",
      "7800: accuracy:0.96 loss: 149.837 (lr:0.1)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9502 test loss: 151.105\n",
      "7820: accuracy:0.95 loss: 151.086 (lr:0.1)\n",
      "7840: accuracy:0.93 loss: 152.984 (lr:0.1)\n",
      "7860: accuracy:0.98 loss: 148.126 (lr:0.1)\n",
      "7880: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "7900: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9438 test loss: 151.726\n",
      "7920: accuracy:0.92 loss: 154.094 (lr:0.1)\n",
      "7940: accuracy:0.91 loss: 155.114 (lr:0.1)\n",
      "7960: accuracy:0.9 loss: 156.432 (lr:0.1)\n",
      "7980: accuracy:0.93 loss: 153.046 (lr:0.1)\n",
      "8000: accuracy:0.92 loss: 154.356 (lr:0.1)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9414 test loss: 151.99\n",
      "8020: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "8040: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "8060: accuracy:0.93 loss: 153.115 (lr:0.1)\n",
      "8080: accuracy:0.91 loss: 155.417 (lr:0.1)\n",
      "8100: accuracy:0.99 loss: 147.115 (lr:0.1)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9411 test loss: 151.988\n",
      "8120: accuracy:0.94 loss: 152.329 (lr:0.1)\n",
      "8140: accuracy:0.92 loss: 154.198 (lr:0.1)\n",
      "8160: accuracy:0.94 loss: 151.972 (lr:0.1)\n",
      "8180: accuracy:0.96 loss: 150.106 (lr:0.1)\n",
      "8200: accuracy:0.94 loss: 151.894 (lr:0.1)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.944 test loss: 151.702\n",
      "8220: accuracy:0.95 loss: 151.089 (lr:0.1)\n",
      "8240: accuracy:0.92 loss: 154.253 (lr:0.1)\n",
      "8260: accuracy:0.93 loss: 153.109 (lr:0.1)\n",
      "8280: accuracy:0.98 loss: 148.115 (lr:0.1)\n",
      "8300: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9452 test loss: 151.601\n",
      "8320: accuracy:0.96 loss: 150.11 (lr:0.1)\n",
      "8340: accuracy:0.94 loss: 152.113 (lr:0.1)\n",
      "8360: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "8380: accuracy:0.92 loss: 154.115 (lr:0.1)\n",
      "8400: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9414 test loss: 151.971\n",
      "8420: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "8440: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "8460: accuracy:0.95 loss: 151.114 (lr:0.1)\n",
      "8480: accuracy:0.94 loss: 152.133 (lr:0.1)\n",
      "8500: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9443 test loss: 151.687\n",
      "8520: accuracy:0.94 loss: 152.117 (lr:0.1)\n",
      "8540: accuracy:0.94 loss: 152.144 (lr:0.1)\n",
      "8560: accuracy:0.96 loss: 150.466 (lr:0.1)\n",
      "8580: accuracy:0.95 loss: 151.115 (lr:0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600: accuracy:0.94 loss: 152.122 (lr:0.1)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9395 test loss: 152.135\n",
      "8620: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "8640: accuracy:0.96 loss: 150.104 (lr:0.1)\n",
      "8660: accuracy:0.92 loss: 154.248 (lr:0.1)\n",
      "8680: accuracy:0.98 loss: 148.115 (lr:0.1)\n",
      "8700: accuracy:0.94 loss: 152.117 (lr:0.1)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9408 test loss: 152.042\n",
      "8720: accuracy:0.94 loss: 152.106 (lr:0.1)\n",
      "8740: accuracy:0.94 loss: 152.112 (lr:0.1)\n",
      "8760: accuracy:0.97 loss: 148.847 (lr:0.1)\n",
      "8780: accuracy:0.96 loss: 150.09 (lr:0.1)\n",
      "8800: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9432 test loss: 151.797\n",
      "8820: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "8840: accuracy:0.89 loss: 157.01 (lr:0.1)\n",
      "8860: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "8880: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "8900: accuracy:0.99 loss: 147.128 (lr:0.1)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9478 test loss: 151.334\n",
      "8920: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "8940: accuracy:0.98 loss: 148.115 (lr:0.1)\n",
      "8960: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "8980: accuracy:0.97 loss: 149.116 (lr:0.1)\n",
      "9000: accuracy:0.93 loss: 152.612 (lr:0.1)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9516 test loss: 150.972\n",
      "9020: accuracy:0.92 loss: 154.114 (lr:0.1)\n",
      "9040: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "9060: accuracy:0.92 loss: 154.085 (lr:0.1)\n",
      "9080: accuracy:0.97 loss: 149.295 (lr:0.1)\n",
      "9100: accuracy:0.91 loss: 155.392 (lr:0.1)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.923 test loss: 153.808\n",
      "9120: accuracy:0.92 loss: 154.062 (lr:0.1)\n",
      "9140: accuracy:0.95 loss: 150.754 (lr:0.1)\n",
      "9160: accuracy:0.93 loss: 152.846 (lr:0.1)\n",
      "9180: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "9200: accuracy:0.98 loss: 148.115 (lr:0.1)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9426 test loss: 151.865\n",
      "9220: accuracy:0.93 loss: 152.59 (lr:0.1)\n",
      "9240: accuracy:0.91 loss: 155.126 (lr:0.1)\n",
      "9260: accuracy:0.95 loss: 150.906 (lr:0.1)\n",
      "9280: accuracy:0.96 loss: 150.121 (lr:0.1)\n",
      "9300: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9474 test loss: 151.374\n",
      "9320: accuracy:0.89 loss: 157.115 (lr:0.1)\n",
      "9340: accuracy:0.98 loss: 148.116 (lr:0.1)\n",
      "9360: accuracy:0.96 loss: 150.118 (lr:0.1)\n",
      "9380: accuracy:0.91 loss: 155.179 (lr:0.1)\n",
      "9400: accuracy:0.96 loss: 150.13 (lr:0.1)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9387 test loss: 152.223\n",
      "9420: accuracy:0.94 loss: 151.754 (lr:0.1)\n",
      "9440: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "9460: accuracy:0.88 loss: 158.035 (lr:0.1)\n",
      "9480: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "9500: accuracy:0.97 loss: 149.093 (lr:0.1)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9521 test loss: 150.916\n",
      "9520: accuracy:0.95 loss: 151.132 (lr:0.1)\n",
      "9540: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "9560: accuracy:0.93 loss: 152.634 (lr:0.1)\n",
      "9580: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "9600: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9511 test loss: 151.02\n",
      "9620: accuracy:0.93 loss: 153.116 (lr:0.1)\n",
      "9640: accuracy:0.92 loss: 154.115 (lr:0.1)\n",
      "9660: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "9680: accuracy:0.91 loss: 155.129 (lr:0.1)\n",
      "9700: accuracy:0.99 loss: 147.134 (lr:0.1)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9454 test loss: 151.566\n",
      "9720: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "9740: accuracy:0.95 loss: 151.116 (lr:0.1)\n",
      "9760: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "9780: accuracy:0.91 loss: 155.1 (lr:0.1)\n",
      "9800: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9438 test loss: 151.748\n",
      "9820: accuracy:0.94 loss: 152.115 (lr:0.1)\n",
      "9840: accuracy:0.93 loss: 153.212 (lr:0.1)\n",
      "9860: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "9880: accuracy:0.99 loss: 147.115 (lr:0.1)\n",
      "9900: accuracy:0.93 loss: 153.115 (lr:0.1)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.946 test loss: 151.483\n",
      "9920: accuracy:0.97 loss: 149.115 (lr:0.1)\n",
      "9940: accuracy:0.98 loss: 148.121 (lr:0.1)\n",
      "9960: accuracy:0.96 loss: 150.115 (lr:0.1)\n",
      "9980: accuracy:0.95 loss: 151.115 (lr:0.1)\n",
      "10000: accuracy:0.96 loss: 150.115 (lr:0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:30<00:30, 30.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: ********* epoch 17 ********* test accuracy:0.9442 test loss: 151.669\n",
      "0: accuracy:0.14 loss: 229.555 (lr:0.8)\n",
      "0: ********* epoch 1 ********* test accuracy:0.0921 test loss: 230.601\n",
      "20: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "40: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "60: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "80: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "100: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "100: ********* epoch 1 ********* test accuracy:0.1032 test loss: 235.79\n",
      "120: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "140: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "160: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "180: accuracy:0.04 loss: 242.115 (lr:0.8)\n",
      "200: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "200: ********* epoch 1 ********* test accuracy:0.1032 test loss: 235.79\n",
      "220: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "240: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "260: accuracy:0.04 loss: 242.115 (lr:0.8)\n",
      "280: accuracy:0.04 loss: 242.115 (lr:0.8)\n",
      "300: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "300: ********* epoch 1 ********* test accuracy:0.1032 test loss: 235.79\n",
      "320: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "340: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "360: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "380: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "400: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "400: ********* epoch 1 ********* test accuracy:0.1032 test loss: 235.79\n",
      "420: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "440: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "460: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "480: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "500: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "500: ********* epoch 1 ********* test accuracy:0.1032 test loss: 235.79\n",
      "520: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "540: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "560: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "580: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "600: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "600: ********* epoch 2 ********* test accuracy:0.1032 test loss: 235.79\n",
      "620: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "640: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "660: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "680: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "700: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "700: ********* epoch 2 ********* test accuracy:0.1032 test loss: 235.79\n",
      "720: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "740: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "760: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "780: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "800: accuracy:0.04 loss: 242.115 (lr:0.8)\n",
      "800: ********* epoch 2 ********* test accuracy:0.1032 test loss: 235.79\n",
      "820: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "840: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "860: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "880: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "900: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "900: ********* epoch 2 ********* test accuracy:0.1032 test loss: 235.79\n",
      "920: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "940: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "960: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "980: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "1000: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.1032 test loss: 235.79\n",
      "1020: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "1040: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "1060: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "1080: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "1100: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.1032 test loss: 235.79\n",
      "1120: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1140: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "1160: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "1180: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "1200: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.1032 test loss: 235.79\n",
      "1220: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "1240: accuracy:0.1 loss: 236.114 (lr:0.8)\n",
      "1260: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "1280: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "1300: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.1032 test loss: 235.79\n",
      "1320: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "1340: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1360: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1380: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1400: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.1032 test loss: 235.79\n",
      "1420: accuracy:0.05 loss: 241.115 (lr:0.8)\n",
      "1440: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "1460: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "1480: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "1500: accuracy:0.09 loss: 237.113 (lr:0.8)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.1032 test loss: 235.79\n",
      "1520: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "1540: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "1560: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "1580: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "1600: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1620: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1640: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "1660: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "1680: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1700: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1720: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "1740: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "1760: accuracy:0.04 loss: 242.115 (lr:0.8)\n",
      "1780: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "1800: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1820: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "1840: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "1860: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1880: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "1900: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1920: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "1940: accuracy:0.05 loss: 241.115 (lr:0.8)\n",
      "1960: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "1980: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "2000: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2020: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "2040: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "2060: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "2080: accuracy:0.03 loss: 243.115 (lr:0.8)\n",
      "2100: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2120: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "2140: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "2160: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "2180: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "2200: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2220: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "2240: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "2260: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "2280: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "2300: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2320: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "2340: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "2360: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "2380: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "2400: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2420: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "2440: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "2460: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "2480: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "2500: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2520: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "2540: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "2560: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "2580: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "2600: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2620: accuracy:0.18 loss: 228.115 (lr:0.8)\n",
      "2640: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "2660: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "2680: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "2700: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2720: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "2740: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "2760: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "2780: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "2800: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2820: accuracy:0.17 loss: 229.115 (lr:0.8)\n",
      "2840: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "2860: accuracy:0.04 loss: 242.115 (lr:0.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2880: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "2900: accuracy:0.04 loss: 242.115 (lr:0.8)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2920: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "2940: accuracy:0.03 loss: 243.115 (lr:0.8)\n",
      "2960: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "2980: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "3000: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3020: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "3040: accuracy:0.17 loss: 229.115 (lr:0.8)\n",
      "3060: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "3080: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3100: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3120: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3140: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "3160: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "3180: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3200: accuracy:0.05 loss: 241.115 (lr:0.8)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3220: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "3240: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "3260: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "3280: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3300: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3320: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3340: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3360: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "3380: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "3400: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3420: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "3440: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3460: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "3480: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "3500: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3520: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3540: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "3560: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3580: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "3600: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3620: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3640: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3660: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "3680: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3700: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3720: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "3740: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "3760: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "3780: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "3800: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3820: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "3840: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "3860: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "3880: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3900: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3920: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "3940: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "3960: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "3980: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4000: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4020: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "4040: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "4060: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4080: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "4100: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4120: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "4140: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "4160: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "4180: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4200: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4220: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "4240: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "4260: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "4280: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "4300: accuracy:0.19 loss: 227.115 (lr:0.8)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4320: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "4340: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4360: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "4380: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "4400: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4420: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "4440: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "4460: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "4480: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "4500: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4520: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4540: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "4560: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "4580: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "4600: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4620: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "4640: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4660: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "4680: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "4700: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4720: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "4740: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "4760: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "4780: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "4800: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4820: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "4840: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "4860: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "4880: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "4900: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4920: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "4940: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "4960: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "4980: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "5000: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5020: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "5040: accuracy:0.05 loss: 241.115 (lr:0.8)\n",
      "5060: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "5080: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "5100: accuracy:0.03 loss: 243.115 (lr:0.8)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5120: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5140: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5160: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "5180: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5200: accuracy:0.18 loss: 228.115 (lr:0.8)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5220: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "5240: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "5260: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "5280: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "5300: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5320: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "5340: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "5360: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "5380: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "5400: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5420: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "5440: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "5460: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "5480: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "5500: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5520: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "5540: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5560: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "5580: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "5600: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5620: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "5640: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "5660: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "5680: accuracy:0.04 loss: 242.115 (lr:0.8)\n",
      "5700: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5720: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "5740: accuracy:0.11 loss: 235.115 (lr:0.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5760: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "5780: accuracy:0.17 loss: 229.115 (lr:0.8)\n",
      "5800: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5820: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5840: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5860: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "5880: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "5900: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5920: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "5940: accuracy:0.18 loss: 228.115 (lr:0.8)\n",
      "5960: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "5980: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "6000: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6020: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "6040: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "6060: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "6080: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "6100: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6120: accuracy:0.19 loss: 227.115 (lr:0.8)\n",
      "6140: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "6160: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "6180: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "6200: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6220: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "6240: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "6260: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "6280: accuracy:0.19 loss: 227.115 (lr:0.8)\n",
      "6300: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6320: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "6340: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "6360: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "6380: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "6400: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6420: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "6440: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "6460: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "6480: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "6500: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6520: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "6540: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "6560: accuracy:0.17 loss: 229.115 (lr:0.8)\n",
      "6580: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "6600: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6620: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "6640: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "6660: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "6680: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "6700: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6720: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "6740: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "6760: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "6780: accuracy:0.03 loss: 243.115 (lr:0.8)\n",
      "6800: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6820: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "6840: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "6860: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "6880: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "6900: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6920: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "6940: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "6960: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "6980: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "7000: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7020: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "7040: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7060: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "7080: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "7100: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7120: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "7140: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "7160: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "7180: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7200: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7220: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "7240: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "7260: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "7280: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7300: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7320: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "7340: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7360: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "7380: accuracy:0.17 loss: 229.115 (lr:0.8)\n",
      "7400: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7420: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "7440: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "7460: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "7480: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "7500: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7520: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "7540: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7560: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "7580: accuracy:0.05 loss: 241.115 (lr:0.8)\n",
      "7600: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7620: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "7640: accuracy:0.18 loss: 228.115 (lr:0.8)\n",
      "7660: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7680: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "7700: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7720: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "7740: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7760: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7780: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "7800: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7820: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "7840: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "7860: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "7880: accuracy:0.05 loss: 241.115 (lr:0.8)\n",
      "7900: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7920: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "7940: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "7960: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "7980: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "8000: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8020: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "8040: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "8060: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "8080: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "8100: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8120: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "8140: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "8160: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8180: accuracy:0.05 loss: 241.115 (lr:0.8)\n",
      "8200: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8220: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "8240: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "8260: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "8280: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "8300: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8320: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "8340: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "8360: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "8380: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8400: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8420: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8440: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "8460: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "8480: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "8500: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8520: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "8540: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8560: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8580: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "8600: accuracy:0.12 loss: 234.115 (lr:0.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8620: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "8640: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "8660: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "8680: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "8700: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8720: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8740: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "8760: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "8780: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "8800: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8820: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "8840: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8860: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "8880: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "8900: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8920: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "8940: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "8960: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "8980: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "9000: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9020: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "9040: accuracy:0.04 loss: 242.115 (lr:0.8)\n",
      "9060: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "9080: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "9100: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9120: accuracy:0.15 loss: 231.115 (lr:0.8)\n",
      "9140: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "9160: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "9180: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "9200: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9220: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "9240: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "9260: accuracy:0.03 loss: 243.115 (lr:0.8)\n",
      "9280: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "9300: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9320: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "9340: accuracy:0.14 loss: 232.115 (lr:0.8)\n",
      "9360: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "9380: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "9400: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9420: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "9440: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "9460: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "9480: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "9500: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9520: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "9540: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "9560: accuracy:0.06 loss: 240.115 (lr:0.8)\n",
      "9580: accuracy:0.13 loss: 233.115 (lr:0.8)\n",
      "9600: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9620: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "9640: accuracy:0.17 loss: 229.115 (lr:0.8)\n",
      "9660: accuracy:0.11 loss: 235.115 (lr:0.8)\n",
      "9680: accuracy:0.09 loss: 237.115 (lr:0.8)\n",
      "9700: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9720: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "9740: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "9760: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "9780: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "9800: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9820: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "9840: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "9860: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "9880: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "9900: accuracy:0.07 loss: 239.115 (lr:0.8)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9920: accuracy:0.08 loss: 238.115 (lr:0.8)\n",
      "9940: accuracy:0.1 loss: 236.115 (lr:0.8)\n",
      "9960: accuracy:0.16 loss: 230.115 (lr:0.8)\n",
      "9980: accuracy:0.12 loss: 234.115 (lr:0.8)\n",
      "10000: accuracy:0.1 loss: 236.115 (lr:0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 2/2 [01:22<00:00, 36.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGCThL4QQMej/Az+94wN6NmKeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a75442cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning rates: [ 0.1  0.8]\n",
      "Accuracy: [0.94419998, 0.1028]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "h_dim = 100 # Fixed number of hidden dimensions\n",
    "n = 500 # Number of point in the grid.\n",
    "xn = np.arange(0,2,10/n)\n",
    "\n",
    "l_rates = np.array([0.1,0.8]) # Initial learning rates\n",
    "f = [nn_train(l, h_dim) for l in tqdm(l_rates)] #Accuracy\n",
    "\n",
    "noise = 1;length =1; m=0\n",
    "m, noise, length, sf = opt_hyparams(l_rates,f)\n",
    "E, cov = gp_posterior(l_rates, f, xn, m, noise, length, sf)\n",
    "data = data_posterior(xn, E, cov)\n",
    "\n",
    "#This plot shows the mean and variance with two initial observations:\n",
    "plt.plot(data['x'],data['Mean'], color = 'black', label = 'Mean')\n",
    "plt.plot(l_rates,f, 'ro', label = 'Obs')\n",
    "plt.fill_between(data['x'], data['Mean']-data['StdDev'], data['Mean']+data['StdDev'],color = 'lightgrey')\n",
    "plt.show()\n",
    "\n",
    "print ('Initial Learning rates:', l_rates)\n",
    "print ('Accuracy:', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we perform 5 interations. The figure below exhibits the learning rates selected by the algorithm with their corresponding accuracies. Additionally it shows the new mean and 1 standard deviation around the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.12 loss: 229.232 (lr:0.0)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1336 test loss: 230.071\n",
      "20: accuracy:0.13 loss: 230.084 (lr:0.0)\n",
      "40: accuracy:0.1 loss: 230.391 (lr:0.0)\n",
      "60: accuracy:0.1 loss: 230.533 (lr:0.0)\n",
      "80: accuracy:0.13 loss: 229.909 (lr:0.0)\n",
      "100: accuracy:0.12 loss: 229.779 (lr:0.0)\n",
      "100: ********* epoch 1 ********* test accuracy:0.1336 test loss: 230.071\n",
      "120: accuracy:0.09 loss: 231.054 (lr:0.0)\n",
      "140: accuracy:0.06 loss: 231.455 (lr:0.0)\n",
      "160: accuracy:0.17 loss: 229.183 (lr:0.0)\n",
      "180: accuracy:0.16 loss: 230.024 (lr:0.0)\n",
      "200: accuracy:0.14 loss: 230.721 (lr:0.0)\n",
      "200: ********* epoch 1 ********* test accuracy:0.1336 test loss: 230.071\n",
      "220: accuracy:0.14 loss: 230.439 (lr:0.0)\n",
      "240: accuracy:0.17 loss: 229.997 (lr:0.0)\n",
      "260: accuracy:0.13 loss: 230.728 (lr:0.0)\n",
      "280: accuracy:0.12 loss: 229.538 (lr:0.0)\n",
      "300: accuracy:0.07 loss: 231.737 (lr:0.0)\n",
      "300: ********* epoch 1 ********* test accuracy:0.1336 test loss: 230.071\n",
      "320: accuracy:0.18 loss: 228.794 (lr:0.0)\n",
      "340: accuracy:0.06 loss: 231.061 (lr:0.0)\n",
      "360: accuracy:0.08 loss: 230.214 (lr:0.0)\n",
      "380: accuracy:0.04 loss: 231.176 (lr:0.0)\n",
      "400: accuracy:0.13 loss: 230.647 (lr:0.0)\n",
      "400: ********* epoch 1 ********* test accuracy:0.1336 test loss: 230.071\n",
      "420: accuracy:0.19 loss: 228.728 (lr:0.0)\n",
      "440: accuracy:0.07 loss: 230.622 (lr:0.0)\n",
      "460: accuracy:0.12 loss: 229.83 (lr:0.0)\n",
      "480: accuracy:0.16 loss: 230.194 (lr:0.0)\n",
      "500: accuracy:0.1 loss: 230.858 (lr:0.0)\n",
      "500: ********* epoch 1 ********* test accuracy:0.1336 test loss: 230.071\n",
      "520: accuracy:0.15 loss: 228.937 (lr:0.0)\n",
      "540: accuracy:0.17 loss: 229.422 (lr:0.0)\n",
      "560: accuracy:0.08 loss: 231.385 (lr:0.0)\n",
      "580: accuracy:0.1 loss: 230.323 (lr:0.0)\n",
      "600: accuracy:0.11 loss: 231.198 (lr:0.0)\n",
      "600: ********* epoch 2 ********* test accuracy:0.1336 test loss: 230.071\n",
      "620: accuracy:0.13 loss: 230.433 (lr:0.0)\n",
      "640: accuracy:0.17 loss: 229.018 (lr:0.0)\n",
      "660: accuracy:0.09 loss: 230.33 (lr:0.0)\n",
      "680: accuracy:0.09 loss: 231.427 (lr:0.0)\n",
      "700: accuracy:0.14 loss: 230.458 (lr:0.0)\n",
      "700: ********* epoch 2 ********* test accuracy:0.1336 test loss: 230.071\n",
      "720: accuracy:0.16 loss: 228.685 (lr:0.0)\n",
      "740: accuracy:0.09 loss: 231.234 (lr:0.0)\n",
      "760: accuracy:0.14 loss: 230.02 (lr:0.0)\n",
      "780: accuracy:0.12 loss: 230.107 (lr:0.0)\n",
      "800: accuracy:0.09 loss: 230.12 (lr:0.0)\n",
      "800: ********* epoch 2 ********* test accuracy:0.1336 test loss: 230.071\n",
      "820: accuracy:0.1 loss: 230.2 (lr:0.0)\n",
      "840: accuracy:0.11 loss: 230.108 (lr:0.0)\n",
      "860: accuracy:0.14 loss: 230.03 (lr:0.0)\n",
      "880: accuracy:0.1 loss: 231.284 (lr:0.0)\n",
      "900: accuracy:0.13 loss: 229.85 (lr:0.0)\n",
      "900: ********* epoch 2 ********* test accuracy:0.1336 test loss: 230.071\n",
      "920: accuracy:0.14 loss: 230.111 (lr:0.0)\n",
      "940: accuracy:0.21 loss: 228.825 (lr:0.0)\n",
      "960: accuracy:0.1 loss: 230.225 (lr:0.0)\n",
      "980: accuracy:0.17 loss: 229.508 (lr:0.0)\n",
      "1000: accuracy:0.12 loss: 230.329 (lr:0.0)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1020: accuracy:0.05 loss: 231.993 (lr:0.0)\n",
      "1040: accuracy:0.13 loss: 231.094 (lr:0.0)\n",
      "1060: accuracy:0.12 loss: 230.5 (lr:0.0)\n",
      "1080: accuracy:0.21 loss: 228.534 (lr:0.0)\n",
      "1100: accuracy:0.18 loss: 228.309 (lr:0.0)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1120: accuracy:0.08 loss: 230.998 (lr:0.0)\n",
      "1140: accuracy:0.15 loss: 229.403 (lr:0.0)\n",
      "1160: accuracy:0.12 loss: 231.18 (lr:0.0)\n",
      "1180: accuracy:0.14 loss: 229.56 (lr:0.0)\n",
      "1200: accuracy:0.12 loss: 230.223 (lr:0.0)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1220: accuracy:0.11 loss: 230.032 (lr:0.0)\n",
      "1240: accuracy:0.13 loss: 230.519 (lr:0.0)\n",
      "1260: accuracy:0.08 loss: 230.468 (lr:0.0)\n",
      "1280: accuracy:0.13 loss: 230.159 (lr:0.0)\n",
      "1300: accuracy:0.13 loss: 229.881 (lr:0.0)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1320: accuracy:0.14 loss: 229.982 (lr:0.0)\n",
      "1340: accuracy:0.15 loss: 229.824 (lr:0.0)\n",
      "1360: accuracy:0.11 loss: 230.848 (lr:0.0)\n",
      "1380: accuracy:0.13 loss: 230.018 (lr:0.0)\n",
      "1400: accuracy:0.13 loss: 230.041 (lr:0.0)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1420: accuracy:0.12 loss: 230.048 (lr:0.0)\n",
      "1440: accuracy:0.18 loss: 228.929 (lr:0.0)\n",
      "1460: accuracy:0.16 loss: 229.804 (lr:0.0)\n",
      "1480: accuracy:0.09 loss: 230.925 (lr:0.0)\n",
      "1500: accuracy:0.08 loss: 230.586 (lr:0.0)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1520: accuracy:0.07 loss: 231.275 (lr:0.0)\n",
      "1540: accuracy:0.13 loss: 228.986 (lr:0.0)\n",
      "1560: accuracy:0.21 loss: 229.094 (lr:0.0)\n",
      "1580: accuracy:0.14 loss: 229.726 (lr:0.0)\n",
      "1600: accuracy:0.17 loss: 229.094 (lr:0.0)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1620: accuracy:0.1 loss: 230.628 (lr:0.0)\n",
      "1640: accuracy:0.14 loss: 229.082 (lr:0.0)\n",
      "1660: accuracy:0.14 loss: 229.815 (lr:0.0)\n",
      "1680: accuracy:0.16 loss: 229.488 (lr:0.0)\n",
      "1700: accuracy:0.09 loss: 230.547 (lr:0.0)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1720: accuracy:0.16 loss: 229.298 (lr:0.0)\n",
      "1740: accuracy:0.09 loss: 231.626 (lr:0.0)\n",
      "1760: accuracy:0.11 loss: 229.788 (lr:0.0)\n",
      "1780: accuracy:0.14 loss: 229.309 (lr:0.0)\n",
      "1800: accuracy:0.1 loss: 230.556 (lr:0.0)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1820: accuracy:0.07 loss: 231.693 (lr:0.0)\n",
      "1840: accuracy:0.14 loss: 230.913 (lr:0.0)\n",
      "1860: accuracy:0.16 loss: 229.695 (lr:0.0)\n",
      "1880: accuracy:0.1 loss: 230.392 (lr:0.0)\n",
      "1900: accuracy:0.08 loss: 231.02 (lr:0.0)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.1336 test loss: 230.071\n",
      "1920: accuracy:0.11 loss: 230.68 (lr:0.0)\n",
      "1940: accuracy:0.06 loss: 230.758 (lr:0.0)\n",
      "1960: accuracy:0.15 loss: 229.771 (lr:0.0)\n",
      "1980: accuracy:0.15 loss: 229.706 (lr:0.0)\n",
      "2000: accuracy:0.15 loss: 229.748 (lr:0.0)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2020: accuracy:0.11 loss: 229.788 (lr:0.0)\n",
      "2040: accuracy:0.13 loss: 230.814 (lr:0.0)\n",
      "2060: accuracy:0.16 loss: 229.685 (lr:0.0)\n",
      "2080: accuracy:0.06 loss: 230.978 (lr:0.0)\n",
      "2100: accuracy:0.07 loss: 231.014 (lr:0.0)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2120: accuracy:0.17 loss: 228.871 (lr:0.0)\n",
      "2140: accuracy:0.11 loss: 230.207 (lr:0.0)\n",
      "2160: accuracy:0.06 loss: 230.989 (lr:0.0)\n",
      "2180: accuracy:0.18 loss: 229.288 (lr:0.0)\n",
      "2200: accuracy:0.11 loss: 230.689 (lr:0.0)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2220: accuracy:0.2 loss: 229.67 (lr:0.0)\n",
      "2240: accuracy:0.08 loss: 230.754 (lr:0.0)\n",
      "2260: accuracy:0.13 loss: 230.268 (lr:0.0)\n",
      "2280: accuracy:0.06 loss: 231.116 (lr:0.0)\n",
      "2300: accuracy:0.11 loss: 230.172 (lr:0.0)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2320: accuracy:0.16 loss: 229.225 (lr:0.0)\n",
      "2340: accuracy:0.15 loss: 230.086 (lr:0.0)\n",
      "2360: accuracy:0.12 loss: 230.428 (lr:0.0)\n",
      "2380: accuracy:0.14 loss: 229.915 (lr:0.0)\n",
      "2400: accuracy:0.11 loss: 229.989 (lr:0.0)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2420: accuracy:0.16 loss: 230.027 (lr:0.0)\n",
      "2440: accuracy:0.14 loss: 229.426 (lr:0.0)\n",
      "2460: accuracy:0.13 loss: 230.507 (lr:0.0)\n",
      "2480: accuracy:0.1 loss: 230.641 (lr:0.0)\n",
      "2500: accuracy:0.13 loss: 229.995 (lr:0.0)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2520: accuracy:0.06 loss: 230.678 (lr:0.0)\n",
      "2540: accuracy:0.16 loss: 229.353 (lr:0.0)\n",
      "2560: accuracy:0.1 loss: 229.382 (lr:0.0)\n",
      "2580: accuracy:0.1 loss: 230.293 (lr:0.0)\n",
      "2600: accuracy:0.09 loss: 230.393 (lr:0.0)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2620: accuracy:0.17 loss: 229.386 (lr:0.0)\n",
      "2640: accuracy:0.09 loss: 229.467 (lr:0.0)\n",
      "2660: accuracy:0.13 loss: 229.106 (lr:0.0)\n",
      "2680: accuracy:0.16 loss: 230.098 (lr:0.0)\n",
      "2700: accuracy:0.12 loss: 229.941 (lr:0.0)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2720: accuracy:0.17 loss: 229.389 (lr:0.0)\n",
      "2740: accuracy:0.09 loss: 230.534 (lr:0.0)\n",
      "2760: accuracy:0.19 loss: 229.137 (lr:0.0)\n",
      "2780: accuracy:0.1 loss: 231.058 (lr:0.0)\n",
      "2800: accuracy:0.15 loss: 230.241 (lr:0.0)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2820: accuracy:0.07 loss: 231.621 (lr:0.0)\n",
      "2840: accuracy:0.14 loss: 229.886 (lr:0.0)\n",
      "2860: accuracy:0.08 loss: 231.511 (lr:0.0)\n",
      "2880: accuracy:0.09 loss: 230.488 (lr:0.0)\n",
      "2900: accuracy:0.12 loss: 230.308 (lr:0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900: ********* epoch 5 ********* test accuracy:0.1336 test loss: 230.071\n",
      "2920: accuracy:0.18 loss: 229.194 (lr:0.0)\n",
      "2940: accuracy:0.14 loss: 229.674 (lr:0.0)\n",
      "2960: accuracy:0.12 loss: 229.841 (lr:0.0)\n",
      "2980: accuracy:0.13 loss: 230.542 (lr:0.0)\n",
      "3000: accuracy:0.17 loss: 229.24 (lr:0.0)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3020: accuracy:0.12 loss: 230.179 (lr:0.0)\n",
      "3040: accuracy:0.09 loss: 231.035 (lr:0.0)\n",
      "3060: accuracy:0.12 loss: 229.504 (lr:0.0)\n",
      "3080: accuracy:0.19 loss: 228.02 (lr:0.0)\n",
      "3100: accuracy:0.14 loss: 230.279 (lr:0.0)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3120: accuracy:0.06 loss: 231.295 (lr:0.0)\n",
      "3140: accuracy:0.14 loss: 229.602 (lr:0.0)\n",
      "3160: accuracy:0.1 loss: 230.188 (lr:0.0)\n",
      "3180: accuracy:0.12 loss: 230.298 (lr:0.0)\n",
      "3200: accuracy:0.1 loss: 231.072 (lr:0.0)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3220: accuracy:0.1 loss: 229.799 (lr:0.0)\n",
      "3240: accuracy:0.13 loss: 230.189 (lr:0.0)\n",
      "3260: accuracy:0.14 loss: 230.185 (lr:0.0)\n",
      "3280: accuracy:0.17 loss: 230.082 (lr:0.0)\n",
      "3300: accuracy:0.19 loss: 229.024 (lr:0.0)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3320: accuracy:0.1 loss: 230.511 (lr:0.0)\n",
      "3340: accuracy:0.18 loss: 229.013 (lr:0.0)\n",
      "3360: accuracy:0.14 loss: 229.063 (lr:0.0)\n",
      "3380: accuracy:0.11 loss: 230.225 (lr:0.0)\n",
      "3400: accuracy:0.13 loss: 230.363 (lr:0.0)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3420: accuracy:0.08 loss: 231.35 (lr:0.0)\n",
      "3440: accuracy:0.07 loss: 231.368 (lr:0.0)\n",
      "3460: accuracy:0.1 loss: 230.505 (lr:0.0)\n",
      "3480: accuracy:0.16 loss: 230.138 (lr:0.0)\n",
      "3500: accuracy:0.08 loss: 231.1 (lr:0.0)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3520: accuracy:0.11 loss: 230.556 (lr:0.0)\n",
      "3540: accuracy:0.07 loss: 230.538 (lr:0.0)\n",
      "3560: accuracy:0.15 loss: 229.799 (lr:0.0)\n",
      "3580: accuracy:0.12 loss: 230.081 (lr:0.0)\n",
      "3600: accuracy:0.09 loss: 229.693 (lr:0.0)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3620: accuracy:0.13 loss: 229.656 (lr:0.0)\n",
      "3640: accuracy:0.14 loss: 230.369 (lr:0.0)\n",
      "3660: accuracy:0.13 loss: 230.268 (lr:0.0)\n",
      "3680: accuracy:0.15 loss: 230.03 (lr:0.0)\n",
      "3700: accuracy:0.08 loss: 231.286 (lr:0.0)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3720: accuracy:0.11 loss: 230.105 (lr:0.0)\n",
      "3740: accuracy:0.09 loss: 230.765 (lr:0.0)\n",
      "3760: accuracy:0.14 loss: 230.189 (lr:0.0)\n",
      "3780: accuracy:0.19 loss: 229.229 (lr:0.0)\n",
      "3800: accuracy:0.13 loss: 230.632 (lr:0.0)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3820: accuracy:0.18 loss: 229.811 (lr:0.0)\n",
      "3840: accuracy:0.1 loss: 229.961 (lr:0.0)\n",
      "3860: accuracy:0.12 loss: 229.81 (lr:0.0)\n",
      "3880: accuracy:0.15 loss: 230.568 (lr:0.0)\n",
      "3900: accuracy:0.11 loss: 230.893 (lr:0.0)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.1336 test loss: 230.071\n",
      "3920: accuracy:0.08 loss: 231.002 (lr:0.0)\n",
      "3940: accuracy:0.16 loss: 230.384 (lr:0.0)\n",
      "3960: accuracy:0.17 loss: 228.91 (lr:0.0)\n",
      "3980: accuracy:0.12 loss: 230.162 (lr:0.0)\n",
      "4000: accuracy:0.15 loss: 230.461 (lr:0.0)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4020: accuracy:0.18 loss: 229.751 (lr:0.0)\n",
      "4040: accuracy:0.12 loss: 230.871 (lr:0.0)\n",
      "4060: accuracy:0.17 loss: 229.128 (lr:0.0)\n",
      "4080: accuracy:0.15 loss: 229.828 (lr:0.0)\n",
      "4100: accuracy:0.1 loss: 230.122 (lr:0.0)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4120: accuracy:0.19 loss: 228.91 (lr:0.0)\n",
      "4140: accuracy:0.15 loss: 229.75 (lr:0.0)\n",
      "4160: accuracy:0.12 loss: 229.992 (lr:0.0)\n",
      "4180: accuracy:0.12 loss: 230.393 (lr:0.0)\n",
      "4200: accuracy:0.17 loss: 229.264 (lr:0.0)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4220: accuracy:0.16 loss: 229.375 (lr:0.0)\n",
      "4240: accuracy:0.14 loss: 230.306 (lr:0.0)\n",
      "4260: accuracy:0.11 loss: 231.179 (lr:0.0)\n",
      "4280: accuracy:0.26 loss: 227.824 (lr:0.0)\n",
      "4300: accuracy:0.16 loss: 230.197 (lr:0.0)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4320: accuracy:0.1 loss: 230.105 (lr:0.0)\n",
      "4340: accuracy:0.13 loss: 231.07 (lr:0.0)\n",
      "4360: accuracy:0.06 loss: 231.387 (lr:0.0)\n",
      "4380: accuracy:0.14 loss: 230.43 (lr:0.0)\n",
      "4400: accuracy:0.19 loss: 228.816 (lr:0.0)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4420: accuracy:0.18 loss: 228.872 (lr:0.0)\n",
      "4440: accuracy:0.09 loss: 230.437 (lr:0.0)\n",
      "4460: accuracy:0.17 loss: 229.477 (lr:0.0)\n",
      "4480: accuracy:0.13 loss: 230.378 (lr:0.0)\n",
      "4500: accuracy:0.14 loss: 230.591 (lr:0.0)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4520: accuracy:0.17 loss: 229.429 (lr:0.0)\n",
      "4540: accuracy:0.12 loss: 229.522 (lr:0.0)\n",
      "4560: accuracy:0.1 loss: 232.007 (lr:0.0)\n",
      "4580: accuracy:0.08 loss: 230.829 (lr:0.0)\n",
      "4600: accuracy:0.12 loss: 230.091 (lr:0.0)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4620: accuracy:0.14 loss: 229.893 (lr:0.0)\n",
      "4640: accuracy:0.14 loss: 230.231 (lr:0.0)\n",
      "4660: accuracy:0.1 loss: 231.492 (lr:0.0)\n",
      "4680: accuracy:0.1 loss: 231.266 (lr:0.0)\n",
      "4700: accuracy:0.19 loss: 229.437 (lr:0.0)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4720: accuracy:0.07 loss: 231.468 (lr:0.0)\n",
      "4740: accuracy:0.15 loss: 230.52 (lr:0.0)\n",
      "4760: accuracy:0.1 loss: 229.578 (lr:0.0)\n",
      "4780: accuracy:0.15 loss: 228.872 (lr:0.0)\n",
      "4800: accuracy:0.13 loss: 228.943 (lr:0.0)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4820: accuracy:0.11 loss: 230.185 (lr:0.0)\n",
      "4840: accuracy:0.13 loss: 230.484 (lr:0.0)\n",
      "4860: accuracy:0.14 loss: 230.208 (lr:0.0)\n",
      "4880: accuracy:0.12 loss: 229.769 (lr:0.0)\n",
      "4900: accuracy:0.16 loss: 229.384 (lr:0.0)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.1336 test loss: 230.071\n",
      "4920: accuracy:0.11 loss: 231.06 (lr:0.0)\n",
      "4940: accuracy:0.11 loss: 230.873 (lr:0.0)\n",
      "4960: accuracy:0.14 loss: 229.046 (lr:0.0)\n",
      "4980: accuracy:0.14 loss: 230.029 (lr:0.0)\n",
      "5000: accuracy:0.15 loss: 229.713 (lr:0.0)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5020: accuracy:0.13 loss: 230.151 (lr:0.0)\n",
      "5040: accuracy:0.15 loss: 229.569 (lr:0.0)\n",
      "5060: accuracy:0.14 loss: 230.662 (lr:0.0)\n",
      "5080: accuracy:0.13 loss: 229.609 (lr:0.0)\n",
      "5100: accuracy:0.15 loss: 229.47 (lr:0.0)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5120: accuracy:0.12 loss: 229.584 (lr:0.0)\n",
      "5140: accuracy:0.1 loss: 231.05 (lr:0.0)\n",
      "5160: accuracy:0.12 loss: 231.273 (lr:0.0)\n",
      "5180: accuracy:0.11 loss: 230.583 (lr:0.0)\n",
      "5200: accuracy:0.16 loss: 230.406 (lr:0.0)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5220: accuracy:0.12 loss: 230.001 (lr:0.0)\n",
      "5240: accuracy:0.19 loss: 228.776 (lr:0.0)\n",
      "5260: accuracy:0.13 loss: 229.951 (lr:0.0)\n",
      "5280: accuracy:0.11 loss: 229.94 (lr:0.0)\n",
      "5300: accuracy:0.1 loss: 231.135 (lr:0.0)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5320: accuracy:0.13 loss: 229.953 (lr:0.0)\n",
      "5340: accuracy:0.14 loss: 229.463 (lr:0.0)\n",
      "5360: accuracy:0.14 loss: 229.544 (lr:0.0)\n",
      "5380: accuracy:0.16 loss: 229.567 (lr:0.0)\n",
      "5400: accuracy:0.18 loss: 230.079 (lr:0.0)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5420: accuracy:0.12 loss: 230.074 (lr:0.0)\n",
      "5440: accuracy:0.09 loss: 229.695 (lr:0.0)\n",
      "5460: accuracy:0.1 loss: 230.776 (lr:0.0)\n",
      "5480: accuracy:0.06 loss: 231.6 (lr:0.0)\n",
      "5500: accuracy:0.17 loss: 230.041 (lr:0.0)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5520: accuracy:0.11 loss: 229.982 (lr:0.0)\n",
      "5540: accuracy:0.17 loss: 229.819 (lr:0.0)\n",
      "5560: accuracy:0.17 loss: 229.284 (lr:0.0)\n",
      "5580: accuracy:0.08 loss: 231.257 (lr:0.0)\n",
      "5600: accuracy:0.18 loss: 228.442 (lr:0.0)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5620: accuracy:0.14 loss: 231.001 (lr:0.0)\n",
      "5640: accuracy:0.14 loss: 230.089 (lr:0.0)\n",
      "5660: accuracy:0.14 loss: 229.206 (lr:0.0)\n",
      "5680: accuracy:0.15 loss: 229.594 (lr:0.0)\n",
      "5700: accuracy:0.11 loss: 230.364 (lr:0.0)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5720: accuracy:0.15 loss: 229.478 (lr:0.0)\n",
      "5740: accuracy:0.19 loss: 229.868 (lr:0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5760: accuracy:0.15 loss: 229.427 (lr:0.0)\n",
      "5780: accuracy:0.16 loss: 229.227 (lr:0.0)\n",
      "5800: accuracy:0.11 loss: 229.527 (lr:0.0)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5820: accuracy:0.17 loss: 230.128 (lr:0.0)\n",
      "5840: accuracy:0.15 loss: 230.213 (lr:0.0)\n",
      "5860: accuracy:0.12 loss: 229.504 (lr:0.0)\n",
      "5880: accuracy:0.15 loss: 230.011 (lr:0.0)\n",
      "5900: accuracy:0.08 loss: 230.556 (lr:0.0)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.1336 test loss: 230.071\n",
      "5920: accuracy:0.18 loss: 228.958 (lr:0.0)\n",
      "5940: accuracy:0.14 loss: 229.523 (lr:0.0)\n",
      "5960: accuracy:0.12 loss: 229.786 (lr:0.0)\n",
      "5980: accuracy:0.13 loss: 229.195 (lr:0.0)\n",
      "6000: accuracy:0.13 loss: 229.149 (lr:0.0)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6020: accuracy:0.14 loss: 229.437 (lr:0.0)\n",
      "6040: accuracy:0.19 loss: 229.368 (lr:0.0)\n",
      "6060: accuracy:0.1 loss: 230.35 (lr:0.0)\n",
      "6080: accuracy:0.13 loss: 230.119 (lr:0.0)\n",
      "6100: accuracy:0.12 loss: 230.535 (lr:0.0)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6120: accuracy:0.16 loss: 229.38 (lr:0.0)\n",
      "6140: accuracy:0.19 loss: 228.042 (lr:0.0)\n",
      "6160: accuracy:0.16 loss: 230.111 (lr:0.0)\n",
      "6180: accuracy:0.11 loss: 230.441 (lr:0.0)\n",
      "6200: accuracy:0.12 loss: 230.113 (lr:0.0)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6220: accuracy:0.17 loss: 229.127 (lr:0.0)\n",
      "6240: accuracy:0.12 loss: 230.282 (lr:0.0)\n",
      "6260: accuracy:0.12 loss: 229.694 (lr:0.0)\n",
      "6280: accuracy:0.08 loss: 230.59 (lr:0.0)\n",
      "6300: accuracy:0.12 loss: 230.474 (lr:0.0)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6320: accuracy:0.12 loss: 230.411 (lr:0.0)\n",
      "6340: accuracy:0.16 loss: 229.405 (lr:0.0)\n",
      "6360: accuracy:0.13 loss: 230.465 (lr:0.0)\n",
      "6380: accuracy:0.14 loss: 230.523 (lr:0.0)\n",
      "6400: accuracy:0.1 loss: 230.717 (lr:0.0)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6420: accuracy:0.14 loss: 229.867 (lr:0.0)\n",
      "6440: accuracy:0.13 loss: 230.268 (lr:0.0)\n",
      "6460: accuracy:0.13 loss: 229.907 (lr:0.0)\n",
      "6480: accuracy:0.13 loss: 230.709 (lr:0.0)\n",
      "6500: accuracy:0.2 loss: 229.129 (lr:0.0)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6520: accuracy:0.05 loss: 231.069 (lr:0.0)\n",
      "6540: accuracy:0.09 loss: 230.846 (lr:0.0)\n",
      "6560: accuracy:0.11 loss: 229.833 (lr:0.0)\n",
      "6580: accuracy:0.14 loss: 229.886 (lr:0.0)\n",
      "6600: accuracy:0.16 loss: 228.961 (lr:0.0)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6620: accuracy:0.14 loss: 229.385 (lr:0.0)\n",
      "6640: accuracy:0.11 loss: 230.169 (lr:0.0)\n",
      "6660: accuracy:0.15 loss: 230.073 (lr:0.0)\n",
      "6680: accuracy:0.12 loss: 229.866 (lr:0.0)\n",
      "6700: accuracy:0.16 loss: 228.501 (lr:0.0)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6720: accuracy:0.14 loss: 230.023 (lr:0.0)\n",
      "6740: accuracy:0.13 loss: 229.904 (lr:0.0)\n",
      "6760: accuracy:0.14 loss: 229.784 (lr:0.0)\n",
      "6780: accuracy:0.13 loss: 230.891 (lr:0.0)\n",
      "6800: accuracy:0.09 loss: 229.91 (lr:0.0)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6820: accuracy:0.11 loss: 230.552 (lr:0.0)\n",
      "6840: accuracy:0.07 loss: 230.522 (lr:0.0)\n",
      "6860: accuracy:0.12 loss: 230.655 (lr:0.0)\n",
      "6880: accuracy:0.12 loss: 230.424 (lr:0.0)\n",
      "6900: accuracy:0.14 loss: 230.094 (lr:0.0)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.1336 test loss: 230.071\n",
      "6920: accuracy:0.13 loss: 229.113 (lr:0.0)\n",
      "6940: accuracy:0.11 loss: 230.606 (lr:0.0)\n",
      "6960: accuracy:0.2 loss: 229.746 (lr:0.0)\n",
      "6980: accuracy:0.08 loss: 231.079 (lr:0.0)\n",
      "7000: accuracy:0.14 loss: 230.27 (lr:0.0)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7020: accuracy:0.1 loss: 231.031 (lr:0.0)\n",
      "7040: accuracy:0.11 loss: 230.17 (lr:0.0)\n",
      "7060: accuracy:0.15 loss: 229.45 (lr:0.0)\n",
      "7080: accuracy:0.13 loss: 229.615 (lr:0.0)\n",
      "7100: accuracy:0.11 loss: 229.754 (lr:0.0)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7120: accuracy:0.14 loss: 229.144 (lr:0.0)\n",
      "7140: accuracy:0.18 loss: 228.877 (lr:0.0)\n",
      "7160: accuracy:0.17 loss: 230.243 (lr:0.0)\n",
      "7180: accuracy:0.2 loss: 228.837 (lr:0.0)\n",
      "7200: accuracy:0.15 loss: 230.057 (lr:0.0)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7220: accuracy:0.17 loss: 229.03 (lr:0.0)\n",
      "7240: accuracy:0.12 loss: 230.207 (lr:0.0)\n",
      "7260: accuracy:0.1 loss: 230.246 (lr:0.0)\n",
      "7280: accuracy:0.1 loss: 230.481 (lr:0.0)\n",
      "7300: accuracy:0.14 loss: 230.98 (lr:0.0)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7320: accuracy:0.12 loss: 230.612 (lr:0.0)\n",
      "7340: accuracy:0.11 loss: 230.392 (lr:0.0)\n",
      "7360: accuracy:0.09 loss: 230.382 (lr:0.0)\n",
      "7380: accuracy:0.18 loss: 229.655 (lr:0.0)\n",
      "7400: accuracy:0.11 loss: 229.962 (lr:0.0)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7420: accuracy:0.12 loss: 230.329 (lr:0.0)\n",
      "7440: accuracy:0.13 loss: 229.499 (lr:0.0)\n",
      "7460: accuracy:0.14 loss: 230.151 (lr:0.0)\n",
      "7480: accuracy:0.15 loss: 230.301 (lr:0.0)\n",
      "7500: accuracy:0.09 loss: 230.569 (lr:0.0)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7520: accuracy:0.13 loss: 230.528 (lr:0.0)\n",
      "7540: accuracy:0.11 loss: 230.199 (lr:0.0)\n",
      "7560: accuracy:0.18 loss: 228.964 (lr:0.0)\n",
      "7580: accuracy:0.15 loss: 229.968 (lr:0.0)\n",
      "7600: accuracy:0.07 loss: 230.747 (lr:0.0)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7620: accuracy:0.1 loss: 231.447 (lr:0.0)\n",
      "7640: accuracy:0.1 loss: 230.703 (lr:0.0)\n",
      "7660: accuracy:0.14 loss: 230.003 (lr:0.0)\n",
      "7680: accuracy:0.11 loss: 230.122 (lr:0.0)\n",
      "7700: accuracy:0.15 loss: 230.147 (lr:0.0)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7720: accuracy:0.13 loss: 230.577 (lr:0.0)\n",
      "7740: accuracy:0.16 loss: 230.728 (lr:0.0)\n",
      "7760: accuracy:0.15 loss: 229.908 (lr:0.0)\n",
      "7780: accuracy:0.11 loss: 230.445 (lr:0.0)\n",
      "7800: accuracy:0.14 loss: 229.866 (lr:0.0)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7820: accuracy:0.09 loss: 230.687 (lr:0.0)\n",
      "7840: accuracy:0.11 loss: 230.389 (lr:0.0)\n",
      "7860: accuracy:0.16 loss: 228.981 (lr:0.0)\n",
      "7880: accuracy:0.16 loss: 228.549 (lr:0.0)\n",
      "7900: accuracy:0.12 loss: 230.518 (lr:0.0)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.1336 test loss: 230.071\n",
      "7920: accuracy:0.18 loss: 229.975 (lr:0.0)\n",
      "7940: accuracy:0.16 loss: 229.463 (lr:0.0)\n",
      "7960: accuracy:0.14 loss: 230.131 (lr:0.0)\n",
      "7980: accuracy:0.11 loss: 230.571 (lr:0.0)\n",
      "8000: accuracy:0.19 loss: 229.04 (lr:0.0)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8020: accuracy:0.16 loss: 229.319 (lr:0.0)\n",
      "8040: accuracy:0.14 loss: 229.027 (lr:0.0)\n",
      "8060: accuracy:0.11 loss: 230.249 (lr:0.0)\n",
      "8080: accuracy:0.13 loss: 230.892 (lr:0.0)\n",
      "8100: accuracy:0.2 loss: 228.957 (lr:0.0)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8120: accuracy:0.15 loss: 229.59 (lr:0.0)\n",
      "8140: accuracy:0.13 loss: 230.478 (lr:0.0)\n",
      "8160: accuracy:0.1 loss: 229.844 (lr:0.0)\n",
      "8180: accuracy:0.13 loss: 229.72 (lr:0.0)\n",
      "8200: accuracy:0.14 loss: 229.977 (lr:0.0)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8220: accuracy:0.15 loss: 230.122 (lr:0.0)\n",
      "8240: accuracy:0.12 loss: 230.228 (lr:0.0)\n",
      "8260: accuracy:0.21 loss: 229.144 (lr:0.0)\n",
      "8280: accuracy:0.1 loss: 231.61 (lr:0.0)\n",
      "8300: accuracy:0.08 loss: 230.998 (lr:0.0)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8320: accuracy:0.06 loss: 230.044 (lr:0.0)\n",
      "8340: accuracy:0.1 loss: 230.921 (lr:0.0)\n",
      "8360: accuracy:0.18 loss: 229.236 (lr:0.0)\n",
      "8380: accuracy:0.2 loss: 227.819 (lr:0.0)\n",
      "8400: accuracy:0.18 loss: 227.349 (lr:0.0)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8420: accuracy:0.16 loss: 230.156 (lr:0.0)\n",
      "8440: accuracy:0.12 loss: 230.17 (lr:0.0)\n",
      "8460: accuracy:0.09 loss: 230.609 (lr:0.0)\n",
      "8480: accuracy:0.09 loss: 230.301 (lr:0.0)\n",
      "8500: accuracy:0.16 loss: 231.215 (lr:0.0)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8520: accuracy:0.2 loss: 228.678 (lr:0.0)\n",
      "8540: accuracy:0.16 loss: 230.269 (lr:0.0)\n",
      "8560: accuracy:0.15 loss: 230.173 (lr:0.0)\n",
      "8580: accuracy:0.14 loss: 229.754 (lr:0.0)\n",
      "8600: accuracy:0.12 loss: 230.376 (lr:0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600: ********* epoch 15 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8620: accuracy:0.12 loss: 229.921 (lr:0.0)\n",
      "8640: accuracy:0.1 loss: 230.93 (lr:0.0)\n",
      "8660: accuracy:0.19 loss: 229.834 (lr:0.0)\n",
      "8680: accuracy:0.22 loss: 228.815 (lr:0.0)\n",
      "8700: accuracy:0.15 loss: 230.303 (lr:0.0)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8720: accuracy:0.15 loss: 229.509 (lr:0.0)\n",
      "8740: accuracy:0.1 loss: 230.163 (lr:0.0)\n",
      "8760: accuracy:0.07 loss: 230.761 (lr:0.0)\n",
      "8780: accuracy:0.19 loss: 228.623 (lr:0.0)\n",
      "8800: accuracy:0.16 loss: 229.475 (lr:0.0)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8820: accuracy:0.12 loss: 229.793 (lr:0.0)\n",
      "8840: accuracy:0.15 loss: 229.794 (lr:0.0)\n",
      "8860: accuracy:0.14 loss: 229.294 (lr:0.0)\n",
      "8880: accuracy:0.16 loss: 229.497 (lr:0.0)\n",
      "8900: accuracy:0.14 loss: 228.966 (lr:0.0)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.1336 test loss: 230.071\n",
      "8920: accuracy:0.09 loss: 230.197 (lr:0.0)\n",
      "8940: accuracy:0.14 loss: 229.292 (lr:0.0)\n",
      "8960: accuracy:0.13 loss: 230.222 (lr:0.0)\n",
      "8980: accuracy:0.15 loss: 230.328 (lr:0.0)\n",
      "9000: accuracy:0.12 loss: 229.722 (lr:0.0)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9020: accuracy:0.15 loss: 229.846 (lr:0.0)\n",
      "9040: accuracy:0.15 loss: 229.627 (lr:0.0)\n",
      "9060: accuracy:0.19 loss: 229.629 (lr:0.0)\n",
      "9080: accuracy:0.16 loss: 229.985 (lr:0.0)\n",
      "9100: accuracy:0.11 loss: 230.175 (lr:0.0)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9120: accuracy:0.2 loss: 228.592 (lr:0.0)\n",
      "9140: accuracy:0.14 loss: 229.93 (lr:0.0)\n",
      "9160: accuracy:0.13 loss: 230.047 (lr:0.0)\n",
      "9180: accuracy:0.1 loss: 230.663 (lr:0.0)\n",
      "9200: accuracy:0.13 loss: 230.419 (lr:0.0)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9220: accuracy:0.13 loss: 229.42 (lr:0.0)\n",
      "9240: accuracy:0.15 loss: 229.312 (lr:0.0)\n",
      "9260: accuracy:0.06 loss: 231.668 (lr:0.0)\n",
      "9280: accuracy:0.2 loss: 228.857 (lr:0.0)\n",
      "9300: accuracy:0.14 loss: 230.186 (lr:0.0)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9320: accuracy:0.12 loss: 231.355 (lr:0.0)\n",
      "9340: accuracy:0.11 loss: 230.735 (lr:0.0)\n",
      "9360: accuracy:0.12 loss: 230.998 (lr:0.0)\n",
      "9380: accuracy:0.16 loss: 229.634 (lr:0.0)\n",
      "9400: accuracy:0.11 loss: 229.442 (lr:0.0)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9420: accuracy:0.14 loss: 230.563 (lr:0.0)\n",
      "9440: accuracy:0.08 loss: 231.607 (lr:0.0)\n",
      "9460: accuracy:0.15 loss: 229.662 (lr:0.0)\n",
      "9480: accuracy:0.15 loss: 229.65 (lr:0.0)\n",
      "9500: accuracy:0.17 loss: 229.12 (lr:0.0)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9520: accuracy:0.13 loss: 229.304 (lr:0.0)\n",
      "9540: accuracy:0.12 loss: 229.888 (lr:0.0)\n",
      "9560: accuracy:0.11 loss: 229.974 (lr:0.0)\n",
      "9580: accuracy:0.14 loss: 230.23 (lr:0.0)\n",
      "9600: accuracy:0.1 loss: 230.856 (lr:0.0)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9620: accuracy:0.1 loss: 230.367 (lr:0.0)\n",
      "9640: accuracy:0.12 loss: 229.891 (lr:0.0)\n",
      "9660: accuracy:0.1 loss: 231.361 (lr:0.0)\n",
      "9680: accuracy:0.13 loss: 230.197 (lr:0.0)\n",
      "9700: accuracy:0.14 loss: 230.047 (lr:0.0)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9720: accuracy:0.13 loss: 230.291 (lr:0.0)\n",
      "9740: accuracy:0.19 loss: 229.483 (lr:0.0)\n",
      "9760: accuracy:0.05 loss: 231.446 (lr:0.0)\n",
      "9780: accuracy:0.18 loss: 228.642 (lr:0.0)\n",
      "9800: accuracy:0.15 loss: 230.048 (lr:0.0)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9820: accuracy:0.09 loss: 230.671 (lr:0.0)\n",
      "9840: accuracy:0.18 loss: 228.625 (lr:0.0)\n",
      "9860: accuracy:0.12 loss: 229.631 (lr:0.0)\n",
      "9880: accuracy:0.13 loss: 229.584 (lr:0.0)\n",
      "9900: accuracy:0.13 loss: 229.582 (lr:0.0)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.1336 test loss: 230.071\n",
      "9920: accuracy:0.15 loss: 229.662 (lr:0.0)\n",
      "9940: accuracy:0.12 loss: 230.104 (lr:0.0)\n",
      "9960: accuracy:0.06 loss: 231.759 (lr:0.0)\n",
      "9980: accuracy:0.13 loss: 230.257 (lr:0.0)\n",
      "10000: accuracy:0.14 loss: 229.145 (lr:0.0)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.1336 test loss: 230.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:38<02:34, 38.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.11 loss: 230.155 (lr:0.38)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1326 test loss: 230.021\n",
      "20: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "40: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "60: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "80: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "100: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "100: ********* epoch 1 ********* test accuracy:0.1028 test loss: 235.83\n",
      "120: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "140: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "160: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "180: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "200: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "200: ********* epoch 1 ********* test accuracy:0.1028 test loss: 235.83\n",
      "220: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "240: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "260: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "280: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "300: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "300: ********* epoch 1 ********* test accuracy:0.1028 test loss: 235.83\n",
      "320: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "340: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "360: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "380: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "400: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "400: ********* epoch 1 ********* test accuracy:0.1028 test loss: 235.83\n",
      "420: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "440: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "460: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "480: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "500: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "500: ********* epoch 1 ********* test accuracy:0.1028 test loss: 235.83\n",
      "520: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "540: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "560: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "580: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "600: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "600: ********* epoch 2 ********* test accuracy:0.1028 test loss: 235.83\n",
      "620: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "640: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "660: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "680: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "700: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "700: ********* epoch 2 ********* test accuracy:0.1028 test loss: 235.83\n",
      "720: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "740: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "760: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "780: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "800: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "800: ********* epoch 2 ********* test accuracy:0.1028 test loss: 235.83\n",
      "820: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "840: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "860: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "880: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "900: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "900: ********* epoch 2 ********* test accuracy:0.1028 test loss: 235.83\n",
      "920: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "940: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "960: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "980: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "1000: accuracy:0.18 loss: 228.115 (lr:0.38)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1020: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "1040: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "1060: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "1080: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "1100: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1120: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "1140: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "1160: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "1180: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "1200: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1220: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "1240: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "1260: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "1280: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "1300: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1320: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "1340: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "1360: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "1380: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "1400: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1420: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "1440: accuracy:0.04 loss: 242.115 (lr:0.38)\n",
      "1460: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "1480: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "1500: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1520: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "1540: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "1560: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "1580: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "1600: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1620: accuracy:0.19 loss: 227.115 (lr:0.38)\n",
      "1640: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "1660: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "1680: accuracy:0.17 loss: 229.115 (lr:0.38)\n",
      "1700: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1720: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "1740: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "1760: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "1780: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "1800: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1820: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "1840: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "1860: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "1880: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "1900: accuracy:0.03 loss: 243.115 (lr:0.38)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "1920: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "1940: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "1960: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "1980: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "2000: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2020: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "2040: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "2060: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "2080: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "2100: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2120: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "2140: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "2160: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "2180: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "2200: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2220: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "2240: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "2260: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "2280: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2300: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2320: accuracy:0.21 loss: 225.115 (lr:0.38)\n",
      "2340: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "2360: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "2380: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "2400: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2420: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "2440: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "2460: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "2480: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "2500: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2520: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "2540: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "2560: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "2580: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "2600: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2620: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2640: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "2660: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "2680: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "2700: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2720: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2740: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2760: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "2780: accuracy:0.17 loss: 229.115 (lr:0.38)\n",
      "2800: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2820: accuracy:0.06 loss: 240.115 (lr:0.38)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2840: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "2860: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "2880: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "2900: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.1028 test loss: 235.83\n",
      "2920: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "2940: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "2960: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "2980: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3000: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3020: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "3040: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "3060: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "3080: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "3100: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3120: accuracy:0.18 loss: 228.115 (lr:0.38)\n",
      "3140: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "3160: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "3180: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "3200: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3220: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "3240: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "3260: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "3280: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "3300: accuracy:0.17 loss: 229.115 (lr:0.38)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3320: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "3340: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "3360: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3380: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "3400: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3420: accuracy:0.17 loss: 229.115 (lr:0.38)\n",
      "3440: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "3460: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "3480: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "3500: accuracy:0.03 loss: 243.115 (lr:0.38)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3520: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3540: accuracy:0.2 loss: 226.115 (lr:0.38)\n",
      "3560: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3580: accuracy:0.04 loss: 242.115 (lr:0.38)\n",
      "3600: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3620: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3640: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "3660: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3680: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "3700: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3720: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "3740: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "3760: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "3780: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "3800: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3820: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3840: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "3860: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "3880: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "3900: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "3920: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "3940: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "3960: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "3980: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "4000: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4020: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "4040: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "4060: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "4080: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "4100: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4120: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "4140: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "4160: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "4180: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "4200: accuracy:0.03 loss: 243.115 (lr:0.38)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4220: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "4240: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "4260: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "4280: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "4300: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4320: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "4340: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "4360: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "4380: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "4400: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4420: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "4440: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "4460: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "4480: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "4500: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4520: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "4540: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "4560: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "4580: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "4600: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4620: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "4640: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "4660: accuracy:0.18 loss: 228.115 (lr:0.38)\n",
      "4680: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "4700: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4720: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "4740: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "4760: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "4780: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "4800: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4820: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "4840: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "4860: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "4880: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "4900: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "4920: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "4940: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "4960: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "4980: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "5000: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5020: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "5040: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "5060: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "5080: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "5100: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5120: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "5140: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "5160: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "5180: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "5200: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5220: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "5240: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "5260: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "5280: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "5300: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5320: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "5340: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "5360: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "5380: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "5400: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5420: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "5440: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "5460: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "5480: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "5500: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5520: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "5540: accuracy:0.17 loss: 229.115 (lr:0.38)\n",
      "5560: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "5580: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "5600: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5620: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "5640: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "5660: accuracy:0.22 loss: 224.115 (lr:0.38)\n",
      "5680: accuracy:0.18 loss: 228.115 (lr:0.38)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5700: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5720: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "5740: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "5760: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "5780: accuracy:0.03 loss: 243.115 (lr:0.38)\n",
      "5800: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5820: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "5840: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "5860: accuracy:0.17 loss: 229.115 (lr:0.38)\n",
      "5880: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "5900: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.1028 test loss: 235.83\n",
      "5920: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "5940: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "5960: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "5980: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "6000: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6020: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "6040: accuracy:0.2 loss: 226.115 (lr:0.38)\n",
      "6060: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6080: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6100: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6120: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "6140: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "6160: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "6180: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "6200: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6220: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "6240: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "6260: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "6280: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "6300: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6320: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6340: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "6360: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "6380: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "6400: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6420: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "6440: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "6460: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "6480: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "6500: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6520: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "6540: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "6560: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "6580: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6600: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6620: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "6640: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "6660: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6680: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "6700: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6720: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6740: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6760: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6780: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "6800: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6820: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6840: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "6860: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "6880: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "6900: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "6920: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "6940: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "6960: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "6980: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "7000: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7020: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "7040: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "7060: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7080: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "7100: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7120: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "7140: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "7160: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7180: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "7200: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7220: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "7240: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "7260: accuracy:0.04 loss: 242.115 (lr:0.38)\n",
      "7280: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "7300: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7320: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "7340: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "7360: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "7380: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "7400: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7420: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7440: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "7460: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "7480: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "7500: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7520: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "7540: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "7560: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "7580: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7600: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7620: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7640: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "7660: accuracy:0.04 loss: 242.115 (lr:0.38)\n",
      "7680: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7700: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7720: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "7740: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "7760: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7780: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "7800: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7820: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "7840: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "7860: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "7880: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "7900: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "7920: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "7940: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "7960: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "7980: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8000: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8020: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8040: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "8060: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "8080: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "8100: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8120: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "8140: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "8160: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "8180: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8200: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8220: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "8240: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "8260: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8280: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "8300: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8320: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "8340: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "8360: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "8380: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "8400: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8420: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "8440: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "8460: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "8480: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "8500: accuracy:0.11 loss: 235.115 (lr:0.38)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8520: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "8540: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8560: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8580: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "8600: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8620: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "8640: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "8660: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8680: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8700: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8720: accuracy:0.17 loss: 229.115 (lr:0.38)\n",
      "8740: accuracy:0.04 loss: 242.115 (lr:0.38)\n",
      "8760: accuracy:0.17 loss: 229.115 (lr:0.38)\n",
      "8780: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "8800: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8820: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "8840: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "8860: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "8880: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "8900: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.1028 test loss: 235.83\n",
      "8920: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "8940: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "8960: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "8980: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "9000: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9020: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "9040: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "9060: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "9080: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "9100: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9120: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "9140: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "9160: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "9180: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9200: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9220: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "9240: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "9260: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "9280: accuracy:0.08 loss: 238.115 (lr:0.38)\n",
      "9300: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9320: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "9340: accuracy:0.16 loss: 230.115 (lr:0.38)\n",
      "9360: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "9380: accuracy:0.12 loss: 234.115 (lr:0.38)\n",
      "9400: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9420: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "9440: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9460: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9480: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "9500: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9520: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "9540: accuracy:0.15 loss: 231.115 (lr:0.38)\n",
      "9560: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "9580: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "9600: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9620: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9640: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "9660: accuracy:0.11 loss: 235.115 (lr:0.38)\n",
      "9680: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "9700: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9720: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "9740: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9760: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "9780: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "9800: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9820: accuracy:0.05 loss: 241.115 (lr:0.38)\n",
      "9840: accuracy:0.07 loss: 239.115 (lr:0.38)\n",
      "9860: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "9880: accuracy:0.13 loss: 233.115 (lr:0.38)\n",
      "9900: accuracy:0.14 loss: 232.115 (lr:0.38)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n",
      "9920: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "9940: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "9960: accuracy:0.09 loss: 237.115 (lr:0.38)\n",
      "9980: accuracy:0.1 loss: 236.115 (lr:0.38)\n",
      "10000: accuracy:0.06 loss: 240.115 (lr:0.38)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.1028 test loss: 235.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [01:15<01:53, 37.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.1 loss: 230.736 (lr:0.2)\n",
      "0: ********* epoch 1 ********* test accuracy:0.0932 test loss: 230.609\n",
      "20: accuracy:0.1 loss: 236.115 (lr:0.2)\n",
      "40: accuracy:0.1 loss: 236.115 (lr:0.2)\n",
      "60: accuracy:0.1 loss: 236.115 (lr:0.2)\n",
      "80: accuracy:0.06 loss: 240.115 (lr:0.2)\n",
      "100: accuracy:0.15 loss: 231.115 (lr:0.2)\n",
      "100: ********* epoch 1 ********* test accuracy:0.101 test loss: 236.01\n",
      "120: accuracy:0.13 loss: 233.115 (lr:0.2)\n",
      "140: accuracy:0.06 loss: 240.115 (lr:0.2)\n",
      "160: accuracy:0.05 loss: 241.115 (lr:0.2)\n",
      "180: accuracy:0.06 loss: 240.115 (lr:0.2)\n",
      "200: accuracy:0.11 loss: 235.115 (lr:0.2)\n",
      "200: ********* epoch 1 ********* test accuracy:0.101 test loss: 236.01\n",
      "220: accuracy:0.15 loss: 231.115 (lr:0.2)\n",
      "240: accuracy:0.09 loss: 237.115 (lr:0.2)\n",
      "260: accuracy:0.09 loss: 236.796 (lr:0.2)\n",
      "280: accuracy:0.28 loss: 217.911 (lr:0.2)\n",
      "300: accuracy:0.36 loss: 210.116 (lr:0.2)\n",
      "300: ********* epoch 1 ********* test accuracy:0.3918 test loss: 206.747\n",
      "320: accuracy:0.34 loss: 212.065 (lr:0.2)\n",
      "340: accuracy:0.38 loss: 207.795 (lr:0.2)\n",
      "360: accuracy:0.39 loss: 207.154 (lr:0.2)\n",
      "380: accuracy:0.5 loss: 195.984 (lr:0.2)\n",
      "400: accuracy:0.44 loss: 201.621 (lr:0.2)\n",
      "400: ********* epoch 1 ********* test accuracy:0.4493 test loss: 201.068\n",
      "420: accuracy:0.41 loss: 204.672 (lr:0.2)\n",
      "440: accuracy:0.37 loss: 209.013 (lr:0.2)\n",
      "460: accuracy:0.43 loss: 203.358 (lr:0.2)\n",
      "480: accuracy:0.45 loss: 200.985 (lr:0.2)\n",
      "500: accuracy:0.44 loss: 201.814 (lr:0.2)\n",
      "500: ********* epoch 1 ********* test accuracy:0.4732 test loss: 198.736\n",
      "520: accuracy:0.42 loss: 204.075 (lr:0.2)\n",
      "540: accuracy:0.53 loss: 193.038 (lr:0.2)\n",
      "560: accuracy:0.55 loss: 191.046 (lr:0.2)\n",
      "580: accuracy:0.47 loss: 199.014 (lr:0.2)\n",
      "600: accuracy:0.47 loss: 199.04 (lr:0.2)\n",
      "600: ********* epoch 2 ********* test accuracy:0.4683 test loss: 199.217\n",
      "620: accuracy:0.46 loss: 200.076 (lr:0.2)\n",
      "640: accuracy:0.42 loss: 203.892 (lr:0.2)\n",
      "660: accuracy:0.45 loss: 201.06 (lr:0.2)\n",
      "680: accuracy:0.5 loss: 196.156 (lr:0.2)\n",
      "700: accuracy:0.48 loss: 198.085 (lr:0.2)\n",
      "700: ********* epoch 2 ********* test accuracy:0.476 test loss: 198.453\n",
      "720: accuracy:0.45 loss: 201.082 (lr:0.2)\n",
      "740: accuracy:0.55 loss: 191.103 (lr:0.2)\n",
      "760: accuracy:0.44 loss: 202.086 (lr:0.2)\n",
      "780: accuracy:0.44 loss: 202.301 (lr:0.2)\n",
      "800: accuracy:0.42 loss: 204.065 (lr:0.2)\n",
      "800: ********* epoch 2 ********* test accuracy:0.4631 test loss: 199.774\n",
      "820: accuracy:0.5 loss: 196.04 (lr:0.2)\n",
      "840: accuracy:0.39 loss: 206.434 (lr:0.2)\n",
      "860: accuracy:0.51 loss: 195.063 (lr:0.2)\n",
      "880: accuracy:0.43 loss: 203.008 (lr:0.2)\n",
      "900: accuracy:0.5 loss: 196.666 (lr:0.2)\n",
      "900: ********* epoch 2 ********* test accuracy:0.4751 test loss: 198.553\n",
      "920: accuracy:0.47 loss: 199.065 (lr:0.2)\n",
      "940: accuracy:0.43 loss: 203.296 (lr:0.2)\n",
      "960: accuracy:0.48 loss: 197.746 (lr:0.2)\n",
      "980: accuracy:0.46 loss: 200.111 (lr:0.2)\n",
      "1000: accuracy:0.41 loss: 205.226 (lr:0.2)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.4342 test loss: 202.641\n",
      "1020: accuracy:0.49 loss: 197.091 (lr:0.2)\n",
      "1040: accuracy:0.47 loss: 199.094 (lr:0.2)\n",
      "1060: accuracy:0.47 loss: 199.112 (lr:0.2)\n",
      "1080: accuracy:0.54 loss: 192.124 (lr:0.2)\n",
      "1100: accuracy:0.53 loss: 193.095 (lr:0.2)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.4428 test loss: 201.798\n",
      "1120: accuracy:0.46 loss: 200.089 (lr:0.2)\n",
      "1140: accuracy:0.47 loss: 199.12 (lr:0.2)\n",
      "1160: accuracy:0.55 loss: 191.059 (lr:0.2)\n",
      "1180: accuracy:0.43 loss: 203.078 (lr:0.2)\n",
      "1200: accuracy:0.51 loss: 195.017 (lr:0.2)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.4726 test loss: 198.809\n",
      "1220: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "1240: accuracy:0.47 loss: 199.042 (lr:0.2)\n",
      "1260: accuracy:0.4 loss: 206.174 (lr:0.2)\n",
      "1280: accuracy:0.49 loss: 196.964 (lr:0.2)\n",
      "1300: accuracy:0.45 loss: 201.113 (lr:0.2)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.4466 test loss: 201.438\n",
      "1320: accuracy:0.43 loss: 203.089 (lr:0.2)\n",
      "1340: accuracy:0.54 loss: 192.018 (lr:0.2)\n",
      "1360: accuracy:0.48 loss: 198.08 (lr:0.2)\n",
      "1380: accuracy:0.41 loss: 204.946 (lr:0.2)\n",
      "1400: accuracy:0.47 loss: 199.113 (lr:0.2)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.4787 test loss: 198.214\n",
      "1420: accuracy:0.42 loss: 204.115 (lr:0.2)\n",
      "1440: accuracy:0.53 loss: 193.094 (lr:0.2)\n",
      "1460: accuracy:0.55 loss: 191.106 (lr:0.2)\n",
      "1480: accuracy:0.53 loss: 193.115 (lr:0.2)\n",
      "1500: accuracy:0.46 loss: 200.11 (lr:0.2)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.4561 test loss: 200.46\n",
      "1520: accuracy:0.43 loss: 203.32 (lr:0.2)\n",
      "1540: accuracy:0.51 loss: 195.113 (lr:0.2)\n",
      "1560: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "1580: accuracy:0.38 loss: 208.098 (lr:0.2)\n",
      "1600: accuracy:0.45 loss: 201.104 (lr:0.2)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.4644 test loss: 199.654\n",
      "1620: accuracy:0.44 loss: 202.028 (lr:0.2)\n",
      "1640: accuracy:0.47 loss: 199.113 (lr:0.2)\n",
      "1660: accuracy:0.41 loss: 205.082 (lr:0.2)\n",
      "1680: accuracy:0.37 loss: 209.114 (lr:0.2)\n",
      "1700: accuracy:0.35 loss: 211.115 (lr:0.2)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.4458 test loss: 201.52\n",
      "1720: accuracy:0.42 loss: 204.051 (lr:0.2)\n",
      "1740: accuracy:0.43 loss: 203.228 (lr:0.2)\n",
      "1760: accuracy:0.41 loss: 205.074 (lr:0.2)\n",
      "1780: accuracy:0.43 loss: 203.166 (lr:0.2)\n",
      "1800: accuracy:0.52 loss: 194.074 (lr:0.2)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.4731 test loss: 198.796\n",
      "1820: accuracy:0.42 loss: 204.115 (lr:0.2)\n",
      "1840: accuracy:0.39 loss: 207.116 (lr:0.2)\n",
      "1860: accuracy:0.5 loss: 196.237 (lr:0.2)\n",
      "1880: accuracy:0.46 loss: 200.079 (lr:0.2)\n",
      "1900: accuracy:0.51 loss: 195.094 (lr:0.2)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.4605 test loss: 200.055\n",
      "1920: accuracy:0.41 loss: 205.105 (lr:0.2)\n",
      "1940: accuracy:0.5 loss: 196.111 (lr:0.2)\n",
      "1960: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "1980: accuracy:0.5 loss: 196.08 (lr:0.2)\n",
      "2000: accuracy:0.49 loss: 197.075 (lr:0.2)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.462 test loss: 199.896\n",
      "2020: accuracy:0.48 loss: 198.077 (lr:0.2)\n",
      "2040: accuracy:0.56 loss: 190.084 (lr:0.2)\n",
      "2060: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "2080: accuracy:0.5 loss: 195.954 (lr:0.2)\n",
      "2100: accuracy:0.49 loss: 197.176 (lr:0.2)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.4697 test loss: 199.117\n",
      "2120: accuracy:0.43 loss: 203.046 (lr:0.2)\n",
      "2140: accuracy:0.52 loss: 194.518 (lr:0.2)\n",
      "2160: accuracy:0.44 loss: 202.114 (lr:0.2)\n",
      "2180: accuracy:0.52 loss: 194.111 (lr:0.2)\n",
      "2200: accuracy:0.5 loss: 196.404 (lr:0.2)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.475 test loss: 198.577\n",
      "2220: accuracy:0.53 loss: 193.057 (lr:0.2)\n",
      "2240: accuracy:0.45 loss: 201.111 (lr:0.2)\n",
      "2260: accuracy:0.46 loss: 200.087 (lr:0.2)\n",
      "2280: accuracy:0.46 loss: 200.09 (lr:0.2)\n",
      "2300: accuracy:0.48 loss: 197.974 (lr:0.2)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.466 test loss: 199.495\n",
      "2320: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "2340: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "2360: accuracy:0.53 loss: 193.099 (lr:0.2)\n",
      "2380: accuracy:0.39 loss: 207.143 (lr:0.2)\n",
      "2400: accuracy:0.41 loss: 204.688 (lr:0.2)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.4808 test loss: 198.001\n",
      "2420: accuracy:0.42 loss: 204.115 (lr:0.2)\n",
      "2440: accuracy:0.44 loss: 202.112 (lr:0.2)\n",
      "2460: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "2480: accuracy:0.41 loss: 204.769 (lr:0.2)\n",
      "2500: accuracy:0.42 loss: 204.101 (lr:0.2)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.4374 test loss: 202.344\n",
      "2520: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "2540: accuracy:0.4 loss: 206.103 (lr:0.2)\n",
      "2560: accuracy:0.41 loss: 205.081 (lr:0.2)\n",
      "2580: accuracy:0.4 loss: 206.086 (lr:0.2)\n",
      "2600: accuracy:0.41 loss: 205.036 (lr:0.2)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.4774 test loss: 198.376\n",
      "2620: accuracy:0.4 loss: 206.115 (lr:0.2)\n",
      "2640: accuracy:0.57 loss: 189.116 (lr:0.2)\n",
      "2660: accuracy:0.52 loss: 194.086 (lr:0.2)\n",
      "2680: accuracy:0.51 loss: 195.065 (lr:0.2)\n",
      "2700: accuracy:0.49 loss: 197.104 (lr:0.2)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.481 test loss: 198.005\n",
      "2720: accuracy:0.4 loss: 206.083 (lr:0.2)\n",
      "2740: accuracy:0.43 loss: 203.113 (lr:0.2)\n",
      "2760: accuracy:0.42 loss: 204.091 (lr:0.2)\n",
      "2780: accuracy:0.54 loss: 192.221 (lr:0.2)\n",
      "2800: accuracy:0.47 loss: 199.069 (lr:0.2)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.4813 test loss: 197.972\n",
      "2820: accuracy:0.51 loss: 195.085 (lr:0.2)\n",
      "2840: accuracy:0.35 loss: 211.099 (lr:0.2)\n",
      "2860: accuracy:0.47 loss: 199.055 (lr:0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2880: accuracy:0.45 loss: 201.106 (lr:0.2)\n",
      "2900: accuracy:0.42 loss: 204.089 (lr:0.2)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.4485 test loss: 201.258\n",
      "2920: accuracy:0.52 loss: 194.107 (lr:0.2)\n",
      "2940: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "2960: accuracy:0.54 loss: 192.099 (lr:0.2)\n",
      "2980: accuracy:0.5 loss: 196.083 (lr:0.2)\n",
      "3000: accuracy:0.49 loss: 197.243 (lr:0.2)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.4815 test loss: 197.948\n",
      "3020: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "3040: accuracy:0.38 loss: 208.114 (lr:0.2)\n",
      "3060: accuracy:0.5 loss: 195.908 (lr:0.2)\n",
      "3080: accuracy:0.42 loss: 204.066 (lr:0.2)\n",
      "3100: accuracy:0.5 loss: 196.114 (lr:0.2)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.4805 test loss: 198.042\n",
      "3120: accuracy:0.54 loss: 192.098 (lr:0.2)\n",
      "3140: accuracy:0.48 loss: 198.112 (lr:0.2)\n",
      "3160: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "3180: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "3200: accuracy:0.39 loss: 207.083 (lr:0.2)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.474 test loss: 198.702\n",
      "3220: accuracy:0.43 loss: 203.099 (lr:0.2)\n",
      "3240: accuracy:0.53 loss: 193.025 (lr:0.2)\n",
      "3260: accuracy:0.5 loss: 196.081 (lr:0.2)\n",
      "3280: accuracy:0.49 loss: 197.121 (lr:0.2)\n",
      "3300: accuracy:0.43 loss: 203.107 (lr:0.2)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.4684 test loss: 199.26\n",
      "3320: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "3340: accuracy:0.51 loss: 195.113 (lr:0.2)\n",
      "3360: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "3380: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "3400: accuracy:0.53 loss: 193.079 (lr:0.2)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.4828 test loss: 197.821\n",
      "3420: accuracy:0.42 loss: 204.111 (lr:0.2)\n",
      "3440: accuracy:0.46 loss: 200.112 (lr:0.2)\n",
      "3460: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "3480: accuracy:0.42 loss: 204.114 (lr:0.2)\n",
      "3500: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.4801 test loss: 198.089\n",
      "3520: accuracy:0.51 loss: 195.241 (lr:0.2)\n",
      "3540: accuracy:0.42 loss: 204.559 (lr:0.2)\n",
      "3560: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "3580: accuracy:0.57 loss: 189.115 (lr:0.2)\n",
      "3600: accuracy:0.48 loss: 198.08 (lr:0.2)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.4767 test loss: 198.42\n",
      "3620: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "3640: accuracy:0.36 loss: 210.177 (lr:0.2)\n",
      "3660: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "3680: accuracy:0.43 loss: 203.111 (lr:0.2)\n",
      "3700: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.4734 test loss: 198.767\n",
      "3720: accuracy:0.45 loss: 201.091 (lr:0.2)\n",
      "3740: accuracy:0.43 loss: 203.105 (lr:0.2)\n",
      "3760: accuracy:0.42 loss: 204.115 (lr:0.2)\n",
      "3780: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "3800: accuracy:0.5 loss: 196.041 (lr:0.2)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.4745 test loss: 198.65\n",
      "3820: accuracy:0.35 loss: 211.115 (lr:0.2)\n",
      "3840: accuracy:0.58 loss: 188.114 (lr:0.2)\n",
      "3860: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "3880: accuracy:0.46 loss: 200.08 (lr:0.2)\n",
      "3900: accuracy:0.49 loss: 197.111 (lr:0.2)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.4716 test loss: 198.939\n",
      "3920: accuracy:0.5 loss: 196.087 (lr:0.2)\n",
      "3940: accuracy:0.5 loss: 196.025 (lr:0.2)\n",
      "3960: accuracy:0.57 loss: 189.115 (lr:0.2)\n",
      "3980: accuracy:0.41 loss: 205.114 (lr:0.2)\n",
      "4000: accuracy:0.48 loss: 198.119 (lr:0.2)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.4557 test loss: 200.53\n",
      "4020: accuracy:0.47 loss: 198.621 (lr:0.2)\n",
      "4040: accuracy:0.44 loss: 202.048 (lr:0.2)\n",
      "4060: accuracy:0.46 loss: 200.113 (lr:0.2)\n",
      "4080: accuracy:0.49 loss: 197.112 (lr:0.2)\n",
      "4100: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.4459 test loss: 201.51\n",
      "4120: accuracy:0.55 loss: 191.115 (lr:0.2)\n",
      "4140: accuracy:0.47 loss: 199.034 (lr:0.2)\n",
      "4160: accuracy:0.36 loss: 210.1 (lr:0.2)\n",
      "4180: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "4200: accuracy:0.47 loss: 198.863 (lr:0.2)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.4779 test loss: 198.307\n",
      "4220: accuracy:0.46 loss: 200.105 (lr:0.2)\n",
      "4240: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "4260: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "4280: accuracy:0.6 loss: 186.115 (lr:0.2)\n",
      "4300: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.4752 test loss: 198.587\n",
      "4320: accuracy:0.5 loss: 196.113 (lr:0.2)\n",
      "4340: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "4360: accuracy:0.55 loss: 191.188 (lr:0.2)\n",
      "4380: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "4400: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.456 test loss: 200.503\n",
      "4420: accuracy:0.36 loss: 210.115 (lr:0.2)\n",
      "4440: accuracy:0.6 loss: 186.108 (lr:0.2)\n",
      "4460: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "4480: accuracy:0.55 loss: 191.1 (lr:0.2)\n",
      "4500: accuracy:0.46 loss: 200.146 (lr:0.2)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.4799 test loss: 198.112\n",
      "4520: accuracy:0.5 loss: 196.081 (lr:0.2)\n",
      "4540: accuracy:0.5 loss: 196.086 (lr:0.2)\n",
      "4560: accuracy:0.48 loss: 198.105 (lr:0.2)\n",
      "4580: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "4600: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.478 test loss: 198.289\n",
      "4620: accuracy:0.37 loss: 209.411 (lr:0.2)\n",
      "4640: accuracy:0.4 loss: 206.113 (lr:0.2)\n",
      "4660: accuracy:0.39 loss: 207.115 (lr:0.2)\n",
      "4680: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "4700: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.476 test loss: 198.506\n",
      "4720: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "4740: accuracy:0.4 loss: 206.114 (lr:0.2)\n",
      "4760: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "4780: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "4800: accuracy:0.4 loss: 206.115 (lr:0.2)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.4815 test loss: 197.957\n",
      "4820: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "4840: accuracy:0.54 loss: 192.106 (lr:0.2)\n",
      "4860: accuracy:0.47 loss: 199.117 (lr:0.2)\n",
      "4880: accuracy:0.47 loss: 199.108 (lr:0.2)\n",
      "4900: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.4845 test loss: 197.651\n",
      "4920: accuracy:0.37 loss: 208.808 (lr:0.2)\n",
      "4940: accuracy:0.47 loss: 199.08 (lr:0.2)\n",
      "4960: accuracy:0.45 loss: 201.111 (lr:0.2)\n",
      "4980: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "5000: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.4808 test loss: 198.032\n",
      "5020: accuracy:0.47 loss: 199.339 (lr:0.2)\n",
      "5040: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "5060: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "5080: accuracy:0.56 loss: 190.157 (lr:0.2)\n",
      "5100: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.479 test loss: 198.211\n",
      "5120: accuracy:0.43 loss: 203.114 (lr:0.2)\n",
      "5140: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "5160: accuracy:0.47 loss: 199.094 (lr:0.2)\n",
      "5180: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "5200: accuracy:0.53 loss: 193.115 (lr:0.2)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.4722 test loss: 198.884\n",
      "5220: accuracy:0.44 loss: 202.104 (lr:0.2)\n",
      "5240: accuracy:0.58 loss: 188.115 (lr:0.2)\n",
      "5260: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "5280: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "5300: accuracy:0.52 loss: 194.107 (lr:0.2)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.4711 test loss: 199.001\n",
      "5320: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "5340: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "5360: accuracy:0.43 loss: 203.113 (lr:0.2)\n",
      "5380: accuracy:0.57 loss: 189.113 (lr:0.2)\n",
      "5400: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.4795 test loss: 198.154\n",
      "5420: accuracy:0.42 loss: 204.114 (lr:0.2)\n",
      "5440: accuracy:0.42 loss: 204.101 (lr:0.2)\n",
      "5460: accuracy:0.42 loss: 204.113 (lr:0.2)\n",
      "5480: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "5500: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.4759 test loss: 198.513\n",
      "5520: accuracy:0.55 loss: 191.115 (lr:0.2)\n",
      "5540: accuracy:0.57 loss: 189.115 (lr:0.2)\n",
      "5560: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "5580: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "5600: accuracy:0.57 loss: 189.115 (lr:0.2)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.4805 test loss: 198.049\n",
      "5620: accuracy:0.47 loss: 199.109 (lr:0.2)\n",
      "5640: accuracy:0.39 loss: 207.094 (lr:0.2)\n",
      "5660: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "5680: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "5700: accuracy:0.55 loss: 191.115 (lr:0.2)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.4735 test loss: 198.761\n",
      "5720: accuracy:0.47 loss: 199.082 (lr:0.2)\n",
      "5740: accuracy:0.46 loss: 200.129 (lr:0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5760: accuracy:0.56 loss: 190.107 (lr:0.2)\n",
      "5780: accuracy:0.48 loss: 198.083 (lr:0.2)\n",
      "5800: accuracy:0.45 loss: 201.118 (lr:0.2)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.4817 test loss: 197.935\n",
      "5820: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "5840: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "5860: accuracy:0.54 loss: 192.115 (lr:0.2)\n",
      "5880: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "5900: accuracy:0.41 loss: 205.115 (lr:0.2)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.4848 test loss: 197.629\n",
      "5920: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "5940: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "5960: accuracy:0.46 loss: 200.095 (lr:0.2)\n",
      "5980: accuracy:0.57 loss: 189.115 (lr:0.2)\n",
      "6000: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.4744 test loss: 198.667\n",
      "6020: accuracy:0.44 loss: 202.12 (lr:0.2)\n",
      "6040: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "6060: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "6080: accuracy:0.5 loss: 196.073 (lr:0.2)\n",
      "6100: accuracy:0.41 loss: 205.115 (lr:0.2)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.455 test loss: 200.607\n",
      "6120: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "6140: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "6160: accuracy:0.4 loss: 206.077 (lr:0.2)\n",
      "6180: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "6200: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.4817 test loss: 197.93\n",
      "6220: accuracy:0.39 loss: 207.114 (lr:0.2)\n",
      "6240: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "6260: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "6280: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "6300: accuracy:0.59 loss: 187.115 (lr:0.2)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.4846 test loss: 197.646\n",
      "6320: accuracy:0.5 loss: 196.114 (lr:0.2)\n",
      "6340: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "6360: accuracy:0.56 loss: 190.115 (lr:0.2)\n",
      "6380: accuracy:0.5 loss: 196.114 (lr:0.2)\n",
      "6400: accuracy:0.53 loss: 193.086 (lr:0.2)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.4739 test loss: 198.719\n",
      "6420: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "6440: accuracy:0.43 loss: 203.114 (lr:0.2)\n",
      "6460: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "6480: accuracy:0.55 loss: 191.102 (lr:0.2)\n",
      "6500: accuracy:0.47 loss: 199.054 (lr:0.2)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.4784 test loss: 198.268\n",
      "6520: accuracy:0.5 loss: 196.108 (lr:0.2)\n",
      "6540: accuracy:0.44 loss: 202.1 (lr:0.2)\n",
      "6560: accuracy:0.48 loss: 198.104 (lr:0.2)\n",
      "6580: accuracy:0.43 loss: 203.11 (lr:0.2)\n",
      "6600: accuracy:0.42 loss: 204.122 (lr:0.2)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.4672 test loss: 199.394\n",
      "6620: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "6640: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "6660: accuracy:0.55 loss: 191.115 (lr:0.2)\n",
      "6680: accuracy:0.55 loss: 191.524 (lr:0.2)\n",
      "6700: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.4793 test loss: 198.172\n",
      "6720: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "6740: accuracy:0.54 loss: 192.115 (lr:0.2)\n",
      "6760: accuracy:0.57 loss: 189.115 (lr:0.2)\n",
      "6780: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "6800: accuracy:0.52 loss: 194.096 (lr:0.2)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.4781 test loss: 198.289\n",
      "6820: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "6840: accuracy:0.33 loss: 213.115 (lr:0.2)\n",
      "6860: accuracy:0.41 loss: 205.115 (lr:0.2)\n",
      "6880: accuracy:0.45 loss: 201.114 (lr:0.2)\n",
      "6900: accuracy:0.46 loss: 200.107 (lr:0.2)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.4775 test loss: 198.354\n",
      "6920: accuracy:0.42 loss: 204.115 (lr:0.2)\n",
      "6940: accuracy:0.55 loss: 191.115 (lr:0.2)\n",
      "6960: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "6980: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "7000: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.4808 test loss: 198.024\n",
      "7020: accuracy:0.5 loss: 196.109 (lr:0.2)\n",
      "7040: accuracy:0.51 loss: 195.09 (lr:0.2)\n",
      "7060: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "7080: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "7100: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.4647 test loss: 199.639\n",
      "7120: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "7140: accuracy:0.43 loss: 203.196 (lr:0.2)\n",
      "7160: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "7180: accuracy:0.41 loss: 205.115 (lr:0.2)\n",
      "7200: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.4751 test loss: 198.596\n",
      "7220: accuracy:0.36 loss: 210.41 (lr:0.2)\n",
      "7240: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "7260: accuracy:0.45 loss: 201.113 (lr:0.2)\n",
      "7280: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "7300: accuracy:0.48 loss: 198.021 (lr:0.2)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.478 test loss: 198.309\n",
      "7320: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "7340: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "7360: accuracy:0.57 loss: 189.115 (lr:0.2)\n",
      "7380: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "7400: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.4795 test loss: 198.157\n",
      "7420: accuracy:0.38 loss: 208.115 (lr:0.2)\n",
      "7440: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "7460: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "7480: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "7500: accuracy:0.35 loss: 211.115 (lr:0.2)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.4784 test loss: 198.264\n",
      "7520: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "7540: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "7560: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "7580: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "7600: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.483 test loss: 197.808\n",
      "7620: accuracy:0.42 loss: 204.113 (lr:0.2)\n",
      "7640: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "7660: accuracy:0.52 loss: 194.129 (lr:0.2)\n",
      "7680: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "7700: accuracy:0.59 loss: 187.115 (lr:0.2)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.4841 test loss: 197.697\n",
      "7720: accuracy:0.4 loss: 206.115 (lr:0.2)\n",
      "7740: accuracy:0.5 loss: 196.114 (lr:0.2)\n",
      "7760: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "7780: accuracy:0.56 loss: 190.115 (lr:0.2)\n",
      "7800: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.4789 test loss: 198.218\n",
      "7820: accuracy:0.41 loss: 205.115 (lr:0.2)\n",
      "7840: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "7860: accuracy:0.53 loss: 193.115 (lr:0.2)\n",
      "7880: accuracy:0.55 loss: 191.115 (lr:0.2)\n",
      "7900: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.4839 test loss: 197.718\n",
      "7920: accuracy:0.39 loss: 206.84 (lr:0.2)\n",
      "7940: accuracy:0.4 loss: 206.065 (lr:0.2)\n",
      "7960: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "7980: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "8000: accuracy:0.5 loss: 196.023 (lr:0.2)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.4764 test loss: 198.473\n",
      "8020: accuracy:0.41 loss: 205.094 (lr:0.2)\n",
      "8040: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "8060: accuracy:0.54 loss: 192.098 (lr:0.2)\n",
      "8080: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "8100: accuracy:0.39 loss: 207.115 (lr:0.2)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.4159 test loss: 204.522\n",
      "8120: accuracy:0.52 loss: 194.112 (lr:0.2)\n",
      "8140: accuracy:0.47 loss: 199.046 (lr:0.2)\n",
      "8160: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "8180: accuracy:0.55 loss: 191.097 (lr:0.2)\n",
      "8200: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.4746 test loss: 198.648\n",
      "8220: accuracy:0.41 loss: 205.115 (lr:0.2)\n",
      "8240: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "8260: accuracy:0.4 loss: 206.115 (lr:0.2)\n",
      "8280: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "8300: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.4848 test loss: 197.628\n",
      "8320: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "8340: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "8360: accuracy:0.5 loss: 196.086 (lr:0.2)\n",
      "8380: accuracy:0.43 loss: 203.079 (lr:0.2)\n",
      "8400: accuracy:0.43 loss: 203.114 (lr:0.2)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.4822 test loss: 197.889\n",
      "8420: accuracy:0.43 loss: 203.098 (lr:0.2)\n",
      "8440: accuracy:0.54 loss: 192.082 (lr:0.2)\n",
      "8460: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "8480: accuracy:0.41 loss: 205.105 (lr:0.2)\n",
      "8500: accuracy:0.46 loss: 200.08 (lr:0.2)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.4608 test loss: 200.028\n",
      "8520: accuracy:0.35 loss: 211.115 (lr:0.2)\n",
      "8540: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "8560: accuracy:0.53 loss: 193.115 (lr:0.2)\n",
      "8580: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "8600: accuracy:0.42 loss: 204.115 (lr:0.2)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.4668 test loss: 199.427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8620: accuracy:0.53 loss: 193.107 (lr:0.2)\n",
      "8640: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "8660: accuracy:0.48 loss: 198.064 (lr:0.2)\n",
      "8680: accuracy:0.55 loss: 191.115 (lr:0.2)\n",
      "8700: accuracy:0.6 loss: 186.115 (lr:0.2)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.4686 test loss: 199.251\n",
      "8720: accuracy:0.53 loss: 193.099 (lr:0.2)\n",
      "8740: accuracy:0.38 loss: 208.107 (lr:0.2)\n",
      "8760: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "8780: accuracy:0.48 loss: 198.13 (lr:0.2)\n",
      "8800: accuracy:0.54 loss: 192.124 (lr:0.2)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.4781 test loss: 198.302\n",
      "8820: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "8840: accuracy:0.5 loss: 196.101 (lr:0.2)\n",
      "8860: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "8880: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "8900: accuracy:0.61 loss: 185.115 (lr:0.2)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.4695 test loss: 199.156\n",
      "8920: accuracy:0.34 loss: 212.099 (lr:0.2)\n",
      "8940: accuracy:0.46 loss: 200.078 (lr:0.2)\n",
      "8960: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "8980: accuracy:0.38 loss: 208.115 (lr:0.2)\n",
      "9000: accuracy:0.55 loss: 191.115 (lr:0.2)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.4825 test loss: 197.86\n",
      "9020: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "9040: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "9060: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "9080: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "9100: accuracy:0.57 loss: 189.115 (lr:0.2)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.484 test loss: 197.712\n",
      "9120: accuracy:0.45 loss: 201.098 (lr:0.2)\n",
      "9140: accuracy:0.41 loss: 205.115 (lr:0.2)\n",
      "9160: accuracy:0.54 loss: 192.115 (lr:0.2)\n",
      "9180: accuracy:0.5 loss: 196.115 (lr:0.2)\n",
      "9200: accuracy:0.4 loss: 206.096 (lr:0.2)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.4814 test loss: 197.971\n",
      "9220: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "9240: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "9260: accuracy:0.56 loss: 190.115 (lr:0.2)\n",
      "9280: accuracy:0.44 loss: 202.089 (lr:0.2)\n",
      "9300: accuracy:0.4 loss: 206.079 (lr:0.2)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.4781 test loss: 198.29\n",
      "9320: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "9340: accuracy:0.43 loss: 203.115 (lr:0.2)\n",
      "9360: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "9380: accuracy:0.51 loss: 195.142 (lr:0.2)\n",
      "9400: accuracy:0.48 loss: 198.115 (lr:0.2)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.4559 test loss: 200.515\n",
      "9420: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "9440: accuracy:0.57 loss: 189.12 (lr:0.2)\n",
      "9460: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "9480: accuracy:0.38 loss: 208.115 (lr:0.2)\n",
      "9500: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.4118 test loss: 204.92\n",
      "9520: accuracy:0.46 loss: 200.149 (lr:0.2)\n",
      "9540: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "9560: accuracy:0.36 loss: 210.115 (lr:0.2)\n",
      "9580: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "9600: accuracy:0.31 loss: 215.114 (lr:0.2)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.4657 test loss: 199.538\n",
      "9620: accuracy:0.38 loss: 208.114 (lr:0.2)\n",
      "9640: accuracy:0.46 loss: 200.115 (lr:0.2)\n",
      "9660: accuracy:0.49 loss: 197.115 (lr:0.2)\n",
      "9680: accuracy:0.46 loss: 200.211 (lr:0.2)\n",
      "9700: accuracy:0.48 loss: 198.084 (lr:0.2)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.4782 test loss: 198.282\n",
      "9720: accuracy:0.46 loss: 200.104 (lr:0.2)\n",
      "9740: accuracy:0.48 loss: 198.114 (lr:0.2)\n",
      "9760: accuracy:0.4 loss: 206.115 (lr:0.2)\n",
      "9780: accuracy:0.44 loss: 202.091 (lr:0.2)\n",
      "9800: accuracy:0.51 loss: 195.115 (lr:0.2)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.4806 test loss: 198.044\n",
      "9820: accuracy:0.52 loss: 194.115 (lr:0.2)\n",
      "9840: accuracy:0.43 loss: 203.106 (lr:0.2)\n",
      "9860: accuracy:0.47 loss: 199.113 (lr:0.2)\n",
      "9880: accuracy:0.45 loss: 201.115 (lr:0.2)\n",
      "9900: accuracy:0.56 loss: 190.114 (lr:0.2)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.4798 test loss: 198.133\n",
      "9920: accuracy:0.46 loss: 200.112 (lr:0.2)\n",
      "9940: accuracy:0.44 loss: 202.115 (lr:0.2)\n",
      "9960: accuracy:0.47 loss: 199.115 (lr:0.2)\n",
      "9980: accuracy:0.42 loss: 204.343 (lr:0.2)\n",
      "10000: accuracy:0.56 loss: 190.085 (lr:0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [01:48<01:12, 36.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: ********* epoch 17 ********* test accuracy:0.4839 test loss: 197.716\n",
      "0: accuracy:0.1 loss: 230.797 (lr:0.58)\n",
      "0: ********* epoch 1 ********* test accuracy:0.0835 test loss: 230.822\n",
      "20: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "40: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "60: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "80: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "100: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "100: ********* epoch 1 ********* test accuracy:0.1135 test loss: 234.76\n",
      "120: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "140: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "160: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "180: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "200: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "200: ********* epoch 1 ********* test accuracy:0.1135 test loss: 234.76\n",
      "220: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "240: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "260: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "280: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "300: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "300: ********* epoch 1 ********* test accuracy:0.1135 test loss: 234.76\n",
      "320: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "340: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "360: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "380: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "400: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "400: ********* epoch 1 ********* test accuracy:0.1135 test loss: 234.76\n",
      "420: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "440: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "460: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "480: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "500: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "500: ********* epoch 1 ********* test accuracy:0.1135 test loss: 234.76\n",
      "520: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "540: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "560: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "580: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "600: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "600: ********* epoch 2 ********* test accuracy:0.1135 test loss: 234.76\n",
      "620: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "640: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "660: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "680: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "700: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "700: ********* epoch 2 ********* test accuracy:0.1135 test loss: 234.76\n",
      "720: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "740: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "760: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "780: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "800: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "800: ********* epoch 2 ********* test accuracy:0.1135 test loss: 234.76\n",
      "820: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "840: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "860: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "880: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "900: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "900: ********* epoch 2 ********* test accuracy:0.1135 test loss: 234.76\n",
      "920: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "940: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "960: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "980: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "1000: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1020: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1040: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "1060: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1080: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1100: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1120: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "1140: accuracy:0.18 loss: 228.115 (lr:0.58)\n",
      "1160: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "1180: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "1200: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1220: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "1240: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "1260: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "1280: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "1300: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1320: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "1340: accuracy:0.04 loss: 242.115 (lr:0.58)\n",
      "1360: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "1380: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "1400: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1420: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1440: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "1460: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "1480: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "1500: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1520: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "1540: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "1560: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "1580: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "1600: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1620: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1640: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "1660: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "1680: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "1700: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1720: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "1740: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "1760: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "1780: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "1800: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1820: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1840: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "1860: accuracy:0.04 loss: 242.115 (lr:0.58)\n",
      "1880: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1900: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.1135 test loss: 234.76\n",
      "1920: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1940: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "1960: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "1980: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "2000: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2020: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "2040: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2060: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "2080: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2100: accuracy:0.19 loss: 227.115 (lr:0.58)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2120: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2140: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "2160: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "2180: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "2200: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2220: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "2240: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "2260: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "2280: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "2300: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2320: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "2340: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "2360: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "2380: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "2400: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2420: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "2440: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "2460: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "2480: accuracy:0.23 loss: 223.115 (lr:0.58)\n",
      "2500: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2520: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "2540: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "2560: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2580: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "2600: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2620: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "2640: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2660: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "2680: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "2700: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2720: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "2740: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "2760: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "2780: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "2800: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.1135 test loss: 234.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2820: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "2840: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "2860: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "2880: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "2900: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.1135 test loss: 234.76\n",
      "2920: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "2940: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "2960: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "2980: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "3000: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3020: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "3040: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "3060: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "3080: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "3100: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3120: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "3140: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "3160: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "3180: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "3200: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3220: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "3240: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "3260: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "3280: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3300: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3320: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3340: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "3360: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "3380: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "3400: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3420: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "3440: accuracy:0.19 loss: 227.115 (lr:0.58)\n",
      "3460: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3480: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "3500: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3520: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "3540: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "3560: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3580: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3600: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3620: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "3640: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "3660: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "3680: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "3700: accuracy:0.03 loss: 243.115 (lr:0.58)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3720: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "3740: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "3760: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "3780: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3800: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3820: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3840: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "3860: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "3880: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "3900: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.1135 test loss: 234.76\n",
      "3920: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "3940: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "3960: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "3980: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "4000: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4020: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "4040: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "4060: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "4080: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "4100: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4120: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "4140: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "4160: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "4180: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "4200: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4220: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "4240: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "4260: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "4280: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "4300: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4320: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "4340: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "4360: accuracy:0.18 loss: 228.115 (lr:0.58)\n",
      "4380: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "4400: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4420: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "4440: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "4460: accuracy:0.19 loss: 227.115 (lr:0.58)\n",
      "4480: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "4500: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4520: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "4540: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "4560: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "4580: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "4600: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4620: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "4640: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "4660: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "4680: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "4700: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4720: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "4740: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "4760: accuracy:0.03 loss: 243.115 (lr:0.58)\n",
      "4780: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "4800: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4820: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "4840: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "4860: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "4880: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "4900: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.1135 test loss: 234.76\n",
      "4920: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "4940: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "4960: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "4980: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5000: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5020: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "5040: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "5060: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "5080: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5100: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5120: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "5140: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "5160: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "5180: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "5200: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5220: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5240: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "5260: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "5280: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5300: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5320: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "5340: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "5360: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "5380: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "5400: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5420: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5440: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "5460: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "5480: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "5500: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5520: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "5540: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "5560: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5580: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "5600: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5620: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "5640: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "5660: accuracy:0.1 loss: 236.115 (lr:0.58)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5680: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "5700: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5720: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "5740: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "5760: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "5780: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "5800: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5820: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "5840: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "5860: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "5880: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "5900: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.1135 test loss: 234.76\n",
      "5920: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "5940: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "5960: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "5980: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "6000: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6020: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6040: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "6060: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6080: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "6100: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6120: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "6140: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "6160: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "6180: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "6200: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6220: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "6240: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "6260: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "6280: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "6300: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6320: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "6340: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6360: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "6380: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6400: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6420: accuracy:0.04 loss: 242.115 (lr:0.58)\n",
      "6440: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "6460: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6480: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "6500: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6520: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6540: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "6560: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6580: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "6600: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6620: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "6640: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "6660: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "6680: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "6700: accuracy:0.02 loss: 244.115 (lr:0.58)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6720: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6740: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "6760: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "6780: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "6800: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6820: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "6840: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "6860: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "6880: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "6900: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.1135 test loss: 234.76\n",
      "6920: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "6940: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "6960: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "6980: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "7000: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7020: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "7040: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "7060: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "7080: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "7100: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7120: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "7140: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "7160: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7180: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7200: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7220: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "7240: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "7260: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "7280: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "7300: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7320: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "7340: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7360: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "7380: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "7400: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7420: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7440: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "7460: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "7480: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "7500: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7520: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "7540: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "7560: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "7580: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "7600: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7620: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "7640: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7660: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7680: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "7700: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7720: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "7740: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "7760: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "7780: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "7800: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7820: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "7840: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "7860: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "7880: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "7900: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.1135 test loss: 234.76\n",
      "7920: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "7940: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "7960: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "7980: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "8000: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8020: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "8040: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "8060: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "8080: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "8100: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8120: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "8140: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "8160: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "8180: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "8200: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8220: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "8240: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "8260: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "8280: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "8300: accuracy:0.18 loss: 228.115 (lr:0.58)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8320: accuracy:0.19 loss: 227.115 (lr:0.58)\n",
      "8340: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "8360: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "8380: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "8400: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8420: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "8440: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "8460: accuracy:0.1 loss: 236.115 (lr:0.58)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8480: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "8500: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8520: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "8540: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "8560: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "8580: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "8600: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8620: accuracy:0.05 loss: 241.115 (lr:0.58)\n",
      "8640: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "8660: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "8680: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "8700: accuracy:0.03 loss: 243.115 (lr:0.58)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8720: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "8740: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "8760: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "8780: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "8800: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8820: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "8840: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "8860: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "8880: accuracy:0.19 loss: 227.115 (lr:0.58)\n",
      "8900: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.1135 test loss: 234.76\n",
      "8920: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "8940: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "8960: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "8980: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "9000: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9020: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "9040: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "9060: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "9080: accuracy:0.17 loss: 229.115 (lr:0.58)\n",
      "9100: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9120: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "9140: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "9160: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "9180: accuracy:0.18 loss: 228.115 (lr:0.58)\n",
      "9200: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9220: accuracy:0.16 loss: 230.115 (lr:0.58)\n",
      "9240: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "9260: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9280: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9300: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9320: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "9340: accuracy:0.19 loss: 227.115 (lr:0.58)\n",
      "9360: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "9380: accuracy:0.15 loss: 231.115 (lr:0.58)\n",
      "9400: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9420: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9440: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "9460: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9480: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "9500: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9520: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "9540: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "9560: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "9580: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9600: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9620: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "9640: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "9660: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "9680: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "9700: accuracy:0.14 loss: 232.115 (lr:0.58)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9720: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "9740: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9760: accuracy:0.08 loss: 238.115 (lr:0.58)\n",
      "9780: accuracy:0.13 loss: 233.115 (lr:0.58)\n",
      "9800: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9820: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "9840: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "9860: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "9880: accuracy:0.1 loss: 236.115 (lr:0.58)\n",
      "9900: accuracy:0.12 loss: 234.115 (lr:0.58)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.1135 test loss: 234.76\n",
      "9920: accuracy:0.09 loss: 237.115 (lr:0.58)\n",
      "9940: accuracy:0.07 loss: 239.115 (lr:0.58)\n",
      "9960: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "9980: accuracy:0.11 loss: 235.115 (lr:0.58)\n",
      "10000: accuracy:0.06 loss: 240.115 (lr:0.58)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.1135 test loss: 234.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [02:20<00:35, 35.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.15 loss: 230.096 (lr:1.06)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1104 test loss: 230.636\n",
      "20: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "40: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "60: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "80: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "100: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "100: ********* epoch 1 ********* test accuracy:0.0974 test loss: 236.37\n",
      "120: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "140: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "160: accuracy:0.03 loss: 243.115 (lr:1.06)\n",
      "180: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "200: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "200: ********* epoch 1 ********* test accuracy:0.0974 test loss: 236.37\n",
      "220: accuracy:0.03 loss: 243.115 (lr:1.06)\n",
      "240: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "260: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "280: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "300: accuracy:0.21 loss: 225.115 (lr:1.06)\n",
      "300: ********* epoch 1 ********* test accuracy:0.0974 test loss: 236.37\n",
      "320: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "340: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "360: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "380: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "400: accuracy:0.17 loss: 229.115 (lr:1.06)\n",
      "400: ********* epoch 1 ********* test accuracy:0.0974 test loss: 236.37\n",
      "420: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "440: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "460: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "480: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "500: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "500: ********* epoch 1 ********* test accuracy:0.0974 test loss: 236.37\n",
      "520: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "540: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "560: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "580: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "600: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "600: ********* epoch 2 ********* test accuracy:0.0974 test loss: 236.37\n",
      "620: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "640: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "660: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "680: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "700: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "700: ********* epoch 2 ********* test accuracy:0.0974 test loss: 236.37\n",
      "720: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "740: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "760: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "780: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "800: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "800: ********* epoch 2 ********* test accuracy:0.0974 test loss: 236.37\n",
      "820: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "840: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "860: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "880: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "900: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "900: ********* epoch 2 ********* test accuracy:0.0974 test loss: 236.37\n",
      "920: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "940: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "960: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "980: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1000: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1020: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "1040: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1060: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "1080: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "1100: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1120: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "1140: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1160: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "1180: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "1200: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1220: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1240: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "1260: accuracy:0.18 loss: 228.115 (lr:1.06)\n",
      "1280: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "1300: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1320: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "1340: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "1360: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1380: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1400: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1420: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "1440: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "1460: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "1480: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "1500: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1520: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1540: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "1560: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "1580: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "1600: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1620: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "1640: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "1660: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "1680: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "1700: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1720: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "1740: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1760: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "1780: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "1800: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1820: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "1840: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "1860: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "1880: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "1900: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.0974 test loss: 236.37\n",
      "1920: accuracy:0.19 loss: 227.115 (lr:1.06)\n",
      "1940: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "1960: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "1980: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "2000: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2020: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "2040: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "2060: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "2080: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "2100: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2120: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "2140: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "2160: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "2180: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "2200: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2220: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "2240: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "2260: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "2280: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "2300: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2320: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "2340: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "2360: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "2380: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "2400: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2420: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "2440: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "2460: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "2480: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "2500: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2520: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "2540: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "2560: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "2580: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "2600: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2620: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "2640: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "2660: accuracy:0.03 loss: 243.115 (lr:1.06)\n",
      "2680: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "2700: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2720: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "2740: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "2760: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "2780: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "2800: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2820: accuracy:0.13 loss: 233.115 (lr:1.06)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2840: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "2860: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "2880: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "2900: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.0974 test loss: 236.37\n",
      "2920: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "2940: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "2960: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "2980: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "3000: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3020: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3040: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "3060: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "3080: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "3100: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3120: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "3140: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "3160: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "3180: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "3200: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3220: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "3240: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3260: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "3280: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "3300: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3320: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "3340: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "3360: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "3380: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "3400: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3420: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3440: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3460: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "3480: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "3500: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3520: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "3540: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3560: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3580: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "3600: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3620: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "3640: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "3660: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "3680: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "3700: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3720: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "3740: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "3760: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "3780: accuracy:0.03 loss: 243.115 (lr:1.06)\n",
      "3800: accuracy:0.19 loss: 227.115 (lr:1.06)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3820: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "3840: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "3860: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "3880: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "3900: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.0974 test loss: 236.37\n",
      "3920: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "3940: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "3960: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "3980: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "4000: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4020: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "4040: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4060: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "4080: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "4100: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4120: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "4140: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "4160: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "4180: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4200: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4220: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4240: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4260: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "4280: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "4300: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4320: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4340: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "4360: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4380: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "4400: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4420: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4440: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "4460: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "4480: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "4500: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4520: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "4540: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "4560: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "4580: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "4600: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4620: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "4640: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "4660: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4680: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4700: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4720: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "4740: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4760: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "4780: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4800: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4820: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "4840: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "4860: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "4880: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4900: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.0974 test loss: 236.37\n",
      "4920: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4940: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "4960: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "4980: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5000: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5020: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "5040: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5060: accuracy:0.03 loss: 243.115 (lr:1.06)\n",
      "5080: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "5100: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5120: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "5140: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5160: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "5180: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "5200: accuracy:0.18 loss: 228.115 (lr:1.06)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5220: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "5240: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "5260: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "5280: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "5300: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5320: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "5340: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "5360: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "5380: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "5400: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5420: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "5440: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "5460: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "5480: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "5500: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5520: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "5540: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "5560: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5580: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "5600: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5620: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "5640: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "5660: accuracy:0.1 loss: 236.115 (lr:1.06)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5680: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "5700: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5720: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "5740: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "5760: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "5780: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "5800: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5820: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "5840: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "5860: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "5880: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5900: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.0974 test loss: 236.37\n",
      "5920: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "5940: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5960: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "5980: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6000: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6020: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "6040: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "6060: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6080: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "6100: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6120: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "6140: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "6160: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "6180: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "6200: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6220: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "6240: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "6260: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "6280: accuracy:0.03 loss: 243.115 (lr:1.06)\n",
      "6300: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6320: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6340: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "6360: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "6380: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "6400: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6420: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6440: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "6460: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "6480: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "6500: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6520: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6540: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "6560: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6580: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "6600: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6620: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "6640: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "6660: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "6680: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "6700: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6720: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "6740: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "6760: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "6780: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6800: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6820: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "6840: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "6860: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "6880: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "6900: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.0974 test loss: 236.37\n",
      "6920: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "6940: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "6960: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "6980: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7000: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7020: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7040: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "7060: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "7080: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "7100: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7120: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "7140: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "7160: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "7180: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7200: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7220: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "7240: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "7260: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "7280: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "7300: accuracy:0.18 loss: 228.115 (lr:1.06)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7320: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "7340: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "7360: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "7380: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "7400: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7420: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "7440: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "7460: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "7480: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7500: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7520: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "7540: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "7560: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7580: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "7600: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7620: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "7640: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "7660: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7680: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "7700: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7720: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7740: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "7760: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "7780: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "7800: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7820: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "7840: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "7860: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "7880: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "7900: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.0974 test loss: 236.37\n",
      "7920: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "7940: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "7960: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "7980: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "8000: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8020: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "8040: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "8060: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "8080: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "8100: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8120: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "8140: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "8160: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "8180: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "8200: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8220: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "8240: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "8260: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "8280: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "8300: accuracy:0.17 loss: 229.115 (lr:1.06)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8320: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "8340: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "8360: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "8380: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "8400: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8420: accuracy:0.16 loss: 230.115 (lr:1.06)\n",
      "8440: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "8460: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "8480: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "8500: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.0974 test loss: 236.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8520: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "8540: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "8560: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "8580: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "8600: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8620: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "8640: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "8660: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "8680: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "8700: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8720: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "8740: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "8760: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "8780: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "8800: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8820: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "8840: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "8860: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "8880: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "8900: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.0974 test loss: 236.37\n",
      "8920: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "8940: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "8960: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "8980: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "9000: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9020: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9040: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "9060: accuracy:0.17 loss: 229.115 (lr:1.06)\n",
      "9080: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "9100: accuracy:0.03 loss: 243.115 (lr:1.06)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9120: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "9140: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "9160: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "9180: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "9200: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9220: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "9240: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "9260: accuracy:0.13 loss: 233.115 (lr:1.06)\n",
      "9280: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "9300: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9320: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "9340: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "9360: accuracy:0.04 loss: 242.115 (lr:1.06)\n",
      "9380: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9400: accuracy:0.14 loss: 232.115 (lr:1.06)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9420: accuracy:0.15 loss: 231.115 (lr:1.06)\n",
      "9440: accuracy:0.06 loss: 240.115 (lr:1.06)\n",
      "9460: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "9480: accuracy:0.03 loss: 243.115 (lr:1.06)\n",
      "9500: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9520: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "9540: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9560: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "9580: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9600: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9620: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "9640: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "9660: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "9680: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9700: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9720: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "9740: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9760: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "9780: accuracy:0.1 loss: 236.115 (lr:1.06)\n",
      "9800: accuracy:0.08 loss: 238.115 (lr:1.06)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9820: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9840: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "9860: accuracy:0.09 loss: 237.115 (lr:1.06)\n",
      "9880: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9900: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.0974 test loss: 236.37\n",
      "9920: accuracy:0.05 loss: 241.115 (lr:1.06)\n",
      "9940: accuracy:0.07 loss: 239.115 (lr:1.06)\n",
      "9960: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "9980: accuracy:0.11 loss: 235.115 (lr:1.06)\n",
      "10000: accuracy:0.12 loss: 234.115 (lr:1.06)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.0974 test loss: 236.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:54<00:00, 34.81s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW5+PHPM0tmsm8QEpKQAKJWRLCkChU3igrVutxu\nuFxtf7Xc1vbW9l5b29rrerXt7UtvW1tb7eZSbqlXsaVIxa1Kr4IVK6KoCAKBhAAhIZBlktm+vz8y\nMx1ClpnMmSWZ5/16zYvJmTPnfDk5Oc85z3cTYwxKKaWyjy3dBVBKKZUeGgCUUipLaQBQSqkspQFA\nKaWylAYApZTKUhoAlFIqS2kAUEqpLKUBQCmlspQGAKWUylKOdBdgOBMmTDD19fXpLoZSSo0Zr732\n2kFjzMRY1s3oAFBfX8/GjRvTXQyllBozRKQx1nU1BaSUUllKA4BSSmUpDQBKKZWlMroOQCmlRuLz\n+WhqaqK3tzfdRUkpt9tNTU0NTqdz1NvQAKCUGtOampooLCykvr4eEUl3cVLCGENbWxtNTU1MnTp1\n1NvRFJBSakzr7e2lvLw8ay7+ACJCeXl5wk89lgQAEfm1iBwQkbeG+PwcETksIptCr5ut2K9SSgFZ\ndfEPs+L/bFUK6EHgJ8DDw6zzV2PMRRbtTymlVIIsCQDGmHUiUm/FtpRSqRMMBunr68Pn8+Hz+fB6\nvQQCAfx+P8FgEGNM5DVQ9B3oYO8H3qEOdsdqxV2s3++nr68v4e0kwu12s3TpUh588MFImerr6/nQ\nhz7EE088Eff2RASn05n0J5tUVgLPF5E3gL3ADcaYLSnct1KK/srD7u5ujhw5QldXF16vF5vNFvls\nsAt9pgsGgwSDwbSWIT8/ny1bttDd3U1ubi7PPPMMVVVVGGNGXTaHw5H0AJCqSuC/A3XGmNnAvcAf\nhlpRRJaJyEYR2dja2pqi4ik1vvl8Pvbv38+7777L7t27aW9vx+v1Av+4gI7Fi38mueCCC3jqqacA\n+N///V8+9alPRT7r7u7mX/7lXzjzzDOZN28ef/rTnwBobGxk0aJFzJ8/n/nz57NhwwYA1q1bx8KF\nC/nEJz7BiSeeyJVXXpmU309KngCMMUei3q8RkftEZIIx5uAg6z4APADQ0NCgZ6RSCQg3F9y/f3/k\n5/Hs61//Ops3b7Z0m6eccgo/+MEPRlzvk5/8JN/97ndZsmQJb775JldffTUvvfQSAN///vc555xz\nuP/+++no6OCss85i4cKFTJw4kdWrV+N2u9m+fTvXXHNN5Duvv/46W7ZsYfLkyZxxxhm89NJLLFiw\nwNL/W0oCgIhUAvuNMUZETqP/yaMtFftWKlt5PB52796N3+8f9xf+TDBr1iwaGxt59NFHueCCC476\n7LnnnmPNmjX88Ic/BPqbru7Zs4eqqir+7d/+jc2bN2Oz2di+fXvkOx/60IeoqakBYM6cOezatSsz\nA4CI/A44B5ggIk3ALYATwBjzc+ATwBdFxA94gKVGz0ilkqazs5Pdu3dn3YU/ljv1ZLrwwgv59re/\nzVNPPUV7e3tkuTGG//mf/+H4448/av3//M//pKKigldeeYVgMEhpaWnkM5fLFXlvt9vx+/2Wl9eq\nVkCXj/D5T+hvJqqUSrL29nZaWlqy7uKfCa6++mpKSko4+eSTWbduXWT5okWL+NnPfsY999yDiLBp\n0ybmzJnDkSNHqK6uxmaz8dvf/pZAIJDS8mpPYKXGkdbWVr34p1FNTQ3XXXfdMcu/9a1v4fP5OO20\n05g7dy633347AMuWLWP58uWcfvrpvPfee+Tn56e0vJLJJ0pDQ4PRCWGUik1XVxeNjY1Zd/EPBALH\npFbGA5fLFWmiO5R33nmHD3zgA0ctE5HXjDENsexDnwCUGge8Xm9W5vxVYjQAKDXGBYNBGhsb094Z\nSo09GgCUGuP27t0b6dSlVDw0ACg1hnk8Hg4fPqypHzUqGgCUGqOMMTQ3N+vFX42aBgClxqjDhw+n\nfRRMNbZpAFBqDAoGg9reP8M0NTXxyU9+klmzZjFz5kxuuOEGvF4vjzzyCF/72tfSXbxBaQBQagw6\ncOCAtvoZJfuKFbhOOAF3fj6uE07AvmJFwts0xnD55ZfzsY99jDfffJPNmzfT1dXFrbfemniBk0gD\ngFJjTCAQoK2tTe/+R8G+YgXOL30J2549iDHY9uzB+aUvJRwEXnjhBdxuN1dffXX/fux2/uu//ouH\nH36Ynp4empqauOCCC5g1axZ33nkn0D9E9GWXXcbpp59OQ0MDjz32WML/v3ilckIYpZQFogcZU/Fx\n3HIL4vEctUw8Hhy33EJg6dJRb/edd97h1FNPPWpZUVERNTU1BAIBNm7cyMaNG8nLy+PMM89k8eLF\nkdFAwzOGHT58eNT7Hy19AlBqDDHGcPDgQb37HyVpaopruVU+8pGPUF5eTm5uLhdffDHr169n5syZ\nPP/883znO9/hpZdeori4OKllGIwGAKXGEG3znxgTGl8/1uWxOvHEE3n99dePWnbkyBGampqw2+2D\nzo88Y8YMXn75ZWbOnMltt93GXXfdlVAZRkMDgFJjhDFGK38T5L/tNkxu7lHLTG4u/ttuS2i75557\nLj09PSxfvhzor6f55je/yVVXXUVeXh7PPfcc7e3teDweVq9ezbx589i7dy95eXlcfvnlfPWrX2XT\npk0JlWE0NAAoNUZ0d3fj8/nSXYwxLbB0Kb6f/pRgbS1GhGBtLb6f/jSh/D/039H//ve/Z+XKlcya\nNYtTTjkFt9vNbaHA0tDQwBVXXMFpp53GJZdcwty5c9myZQtnnXUWp59+OnfddRc33nijFf/F+Mqd\nyY+TOhy0Uv+wY8cOenp60l2MjKPDQetw0EqNaz6fD8+A1itKJUoDgFJjQEdHR7qLoMYhDQBKjQHt\n7e3a+mcY2XhsrPg/awBQKsP19vbi9/vTXYyMJSJ0dHRkVRAwxtDW1obb7U5oO9oTWKkMp3f/wxMR\nDh48SGtra7qLYhkRweFwHNN/IJrb7aYmwf4LGgCUymDGGM3/j0BEhr1QjkU2m43p06fjcrmSu5+k\nbl0plZCurq50F0GNY5YEABH5tYgcEJG3hvhcROTHIrJdRDaLyAet2K9S4117e7v2/FVJY9UTwIPA\n4mE+XwLMCL2WAT+zaL9KjVvBYFCfAFRSWRIAjDHrgOHGqL0EeNj02wCUiEiVFftWarzq7u4ed7lt\nlVlSVQdQDeyJ+rkptOwYIrJMRDaKyMZMqNXv6+vTR3CVFocPH9ZzTyVVxlUCG2MeMMY0GGMaJk6c\nmNqdL18O9fUYm41AbS0td9/Ntm3beP/99/F6vakti8pqxhiOHDmS7mKocS5VAaAZqI36uSa0LHMs\nXw7LlkFjI2IM9qYmJn3nOxQ/+SR9fX1s376dzs7OdJdSZQkd90elQqoCwCrg6lBroHnAYWNMS4r2\nHZubboIBIy3aentx3nILXV1dBINBdu/erXdlWcAYk/bUy5EjR9JeBjX+WdIRTER+B5wDTBCRJuAW\nwAlgjPk5sAb4KLAd6AE+a8V+rWR272aw6raKvj4+feON/PjHP8Zut7N//34KCwu1cm6cCQaDNDY2\n4vF4Ihfe0tJSJk+enJbfdTrmh1XZx5IAYIy5fITPDfAlK/aVDMFgEH9lJTktxz6UHCkuZt26dfz3\nf/83N9xwA16vl56eHvLz89NQUpUMxhgaGxvp6ek5asiFjo4OAoEAtbW1KQ0CXq9Xx/5RKZFxlcDp\n0N7eziMf+ADdA5YH3W46v/UtLr/8ch566CFWrlwZmZZPjQ/GGJqamo65+Ic/6+zspLGxMaXpGE0z\nqlTJ+gAQDAZZs2YNn//LX7h/7ly8VVUYEQI1NbTedRdHLrqIb3zjG3z4wx/mjjvuoKWlhZ6eHnp7\ne9NddGWBffv2ceTIkSEHWzPG0N3dndKgn20jW6r0yfoA0NHRwT333MPkyZNZcN99bHvmGXq7u7Hv\n2cOkr32NKVOm4HQ6ueWWWzDGsHz5cn0KGCf6+vpiGmkzPPRuKtIygUCAvr6+pO9HKcjyAGCMYcOG\nDbzxxhssXbqUvLw8CgsLyc3NjaxTWFhIfn4+kydP5rzzzuPxxx+nq6uLzs5OnaB7jNu/f39cd9qp\n6JiovX9VKmV1ADhy5AgrV67Ebrdz0UUXISJUVlYes151dTUiwjXXXENXVxcrV64E4NChQ6kusrKI\n1+uNq1+HMYb29vakB/3Ozk5t/qlSJmsDgDGG5uZmVq1axZlnnsnEiRMpKysjJyfnmHWdTieTJk1i\n1qxZfPCDH2T58uX4fD5tqjeGHThwIO48eypSf9rZUKVS1gYAr9fLunXraG1t5ZJLLgGgoqJiyPXL\ny8txOBxcffXV7N27l2effVab641RiQTvjo6OpA0L4vV6CQQCSdm2UoPJ2gDQ2dnJH/7wB0pLSzn7\n7LPJy8vDbrcPuX44PbRw4UKmTJnCww8/HGkmqMaW0dz9hxljOHjwoMUl6qf5f5VqWRsAdu/ezfPP\nP8+FF15ITk4OJSUlI36nqKgIh8PBlVdeyZtvvsnbb7+taaAxJhAIJDzFYrKaaerwDyrVsjIABINB\nVq5cic/ni6R/CgsLR/yeiDBhwgQuvPBCHA4Ha9eupbu7W/9ox5DOzk5L7rK7uwd2G0xMuL+BUqmU\nlQGgu7ubNWvWcMIJJ3DiiSficrlwOGIbFaOsrIySkhJOP/101q5dC0DPgEHkVOayYorFYDBoeQsw\n7Vio0iErA0BzczObN2/mrLPOQkRiSv+E2e12iouLWbx4cWQ7mgYaG/x+v2XDLFudrunq6tLevyrl\nsjIAPPfccwQCAebPnw/05/bjMXHiRBYuXBhJAw03lIDKHEeOHLGsklVELG0AoOeQSoesCwBer5eX\nXnqJ3Nxc5syZg8PhGLTt/3BcLheVlZWcccYZrF27Fr/fr933xwAr0j9hwWCQ9vbhpsGOb1uaAlLp\nkHUBoKuri/Xr19PQ0IDT6Ywr/RNtwoQJLFmyhH379vHGG2+MueagwWCQlpYWGhsbsyJ4+Xw+y/+f\nPT09lrTb93g82vxTpUXWBYC3336bXbt2MX/+fGw2W9zpn7DCwkLOPfdccnJyImmgsaK3t5ft27fT\n3t5OZ2cn27dvp6mpaVyPbZSsehortqstyVS6ZFUAMMbw/PPPA0Ty/263e1TbEhFqa2tZsGABTz/9\nND09PWPij7ijoyMyyX0452yMoaOjgx07dozbPHQso37GyxhjSQAYa0+PavzIqgDQ29vLhg0bqKio\nYPr06eTn5yf06F1WVsbixYs5cOAAmzZtyvjmoIFAgL179w55IfT7/bS1taWkLD6fj7a2Nnbt2sXb\nb7/Nzp07k/YE4vP5krbtRAO/MUbz/yptsioAdHd3s2HDBubNm4fdbh91+ifM5XKxaNEiHA4Hf/nL\nX+jq6rKopMlx8ODBYe+Cw4OdJXt8I6/Xy7Zt29i3bx9dXV0Eg0G6u7vZtm1bUlI1ybzDFpGEAr/m\n/1U6ZVUAePXVVzl06BDz5s3DGGPJvL5Tp07ltNNO44UXXsjoegC/3z9iAID+yuF9+/YlrRyBQICd\nO3cSDAaPKUswGKSpqcnyETeT2cQyGAwmFGA0/6/SKasCwHPPPQf05//tdnvczT8HU1RUxNlnn83O\nnTvZvn17xo7muG/fvpgvgocPH7asw1S0YDDIrl27hk3HGGNobW21bMTNVAyxkEjg1/y/SqesCQDB\nYJD/+7//Y8aMGUyYMIGCggJLtmuz2bj44osBeOGFFzJyPJe+vr64UivGGPbu3Wt5OZqbm2PKd4cn\narfirr2npyfpKRa/3z+qOgZjTFICrVKxypoA0NXVxebNm2loaMBms8U0+FusTjnlFGbMmJGxaaB4\npz6E/gpzK8e993g8caViwusnKhUjbIrIqOp/ent7Nf+v0sqSACAii0Vkq4hsF5FvDvL5Z0SkVUQ2\nhV7XWrHfeGzcuBGPx8OcOXMsy/+H5ebmsnDhQv7+97/T3Nxs2XatkEiO2soBz1paWuIKQuGnkERT\naqkIyMFgcFSV193d3eO22a0aGxIOACJiB34KLAFOAi4XkZMGWfX3xpg5odcvE91vvF566SWAyPAP\nsY7+GQsR4eKLLyYQCPDiiy8mbcao0Rjt8MfGGA4dOmTJBaq7u3tUqY5gMJjQROw+ny9lM7aN5mLe\n2dmpAUCllRVPAKcB240xO4wxXmAFcIkF27XU3/72NyZOnEhVVZWl6Z+wc845h8/n53PNLbfgdLuh\nvh6WL7d8P/Fqa2sbdQokGAwm3LfBGBP33X/0dxMZv6erqytlKRYRiSvIaf5fZQIrAkA1sCfq56bQ\nsoE+LiKbReQxEam1YL8xCwQCvP7668yZMwe73W5ZBXC0vCee4Me9vUzq7UWMgcZGWLYsrUEg0eGP\nrRjwrKurK+ExeEabxjl8+HDKmlgGg8G4ytnX16d3/yrtUlUJ/Ceg3hhzCvAM8NBQK4rIMhHZKCIb\nE3n8j9bY2EhzczOzZ8+2PP8fcdNNuAfmq3t64KabrN9XjKzoVJVIJWoid/9hwWBwVHPwpmOGrXjq\nWjK917jKDlYEgGYg+o6+JrQswhjTZowJ3wb+Epg71MaMMQ8YYxqMMQ0TJ060oHjw17/+FYDZs2fj\ndDqHnfx91Hbvjm95Clgx/o2IjPoO3OPxWDIEQ19fX9zDJaSjh63X6405WOoEMCoTWBEAXgVmiMhU\nEckBlgKrolcQkaqoHy8G3rFgvzFbv349TqeTk046KSn5fwCmTIlveZL19fVZUhmdSBoolp7HsTDG\nxD1GUXiIiVSKZ1gIfQJQmSDhAGCM8QNfBtbSf2F/1BizRURuF5GLQ6t9RUS2iMgbwFeAzyS633i8\n+uqrfOADH8Dtdicl/w/AnXdCXt5Ri0xubv/yNOjo6LBsWx6PJ+7mmIFAwNJerh0dHXFd0NPRwzY8\nptFI/H5/xvYYV9nFkjoAY8waY8zxxpjpxpg7Q8tuNsasCr3/ljFmpjFmtjHmXGPMu1bsNxYej4e3\n3nor0v4/b8BF2jJXXgkPPIC/upog0F5YyL477sBccUVy9jeCjo4Oy1IMo+noZPWk6SISc51GOmfY\niiXwdHd3awcwlRHGfU/gV155Ba/Xm9z8f9iVV2Lfs4dTTzmFs+vqaF+8OC1N/axu/x5vCxdjjGXp\nn+gyxJoGSucIm319fSM+qegAcCpTjPsAEN0BLGnpnygiwnnnncdbb73F/v372b9/f9L3OVAy0h/x\ndFpK1gWur68vpsCWzgusiIz49JHpw4ar7DHuA8D69eupqqqisrIyJQEAiAwO9+KLL9LT05O06QiH\ncvjw4aS0MIk1rXLw4MGkXYBjCW7pHI8pGAwOe4EPBoMZ1VNcZbdxHwBee+215Lb/H8TcuXOpra3l\nhRdewBhDc3NzyoYksKL37lDbjeXC6vf7k9b+Pjw8xXCCwWDaJ7kfLkh5PB5stnH/Z6fGiHF9Jra0\ntLBv3z5mzZqV/Px/lLy8PM455xw2bNgQmTIwVYPEJbOCMZYnGStbHw1mpBZJqRj+eSS9vb1DPoFp\n/l9lknEdANavXw/AySefnLL0D/Tngc8//3y8Xi8vv/wy0J/3TUVqIpnDH8dSuZyMydejiciwd9jp\naP8/0HDjAmn+X2WScR8AbDYbJ510UkoDAMDChQspLS1l7dq1QH/6Ys+ePUkdnsAYk/QgM9zFt7e3\nN2mTr4cFg8FhnzIyYYatoYah0AHgVKYZ1wFg48aNTJ8+ndzc3JTl/8OKi4s5//zzIxXB0H8B2LVr\nV9LuAmNpgpgIY8ywaSCrho8eyVBplEypYDXGDBqI+vr60p6eUirauA0Axhg2bdrEySefjMPhSFn+\nPyw3N5clS5bg8Xh48cUXjypXY2Nj0ppqJttQF19jTNLz/2FDdUzLpA5WHo/nmGCo4/+oTDNuA8DO\nnTvp6OhIef4/zGazMW/ePCoqKvjzn/981GfGGHbv3k1jY6Old6xW9v4diogMmt5I5exWQ6WBUjH9\nY6wGqweIZ0pMpVLBummxMswrr7wCwKxZs9ISAKA/DXTBBRewYsUKjhw5QlFRUeSzcJqgq6uLsrIy\nysrKcLlco96Xz+cbNph4vV7Wr1/PK6+8gt/vx2az4XQ6mTNnDvPmzYs5RRae/nDgoHqJTNwyGp2d\nnQSDwaOaVGZC/j8s3B8gPPSI5v9VJhq3AWDDhg04nU5mzJiR8vx/WH5+Ph/96Ed55JFHeP7557n0\n0kuPWSc80mV7eztOp5PS0lJKSkpwOp1x7Wuoyt8dO3bw85//nHXr1tHd3Y3b7cblckXayz/44IM4\nnU4+9KEP8elPf5pzzz13xDRK+E42vJ7VA7/FIpwGCgfVvr6+jBtgrbOzk4qKCuAfzVP1CUBlknEb\nAKJHALVy/t945OXlMXPmTKqrq3nqqacGDQBhxhi8Xi8HDhzgwIEDuFwuysvLKS4ujqnj0MD0j9/v\n58EHH+S+++7D7XazePFiFi1axOmnnx4JLj6fj02bNvHiiy/y7LPPcv311zNz5ky+9KUvsWDBgmED\ngcfjidzdxjtUsxWCwSCHDh2KBIBMbF7Z29sbeUrJhOapSg00LusAAoEAb7zxBjNnzkzb3T+A3W4n\nJyeHJUuWsGHDhpjG1TfGYIyht7eXvXv3snXr1hHb1vv9/qOGaWhsbOSKK67gRz/6Eeeeey6rVq3i\n1ltvZcGCBUc9WYTv/G+44QZWr17N7bffzqFDh7juuuv44he/yN69ewfdX3Sv4PCMXem4s42+qCZr\n+ItERM8PkM7hKZQayrgMAO+++y49PT2cfPLJyZsAJkb5+fksXryYQCBwTGXwSIwxBAIBWlpaeO+9\n94bMIUenX7Zu3crVV19NS0sL99xzD3fffTcTJkwYcV8Oh4PLLruM1atXc+ONN/L6669z6aWXsnz5\n8kFTK+HmoKmoeB5KOA0UDAYzMr8ergcIBAJpH55CqcGMvwCwfDl1Z59NAPjqD39IwR//mNbiFBQU\ncOKJJ3LyySezYsWKUaUBjDH4fD527Ngx6Fg44fb3mzZt4rOf/SxOp5OHHnqI8847L+59OZ1Orrrq\nKv7whz8wd+5cvve97/HZz36WPXv2HLWe3++nr6+PAwcOpC0AhNNAmdT8c6DOzk56enp0/B+VkcbX\nWbl8OSxbRkFbGzYgr7UVx3XX9S9Pk3Ce/KqrrmLXrl2R4alHwxjD3r17aW5ujgSSQCCAx+Ph1Vdf\nZdmyZZSWlvLwww8zbdq0hMpdVVXFfffdx1133cX27dv5+Mc/zqOPPnrUxX7v3r1pz2t3dXVx+PDh\ntJdjKH19fRldPpXdxlcAuOkmGDgSZk9P//I0ycnJwWazcf755zNx4kR++9vfJrS9cIerlrvvJjhl\nCjank6nnnstfv/hFqqqqeOihh5g8ebIlZRcRPvaxj7Fy5Upmz57NHXfcwRe+8AWCv/0tM847j/rp\n0zlu0SKKn3zSkv3Fq/jJJ5lx3nlUT5nC8eefn7ZyDMdms6Wsg5xS8RpfrYB2745veYrk5eURCARY\nunQp9957L++//z7Tp08f9faKVq+m6tZbsYUqfvNaW/kJ8P6nPoXEkO+PV2VlJffffz+///3vaf7B\nD5j+8svkhD7LaWmh+tZbATh84YWW73soxU8+SXXUMYgux6ElS2htbeXgwYMcPnyYjo4Ourq66Ovr\no7e3F7/fH6lsFxGcTicOh4OcnBzcbjdutzsyfEj4VVhYSFFRETk5OcOU6tgyTvrRj3Du24evspL9\n11+f0mOk1Egk01pORGtoaDAbN26M/Qv19dDYeOzyujrYtcuqYsWtra2Nffv20dbWxvnnn8/FF1/M\nzTffPOrtHX/++eS0tByz3FtVxXtPP51IUUc0/SMfIffAgbTsO9pQx6DJbmeqSNLmX3C5XBQXF1NU\nVERRUVGk30ZpaSnl5eWR1+wtW5hz333Yo1pnBd1umm+9VYOAGpHNZmP69Omj6hwqIq8ZYxpiWXd8\nPQHceScsW3Z0Gigvr395GuXl5SEilJWVcdFFF/GnP/2J66+/nuLi4lFtz7lvX1zLreRubU3LvsNj\nKL3++uu8/vrrPDbIxR9gciDANZ/7HFVVVUyYMCFygS4oKIjc3TscDkQEESEYDBIIBPD5fPT19dHX\n14fH48Hj8dDd3U13dzddXV10dXXR2dlJZ2cnR44ciTxZNDY2smnTJjo6Oo5qLbUTGDj6lK23l/w7\n7+R3ItTU1FBbW0tJSUnGVmCr8W98BYArrwTAfPvbsGcPMmVK/8U/tDxd3G53pPL0iiuu4PHHH+eR\nRx7hy1/+8qi256usHPTu11dZmVA5E9n3bmP493//d/75n/+Z2bNnJ3xRCwQCbNu2jddeey3yCvej\nKC4u5oDLReUgTSv9VVV89atfjXk/NpstMixGuMJ+NMJ9I9ra2mhra6Puc58bdL2Szk5uvPHGyM+F\nhYXU1dVRX1/P1KlTqa+vZ9q0adTV1cXdG1ypeI2vAABw5ZVImi/4A4kIbrcbj8fD8ccfz5IlS/jV\nr37F4sWLOe644+Le3q+mTePqlhaiu7gF3W72X3+9dYUewv7rrz8q9w4QcLl46vTTWb9+PU8//TTH\nHXccZ511FgsWLGDOnDkjXsjC02Zu3bqVd999lzfeeIPNmzdHBp2rrq7mjDPOYO7cuZx66qlMnToV\n35o1BAeUI1XHYDA2m42SkhJKSkqYPn06vqqqIYP0Ez/7GU1NTezevTsyKODGjRtZvXp1ZD273c6U\nKVOYPn36Ua/6+vq46iGUGo4ldQAishj4Ef1Pvb80xnxvwOcu4GFgLtAGfNoYs2uk7cZdB5DB9u/f\nT2sofdLe3s6ll15KTU0NjzzySFxDVf/mN7/hnnvu4Scf/jCf37kzLRWMQ1Vu9vT0sHr1atauXcvf\n//53/H4/DoeD6upqampqmDRpUiT14vP5aG1t5cCBA7S0tEQu9jabjeOPP57Zs2cze/ZsGhoaqKqq\niqscmWBgJTWMXAfQ09PDzp072blzJzt27OD9999nx44d7N69O9KM1G63U1tby7Rp0yKvqVOnUldX\nl/ZOj8q4fxfkAAAXaElEQVQ6qaoDSDgAiIgdeA84D2gCXgUuN8a8HbXOdcApxpgviMhS4DJjzKdH\n2vZ4CgDd3d00NjZG/pDXrFnDjTfeyA033MA111wT0zYeeOAB7r33Xi644AK+//3vY7fbyc3NpbS0\nlJaWlpR2yLLb7UycOJH9+/cPut+uri5eeeUVNm/ezJ49e2hqaooMGWGMwW63U1FRwcSJE6msrGTG\njBmccMIJHHfccXGlYmw2W8a2sbcqQPX19bFr1y62b98eCQzvv/8+TU1NR1V2l5WVUVdXR21tLTU1\nNVRXV1NVVUVVVRWTJk3SlNIYMpYCwHzgVmPMBaGfvwVgjPlu1DprQ+usFxEHsA+YaEbY+XgKAMFg\nkHfeeSdysTTG8JWvfIX169fz+OOPU1dXN+R3jTHce++9/OIXv+Ciiy7ijjvuiFRkTp48mdLSUrq6\nuo66U0ym8H7z8/PZtm1bWsfgCVeup2NAuljYbDZEJCkjlfp8Pvbs2cPOnTsjqaRdu3bR3Nx8TGAO\nH6eKigomTJhAeXl5pOVSUVFRpGVTYWEh+fn5FBQUkJeXh9Pp1ErqNBhLrYCqgehxApqA04daxxjj\nF5HDQDlw0IL9jwk2m43c3NzI4GAiwn/8x39w6aWX8pnPfIbvfve7zJs375jvtbe3c/fdd7Nq1So+\n/vGPc/PNNx81rEB4NMyCggJqa2vZvXt30i/Idrs90nrF4XAkfR7g4eTk5FBaWpr0yegTUVpaysGD\n1p/qTqczkgYayOv1snfvXlpaWti3bx8tLS0cOHCA1tZWWltb2bp1K4cOHRrxdxc+b10uV+TldDoj\nL4fDEXnZbDbsdnvkXxGJBMDBXmHh94MtG/h+KOMtSBUUFHD//fcnfT8ZVwksIsuAZQBTpkxJc2ms\nVVxcfNRUgRUVFfzmN7/h61//OsuWLePaa6/l2muvxW63EwwGeeyxx/jZz36Gx+Ph85//PP/6r/96\n1Imel5d3VP1BuLNSMmeestlsVFVVRcpRXFyclItbLESE4uJiXC4Xdrs9aW3/E1FQUEBBQUHKJ8zJ\nycmhvr6e+vr6IdcxxkSG0gg3bQ03e+3u7sbj8dDT04PH48Hr9dLb20tfXx8+nw+/34/P54tMROTx\nePD7/ZFmtcFgEGNM5N/o99H7j34ijl4+2PtsUlpampL9WBEAmoHaqJ9rQssGW6cplAIqpr8y+BjG\nmAeAB6A/BWRB+TLGYDOTnXDCCaxYsYLvf//7/OIXv+AXv/jFUZ+fccYZfOMb3zjmLs9ms1FWVnbM\n9iZPnhwZgTIZ7Hb7UTObFRUVpfziFq2oqCgSCDItDWSz2SgqKiIvLy8jL2QiQmFhoVYeZ6BUDR5o\nRQB4FZghIlPpv9AvBa4YsM4q4BpgPfAJ4PmR8v/jUXhcoIEX57y8PG677TYWLVrE1q1bgf47n5kz\nZzJ//vxBH2+NMYP+4drtdqqrq9mzZ4/lFx2bzUZlZeVR5cnNzbV0H/GWJ5wjLS4u5tChQxlVIWyM\noaCgAJvNRk5Ojg4JrTJOwgEglNP/MrCW/magvzbGbBGR24GNxphVwK+AR0RkO9BOf5DIOuE7rqEG\nBzvzzDM588wzY9pWYWHhkHcJ4cq8zs5OS4PAwLt/+Mf/KTw/QCoVFhZGglE6A9FQwjly6C+rBgCV\naSypAzDGrAHWDFh2c9T7XuCTVuxrrAvn6BO5U7XZbCPmCKurq3nvvfcsSwWJyDF3/2HFxcWRSdpT\nxWazHTWUhohQVFSUUSNvRj+hpaMeQKmRjK/hoMeA/Px8S+7KB6tPiGa326mpqbGsdYTD4Tjm7j+6\nLKnO6BljjpnuM9b5k1PBZrMdFQDy8vL04q8yTmb8tWSR8DzBiYh1ALFwq6BEg4DNZov04h3qc7fb\nndA+4pWbm3vMxd6q4GoFY8xRHdqi6yuUyhQaANJgtKOAwj869MRq8uTJCd8V2+32EctcXFycsrbY\n4VY/A6UjEA3F7XYfc9y1tY3KNBoA0iC68jJeDocjrouc3W6nrq5u1PsTEWpra0f8/kgpKasNtT8r\nnngSFa6PGCjcIkipTKFnYxrk5uaOalwWEaG8vDzu7+Xl5TF58uS4L4wiwoQJE2Iam8flcqXs4jZc\nOiUT7rJFZNAAlZubq/UAKqNoAEiT8vLyUd2plpSUjGp/paWllJWVxbVPl8tFRUVFTOsOddFLhuEu\n8i6XK+1PAMaYQZ/S7Ha7DsimMooGgDQZzYU8Nzc30q58NCorK8nPz4/pAmmz2ZgyZUpcF9OioqKk\nPwWEe9cOJZWBaCjhGeCG+kypTKEBIE1iqViNZrPZRpX+iSYi1NXVDfv0ER7gbTQTj6SiOehgzT8H\nSkUgGspQ+f+wgoKCtD+hKBWmASCN4kkDhXvcJircoWvq1KmRERzDr3DrmuOPP35Ud6pWNHEdSXjg\nt+GkszmoiAwboHJzczUAqIyRcaOBZpNwZbDX6x12PRGhqqrK0rvavLw8jj/+eDweT2QEx5ycnIRT\nFMXFxRw4cMCiUh5ruLvrsPAQDOkapnq49v4ulytj+ioopU8AaTZhwoQR7wjdbndCfQeGYrPZyM/P\np7CwkJKSEkvy08ls6jiwd+1w0tUaaLj8P/xjfmilMoEGgDQrKSkZtmWIiFBdXT1m0gbJHpQt1ovn\ncIPlJctI+f+wkeowlEoVDQBpZrPZmDZt2qCte0SE0tLSMXXHaFVdxWDiqUBNVz1ALBf3/Px87RCm\nMoKehRnA4XAwderUoy4KIoLT6WTSpElpLNnolJSUWH6Bs9lscTWdDU9lmEoiElMleG5urtYDqIyg\nlcAZwuVyMXXqVJqbmykoKKCkpCQjOjWNRjKag4YnV4lHSUnJUVNwJlusfSwcDkfGTmGpsos+AWSQ\n3NxcjjvuOCorK3G73WPy4g+x58LjMZqcfiorggfOTzCSTJzARmUfDQAqKaxMA8UyAc5gomfkSrZ4\nn1C0Q5jKBBoAVFJYnQYa7fAOyWg+O5icnJy4gs1IzUWVSgUNACoprGwNlMgQz6kYFmKo+QmG43a7\ntSJYpZ0GAJU0paWlCV984239M1Cqcu3x1nmIiM4QptJOA4BKGqva4ifScSoVo4OOdrrHdI9aqpQG\nAJU0o628jWbFDF/Jnix+tDO8aYcwlW569qmkimWso6GICBMnTky4DMkcpjre5p/RtEOYSjcNACqp\ncnJyRl0ZXFBQYEme3G63J20illjmJxhKuEOYUumSUAAQkTIReUZEtoX+HfR5X0QCIrIp9FqVyD7V\n2FNRUTGq+YitHAajrKwsKemW3NzchLarA8OpdEr0L+KbwHPGmBnAc6GfB+MxxswJvS5OcJ9qjHG7\n3XG3xsnLy7N0ELzCwkLL0y1W1HFohzCVTokGgEuAh0LvHwIuTXB7apyqqKiI+U7Z6rt/iG8ugVgZ\nYxIe8kLnCFbplGgAmGSMaQm93wcM9VfrFpGNIrJBRDRIZKH8/Pxh5z2I5na7k3JhtDoNVFhYmHAO\nP9lTaCo1nBH7rovIs0DlIB/dFP2DMcaIyFDP2HXGmGYRmQY8LyJvGmPeH2J/y4BlAFOmTBmpeGqM\nCE9Iv337doLB4JDr2Ww2qqurk1IGK/PtNpuNsrKyhLcjIuTm5tLT02NBqZSKz4i3Q8aYRcaYkwd5\n/RHYLyJVAKF/B50M1hjTHPp3B/ACcOow+3vAGNNgjGmwogmgyhw5OTnU19cPmfMWEerr65M2AY6I\nJNSreOC2rAoo2iFMpUuiz8OrgGtC768B/jhwBREpFRFX6P0E4Azg7QT3q8aovLw8amtrjwkC4SeE\nZOfES0tLLal0tWo7oB3CVPokOlbu94BHReRzQCPwKQARaQC+YIy5FvgAcL+IBOkPON8zxmgAyGJF\nRUVUV1fT3d1NIBAgGAxSXl6ekjtht9uN0+nE6/WOehvhqTqtoh3CVLokFACMMW3ARwZZvhG4NvT+\nZWBWIvtR409JSYll6Zh4hFsYNTc3D1sXMRyXy2XpQG42m42cnBz6+vos26ZSsdDnTpV1ioqKRt16\nR0QoLy+3uESpnb1MqTANACrrhJ8CRpN3dzgcSXlyKSgo0HoAlXJ6xqmsNJoRQkWEyZMnJ6Xnbl5e\n3qhTUkqNlgYAlZVG8xTgdruTlqoZ7ZwCSiVCA4DKWvFMXC8iSeugFqb9AVSqaQBQWSvc8SyWIFBU\nVJS0DmphWg+gUk3PNpXV3G43dXV1w/ZOdrvdVFVVJb0sWg+gUk0DgMp6+fn5VFdXD9o7ubCwkGnT\npuFwJNpncmR2u10Hh1MplfyzWqkxIFwf0NnZSW9vL16vl7KyslFNZpOIwsJC2traUrY/ld00ACgV\nUlRUlPD4/okqKCjg0KFDmgpSKaEpIKUySF5eno4LpFJGA4BSGUTrAVQqaQBQKsOkOw2lsocGAKUy\nTGFhofYHUCmhZ5lSGUbnB1CpogFAqQxj5XSTSg1HA4BSGWg0o5UqFS89w5TKQAUFBZoGUkmnAUCp\nDOR0OlMy/ITKbhoAlMpQOk2kSjYNAEplqKKiIq0HUEmlZ5dSGUqHhVDJpgFAqQxls9nIy8tLdzHU\nOJZQABCRT4rIFhEJikjDMOstFpGtIrJdRL6ZyD6VyialpaWaBlJJk+iZ9RbwT8C6oVYQETvwU2AJ\ncBJwuYiclOB+lcoKhYWFmgZSSZNQADDGvGOM2TrCaqcB240xO4wxXmAFcEki+1UqW9jtdnJzc9Nd\nDDVOpeLZshrYE/VzU2iZUioGpaWlKZ2VTGWPEXuaiMizQOUgH91kjPmj1QUSkWXAMoApU6ZYvXml\nxhztD6CSZcQAYIxZlOA+moHaqJ9rQsuG2t8DwAMADQ0NmvxUWc/hcOB2u/F4POkuihpnUpECehWY\nISJTRSQHWAqsSsF+lRo3NA2kkiHRZqCXiUgTMB94UkTWhpZPFpE1AMYYP/BlYC3wDvCoMWZLYsVW\nKrvoLGEqGRIabcoY8wTwxCDL9wIfjfp5DbAmkX0plc00DaSSQXuYKDVGlJeXa6cwZSk9m5QaI4qK\nirRTmLKUBgClxgibzUZJSUm6i6HGEQ0ASo0h5eXl2hpIWUYDgFJjiNvtJicnJ93FUOOEBgClxhh9\nClBW0QCg1BhTXFyc7iKocUIDgFJjjN1u1yCgLKEBQKkxqKKiQtNAKmEaAJQag3JycvQpQCVMA4BS\nY5Q+BahEaQBQaozKycnRQeJUQjQAKDWG6VOASoQGAKXGMJfLpU8BatQ0ACg1xlVVVekooWpU9KxR\naoxzOBxUVVVpKkjFTQOAUuNASUkJubm56S6GGmM0ACg1DogINTU1+hSg4qIBQKlxIicnh8rKSg0C\nKmYaAJQaR8rKyiguLtYgoGKiAUCpcUREqK6uJjc3V4OAGpEGAKXGGRGhrq4Oh8OR7qKoDKcBQKlx\nyG63M336dFwulz4JqCFpAFBqnHI4HEyfPp2CggINAmpQCQUAEfmkiGwRkaCINAyz3i4ReVNENonI\nxkT2qZSKnc1mY8qUKUyYMEGDgDpGoknCt4B/Au6PYd1zjTEHE9yfUipOIsKkSZMoLi5m7969eDwe\njDHpLpbKAAkFAGPMO4DeWSg1BrjdbqZOnUpnZyf79+/H6/VqIMhyqWomYICnRcQA9xtjHhhqRRFZ\nBiwDmDJlSoqKp1R2EBGKioooKiqit7eX9vZ2Dh8+TDAYBNCAkGVGDAAi8ixQOchHNxlj/hjjfhYY\nY5pFpAJ4RkTeNcasG2zFUHB4AKChoUHPRqWSxO12M3nyZKqqqvB6vXR3d9PV1UVvby8+nw/4x9O9\nMUaDwzg0YgAwxixKdCfGmObQvwdE5AngNGDQAKCUSi0RweVy4XK5KCsrA/ov+H6/H5/Ph9/vx+/3\nEwgEIu+DwWDkFQ4O0UEiOlgMDBwaSEYmIikZ4jvpKSARyQdsxpjO0PvzgduTvV+l1OiJCE6nE6fT\nme6iqCRKtBnoZSLSBMwHnhSRtaHlk0VkTWi1ScD/icgbwN+AJ40xTyWyX6WUUolLtBXQE8ATgyzf\nC3w09H4HMDuR/SillLKe9gRWSqkspQFAKaWylAYApZTKUhoAlFIqS2kAUEqpLKUBQCmlspRkcq88\nEWkFGkf59QlAJo4+mqnlgswtW6aWCzK3bFqu+GVq2eItV50xZmIsK2Z0AEiEiGw0xgw5R0G6ZGq5\nIHPLlqnlgswtm5YrfplatmSWS1NASimVpTQAKKVUlhrPAWDIOQfSLFPLBZlbtkwtF2Ru2bRc8cvU\nsiWtXOO2DkAppdTwxvMTgFJKqWGMuQAgIotFZKuIbBeRbw7yuUtEfh/6/BURqY/67Fuh5VtF5II0\nlO3fRORtEdksIs+JSF3UZwER2RR6rUpxuT4jIq1R+7826rNrRGRb6HWNleWKsWz/HVWu90SkI+qz\nZB6zX4vIARF5a4jPRUR+HCr3ZhH5YNRnSTtmMZTrylB53hSRl0VkdtRnu0LLN4nIxhSX6xwRORz1\n+7o56rNhz4EUlO3rUeV6K3RelYU+S+YxqxWRv4SuCVtE5PpB1knueTZwNp9MfgF24H1gGpADvAGc\nNGCd64Cfh94vBX4fen9SaH0XMDW0HXuKy3YukBd6/8Vw2UI/d6XxmH0G+Mkg3y0DdoT+LQ29L01l\n2Qas/6/Ar5N9zELbPgv4IPDWEJ9/FPgzIMA84JUUHbORyvXh8P6AJeFyhX7eBUxI0/E6B1id6DmQ\njLINWPdjwPMpOmZVwAdD7wuB9wb520zqeTbWngBOA7YbY3YYY7zACuCSAetcAjwUev8Y8BERkdDy\nFcaYPmPMTmB7aHspK5sx5i/GmJ7QjxuAGgv3P+pyDeMC4BljTLsx5hDwDLA4jWW7HPidhfsfkumf\ns7p9mFUuAR42/TYAJSJSRZKP2UjlMsa8HNovpO4ci+V4DSWR8zMZZUvlOdZijPl76H0n8A5QPWC1\npJ5nYy0AVAN7on5u4tgDFlnHGOMHDgPlMX432WWL9jn6I3uYW0Q2isgGEbk0DeX6eOgR8zERqY3z\nu8kuG6F02VTg+ajFyTpmsRiq7Mk+ZvEYeI4Z4GkReU1ElqWhPPNF5A0R+bOIzAwty5jjJSJ59F9E\nH49anJJjJv2p6lOBVwZ8lNTzLOlzAqtjichVQANwdtTiOmNMs4hMA54XkTeNMe+nqEh/An5njOkT\nkX+h/wlqYYr2HaulwGPGmEDUsnQes4wmIufSHwAWRC1eEDpeFcAzIvJu6O44Ff5O/++rS0Q+CvwB\nmJGifcfqY8BLxpjop4WkHzMRKaA/6HzVGHPEym2PZKw9ATQDtVE/14SWDbqOiDiAYqAtxu8mu2yI\nyCLgJuBiY0xfeLkxpjn07w7gBfrvBlJSLmNMW1RZfgnMjfW7yS5blKUMeDRP4jGLxVBlT/YxG5GI\nnEL/7/ESY0xbeHnU8TpA/1SuVqZAh2WMOWKM6Qq9XwM4RWQCGXC8ogx3jiXlmImIk/6L/3JjzMpB\nVknueZaMyo1kveh/YtlBfyogXGE0c8A6X+LoSuBHQ+9ncnQl8A6srQSOpWyn0l/hNWPA8lLAFXo/\nAdiGRRVhMZarKur9ZcAG84+Kpp2h8pWG3pel8piF1juR/so4ScUxi9pHPUNXal7I0ZVzf0vFMYuh\nXFPor9/68IDl+UBh1PuXgcUpLFdl+PdH/0V0d+jYxXQOJLNsoc+L6a8nyE/VMQv9/x8GfjjMOkk9\nzyw9yKl40V8r/h79F9KbQstup/+OGsAN/G/oj+BvwLSo794U+t5WYEkayvYssB/YFHqtCi3/MPBm\n6OR/E/hcisv1XWBLaP9/AU6M+u7/Cx3L7cBnU33MQj/fCnxvwPeSfcx+B7QAPvrzq58DvgB8IfS5\nAD8NlftNoCEVxyyGcv0SOBR1jm0MLZ8WOlZvhH7XN6W4XF+OOsc2EBWgBjsHUlm20Dqfob+RSPT3\nkn3MFtBfx7A56vf10VSeZ9oTWCmlstRYqwNQSillEQ0ASimVpTQAKKVUltIAoJRSWUoDgFJKZSkN\nAEoplaU0ACilVJbSAKCUUlnq/wN0SP5vhYWi8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a70ee19b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning rates: [ 0.1   0.8   0.    0.38  0.2   0.58  1.06]\n",
      "Accuracy: [ 0.94419998  0.1028      0.1336      0.1028      0.48390001  0.1135\n",
      "  0.0974    ]\n"
     ]
    }
   ],
   "source": [
    "#If sf is used, we have to use opt_hyparams_noise, otherwise use opt_hyparams. \n",
    "n_iter = 5\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    next_candidate = acquisition_fun(l_rates, f, xn, np.array(data['Mean']), np.array(data['StdDev']))\n",
    "    l_rates = np.append(l_rates,next_candidate)\n",
    "    f = np.append(f,nn_train(next_candidate, h_dim))\n",
    "    E, cov = gp_posterior(l_rates, f, xn, m, noise, length, sf)\n",
    "    data = data_posterior(xn, E, cov)\n",
    "\n",
    "    try:\n",
    "        m, noise, length, sf = opt_hyparams(l_rates,f)\n",
    "    except ValueError:\n",
    "        m, noise, length, sf = m, noise, length, sf\n",
    "\n",
    "\n",
    "plt.plot(data['x'],data['Mean'], color = 'black', label = 'Mean')\n",
    "plt.plot(l_rates,f, 'ro', label = 'Obs')\n",
    "plt.fill_between(data['x'], data['Mean']-data['StdDev'], data['Mean']+data['StdDev'],color = 'lightgrey')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print ('Initial Learning rates:', l_rates)\n",
    "print ('Accuracy:', f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we observe how the algorithm explores the domain of the hyperparameter, searching for the most appropiate learning rate. In this examples the initial learning rates are 0.1 and 8. After 5 interations the algorithm recomends a learning rate that allows to increase the accuracy from the region in [0.9, 0.92] to [0.93, 0.95]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accuracy evaluation with decaying learning rate - w/o bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.1 loss: 228.615 (lr:0.003)\n",
      "0: ********* epoch 1 ********* test accuracy:0.081 test loss: 230.111\n",
      "20: accuracy:0.42 loss: 220.247 (lr:0.0029711445178725875)\n",
      "40: accuracy:0.46 loss: 206.584 (lr:0.0029425761525895904)\n",
      "60: accuracy:0.45 loss: 204.837 (lr:0.0029142920472906737)\n",
      "80: accuracy:0.72 loss: 186.098 (lr:0.0028862893735417373)\n",
      "100: accuracy:0.85 loss: 176.142 (lr:0.0028585653310520707)\n",
      "100: ********* epoch 1 ********* test accuracy:0.7678 test loss: 179.246\n",
      "120: accuracy:0.73 loss: 180.379 (lr:0.0028311171473943213)\n",
      "140: accuracy:0.77 loss: 175.923 (lr:0.0028039420777272502)\n",
      "160: accuracy:0.73 loss: 176.193 (lr:0.0027770374045212438)\n",
      "180: accuracy:0.77 loss: 175.147 (lr:0.0027504004372865616)\n",
      "200: accuracy:0.78 loss: 170.835 (lr:0.0027240285123042826)\n",
      "200: ********* epoch 1 ********* test accuracy:0.8009 test loss: 169.799\n",
      "220: accuracy:0.74 loss: 175.683 (lr:0.0026979189923599317)\n",
      "240: accuracy:0.82 loss: 166.903 (lr:0.0026720692664797567)\n",
      "260: accuracy:0.75 loss: 174.02 (lr:0.0026464767496696276)\n",
      "280: accuracy:0.81 loss: 167.706 (lr:0.002621138882656537)\n",
      "300: accuracy:0.88 loss: 161.66 (lr:0.0025960531316326675)\n",
      "300: ********* epoch 1 ********* test accuracy:0.8123 test loss: 167.328\n",
      "320: accuracy:0.78 loss: 171.856 (lr:0.002571216988002013)\n",
      "340: accuracy:0.78 loss: 170.743 (lr:0.002546627968129513)\n",
      "360: accuracy:0.81 loss: 165.31 (lr:0.0025222836130926888)\n",
      "380: accuracy:0.8 loss: 166.801 (lr:0.0024981814884357505)\n",
      "400: accuracy:0.88 loss: 161.908 (lr:0.0024743191839261473)\n",
      "400: ********* epoch 1 ********* test accuracy:0.82 test loss: 166.053\n",
      "420: accuracy:0.82 loss: 168.03 (lr:0.0024506943133135424)\n",
      "440: accuracy:0.82 loss: 163.981 (lr:0.0024273045140911875)\n",
      "460: accuracy:0.76 loss: 169.505 (lr:0.0024041474472596687)\n",
      "480: accuracy:0.78 loss: 168.791 (lr:0.002381220797093005)\n",
      "500: accuracy:0.79 loss: 168.478 (lr:0.002358522270907074)\n",
      "500: ********* epoch 1 ********* test accuracy:0.8241 test loss: 165.381\n",
      "520: accuracy:0.81 loss: 166.52 (lr:0.0023360495988303423)\n",
      "540: accuracy:0.82 loss: 166.561 (lr:0.002313800533576874)\n",
      "560: accuracy:0.83 loss: 166.658 (lr:0.0022917728502216037)\n",
      "580: accuracy:0.87 loss: 161.235 (lr:0.0022699643459778394)\n",
      "600: accuracy:0.86 loss: 161.594 (lr:0.002248372839976982)\n",
      "600: ********* epoch 2 ********* test accuracy:0.8262 test loss: 164.884\n",
      "620: accuracy:0.78 loss: 167.508 (lr:0.0022269961730504387)\n",
      "640: accuracy:0.77 loss: 169.636 (lr:0.0022058322075137037)\n",
      "660: accuracy:0.83 loss: 164.883 (lr:0.0021848788269525857)\n",
      "680: accuracy:0.79 loss: 167.2 (lr:0.002164133936011568)\n",
      "700: accuracy:0.85 loss: 162.115 (lr:0.002143595460184269)\n",
      "700: ********* epoch 2 ********* test accuracy:0.8293 test loss: 164.49\n",
      "720: accuracy:0.84 loss: 163.125 (lr:0.00212326134560599)\n",
      "740: accuracy:0.88 loss: 160.289 (lr:0.0021031295588483287)\n",
      "760: accuracy:0.85 loss: 163.889 (lr:0.002083198086715832)\n",
      "780: accuracy:0.78 loss: 168.72 (lr:0.0020634649360446776)\n",
      "800: accuracy:0.88 loss: 160.73 (lr:0.002043928133503354)\n",
      "800: ********* epoch 2 ********* test accuracy:0.8323 test loss: 164.109\n",
      "820: accuracy:0.85 loss: 162.806 (lr:0.0020245857253953265)\n",
      "840: accuracy:0.82 loss: 166.43 (lr:0.0020054357774636645)\n",
      "860: accuracy:0.86 loss: 161.757 (lr:0.001986476374697618)\n",
      "880: accuracy:0.79 loss: 166.946 (lr:0.00196770562114111)\n",
      "900: accuracy:0.82 loss: 165.506 (lr:0.001949121639703143)\n",
      "900: ********* epoch 2 ********* test accuracy:0.8303 test loss: 164.138\n",
      "920: accuracy:0.85 loss: 162.127 (lr:0.0019307225719700854)\n",
      "940: accuracy:0.83 loss: 164.349 (lr:0.0019125065780198325)\n",
      "960: accuracy:0.81 loss: 165.229 (lr:0.0018944718362378086)\n",
      "980: accuracy:0.82 loss: 166.375 (lr:0.0018766165431348069)\n",
      "1000: accuracy:0.86 loss: 161.893 (lr:0.0018589389131666372)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.8335 test loss: 163.773\n",
      "1020: accuracy:0.85 loss: 161.12 (lr:0.0018414371785555712)\n",
      "1040: accuracy:0.8 loss: 166.31 (lr:0.001824109589113564)\n",
      "1060: accuracy:0.81 loss: 166.406 (lr:0.0018069544120672303)\n",
      "1080: accuracy:0.84 loss: 163.465 (lr:0.0017899699318845701)\n",
      "1100: accuracy:0.86 loss: 161.141 (lr:0.0017731544501034114)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.8332 test loss: 163.597\n",
      "1120: accuracy:0.75 loss: 171.316 (lr:0.0017565062851615633)\n",
      "1140: accuracy:0.84 loss: 162.827 (lr:0.0017400237722286578)\n",
      "1160: accuracy:0.81 loss: 167.032 (lr:0.001723705263039666)\n",
      "1180: accuracy:0.82 loss: 165.997 (lr:0.0017075491257300707)\n",
      "1200: accuracy:0.83 loss: 163.368 (lr:0.0016915537446726768)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.8369 test loss: 163.38\n",
      "1220: accuracy:0.84 loss: 162.657 (lr:0.0016757175203160495)\n",
      "1240: accuracy:0.88 loss: 158.423 (lr:0.001660038869024556)\n",
      "1260: accuracy:0.85 loss: 162.622 (lr:0.001644516222920002)\n",
      "1280: accuracy:0.82 loss: 165.815 (lr:0.001629148029724841)\n",
      "1300: accuracy:0.87 loss: 159.938 (lr:0.0016139327526069466)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.8351 test loss: 163.385\n",
      "1320: accuracy:0.84 loss: 162.916 (lr:0.0015988688700259279)\n",
      "1340: accuracy:0.84 loss: 162.801 (lr:0.0015839548755809732)\n",
      "1360: accuracy:0.78 loss: 169.49 (lr:0.0015691892778602098)\n",
      "1380: accuracy:0.83 loss: 164.382 (lr:0.0015545706002915614)\n",
      "1400: accuracy:0.81 loss: 165.098 (lr:0.0015400973809950877)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.8364 test loss: 163.275\n",
      "1420: accuracy:0.85 loss: 161.832 (lr:0.001525768172636799)\n",
      "1440: accuracy:0.92 loss: 156.237 (lr:0.001511581542283918)\n",
      "1460: accuracy:0.87 loss: 160.294 (lr:0.0014975360712615872)\n",
      "1480: accuracy:0.83 loss: 164.184 (lr:0.0014836303550109999)\n",
      "1500: accuracy:0.86 loss: 162.431 (lr:0.0014698630029489428)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.8376 test loss: 163.17\n",
      "1520: accuracy:0.91 loss: 157.104 (lr:0.0014562326383287369)\n",
      "1540: accuracy:0.77 loss: 168.097 (lr:0.0014427378981025616)\n",
      "1560: accuracy:0.78 loss: 168.912 (lr:0.0014293774327851483)\n",
      "1580: accuracy:0.88 loss: 159.783 (lr:0.001416149906318832)\n",
      "1600: accuracy:0.83 loss: 162.975 (lr:0.0014030539959399427)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.8392 test loss: 162.967\n",
      "1620: accuracy:0.84 loss: 162.96 (lr:0.0013900883920465294)\n",
      "1640: accuracy:0.92 loss: 155.352 (lr:0.001377251798067398)\n",
      "1660: accuracy:0.79 loss: 168.226 (lr:0.0013645429303324535)\n",
      "1680: accuracy:0.83 loss: 162.974 (lr:0.0013519605179443314)\n",
      "1700: accuracy:0.86 loss: 160.537 (lr:0.0013395033026513076)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.8403 test loss: 162.9\n",
      "1720: accuracy:0.88 loss: 159.792 (lr:0.0013271700387214717)\n",
      "1740: accuracy:0.85 loss: 161.747 (lr:0.0013149594928181531)\n",
      "1760: accuracy:0.83 loss: 163.651 (lr:0.0013028704438765861)\n",
      "1780: accuracy:0.89 loss: 158.825 (lr:0.001290901682981802)\n",
      "1800: accuracy:0.77 loss: 170.01 (lr:0.0012790520132477375)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.8408 test loss: 162.785\n",
      "1820: accuracy:0.8 loss: 167.134 (lr:0.0012673202496975445)\n",
      "1840: accuracy:0.81 loss: 166.909 (lr:0.0012557052191450912)\n",
      "1860: accuracy:0.82 loss: 164.071 (lr:0.0012442057600776434)\n",
      "1880: accuracy:0.84 loss: 164.144 (lr:0.0012328207225397114)\n",
      "1900: accuracy:0.82 loss: 164.63 (lr:0.0012215489680180538)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.8408 test loss: 162.735\n",
      "1920: accuracy:0.85 loss: 161.006 (lr:0.0012103893693278251)\n",
      "1940: accuracy:0.85 loss: 163.22 (lr:0.0011993408104998568)\n",
      "1960: accuracy:0.84 loss: 163.742 (lr:0.001188402186669059)\n",
      "1980: accuracy:0.76 loss: 170.359 (lr:0.0011775724039639326)\n",
      "2000: accuracy:0.85 loss: 161.349 (lr:0.0011668503793971828)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.8404 test loss: 162.749\n",
      "2020: accuracy:0.86 loss: 159.414 (lr:0.0011562350407574179)\n",
      "2040: accuracy:0.85 loss: 161.114 (lr:0.0011457253265019273)\n",
      "2060: accuracy:0.88 loss: 159.366 (lr:0.0011353201856505275)\n",
      "2080: accuracy:0.85 loss: 161.954 (lr:0.0011250185776804627)\n",
      "2100: accuracy:0.89 loss: 159.397 (lr:0.0011148194724223506)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.841 test loss: 162.657\n",
      "2120: accuracy:0.78 loss: 170.474 (lr:0.0011047218499571666)\n",
      "2140: accuracy:0.81 loss: 165.872 (lr:0.0010947247005142493)\n",
      "2160: accuracy:0.84 loss: 162.613 (lr:0.0010848270243703235)\n",
      "2180: accuracy:0.8 loss: 166.903 (lr:0.0010750278317495268)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200: accuracy:0.83 loss: 163.989 (lr:0.0010653261427244307)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.8416 test loss: 162.366\n",
      "2220: accuracy:0.85 loss: 161.043 (lr:0.0010557209871180483)\n",
      "2240: accuracy:0.86 loss: 159.05 (lr:0.0010462114044068145)\n",
      "2260: accuracy:0.94 loss: 155.533 (lr:0.0010367964436245336)\n",
      "2280: accuracy:0.91 loss: 157.154 (lr:0.0010274751632672816)\n",
      "2300: accuracy:0.89 loss: 157.604 (lr:0.0010182466311992545)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9106 test loss: 157.22\n",
      "2320: accuracy:0.9 loss: 158.618 (lr:0.0010091099245595554)\n",
      "2340: accuracy:0.93 loss: 154.46 (lr:0.0010000641296699067)\n",
      "2360: accuracy:0.86 loss: 159.863 (lr:0.0009911083419432806)\n",
      "2380: accuracy:0.9 loss: 157.52 (lr:0.0009822416657934419)\n",
      "2400: accuracy:0.96 loss: 153.707 (lr:0.0009734632145453863)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.921 test loss: 156.116\n",
      "2420: accuracy:0.88 loss: 159.921 (lr:0.0009647721103466736)\n",
      "2440: accuracy:0.95 loss: 155.258 (lr:0.0009561674840796413)\n",
      "2460: accuracy:0.93 loss: 153.496 (lr:0.0009476484752744924)\n",
      "2480: accuracy:0.88 loss: 158.483 (lr:0.0009392142320232469)\n",
      "2500: accuracy:0.9 loss: 156.567 (lr:0.0009308639108945514)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.922 test loss: 155.679\n",
      "2520: accuracy:0.88 loss: 158.875 (lr:0.0009225966768493343)\n",
      "2540: accuracy:0.93 loss: 155.197 (lr:0.0009144117031573014)\n",
      "2560: accuracy:0.88 loss: 158.452 (lr:0.0009063081713142631)\n",
      "2580: accuracy:0.94 loss: 154.871 (lr:0.0008982852709602818)\n",
      "2600: accuracy:0.93 loss: 153.438 (lr:0.0008903421997986366)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9239 test loss: 155.398\n",
      "2620: accuracy:0.93 loss: 154.124 (lr:0.0008824781635155918)\n",
      "2640: accuracy:0.93 loss: 153.832 (lr:0.000874692375700966)\n",
      "2660: accuracy:0.93 loss: 156.166 (lr:0.0008669840577694896)\n",
      "2680: accuracy:0.95 loss: 152.383 (lr:0.0008593524388829455)\n",
      "2700: accuracy:0.91 loss: 155.626 (lr:0.0008517967558730855)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9232 test loss: 155.177\n",
      "2720: accuracy:0.91 loss: 156.738 (lr:0.0008443162531653122)\n",
      "2740: accuracy:0.93 loss: 155.616 (lr:0.0008369101827031209)\n",
      "2760: accuracy:0.94 loss: 154.231 (lr:0.000829577803873294)\n",
      "2780: accuracy:0.95 loss: 154.288 (lr:0.000822318383431838)\n",
      "2800: accuracy:0.95 loss: 152.222 (lr:0.0008151311954306589)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9238 test loss: 155.041\n",
      "2820: accuracy:0.93 loss: 155.383 (lr:0.0008080155211449677)\n",
      "2840: accuracy:0.97 loss: 151.256 (lr:0.0008009706490014058)\n",
      "2860: accuracy:0.91 loss: 156.516 (lr:0.0007939958745068883)\n",
      "2880: accuracy:0.9 loss: 157.629 (lr:0.0007870905001781532)\n",
      "2900: accuracy:0.99 loss: 150.733 (lr:0.0007802538354720133)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9253 test loss: 154.971\n",
      "2920: accuracy:0.9 loss: 155.983 (lr:0.0007734851967163007)\n",
      "2940: accuracy:0.93 loss: 155.247 (lr:0.0007667839070414992)\n",
      "2960: accuracy:0.88 loss: 158.091 (lr:0.000760149296313057)\n",
      "2980: accuracy:0.91 loss: 155.032 (lr:0.0007535807010643724)\n",
      "3000: accuracy:0.94 loss: 155.256 (lr:0.0007470774644304465)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9266 test loss: 154.776\n",
      "3020: accuracy:0.88 loss: 159.128 (lr:0.0007406389360821968)\n",
      "3040: accuracy:0.94 loss: 153.476 (lr:0.0007342644721614229)\n",
      "3060: accuracy:0.97 loss: 150.633 (lr:0.0007279534352164206)\n",
      "3080: accuracy:0.93 loss: 154.417 (lr:0.0007217051941382361)\n",
      "3100: accuracy:0.95 loss: 152.263 (lr:0.0007155191240975549)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9263 test loss: 154.682\n",
      "3120: accuracy:0.88 loss: 157.712 (lr:0.0007093946064822178)\n",
      "3140: accuracy:0.95 loss: 152.318 (lr:0.0007033310288353594)\n",
      "3160: accuracy:0.94 loss: 154.604 (lr:0.000697327784794162)\n",
      "3180: accuracy:0.91 loss: 157.528 (lr:0.000691384274029219)\n",
      "3200: accuracy:0.95 loss: 152.734 (lr:0.0006854999021845007)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9264 test loss: 154.644\n",
      "3220: accuracy:0.93 loss: 154.423 (lr:0.0006796740808179191)\n",
      "3240: accuracy:0.91 loss: 157.199 (lr:0.0006739062273424826)\n",
      "3260: accuracy:0.93 loss: 154.648 (lr:0.0006681957649680373)\n",
      "3280: accuracy:0.98 loss: 150.764 (lr:0.0006625421226435866)\n",
      "3300: accuracy:0.96 loss: 151.7 (lr:0.000656944735000187)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.926 test loss: 154.576\n",
      "3320: accuracy:0.88 loss: 156.996 (lr:0.0006514030422944097)\n",
      "3340: accuracy:0.84 loss: 160.968 (lr:0.0006459164903523658)\n",
      "3360: accuracy:0.92 loss: 155.63 (lr:0.000640484530514289)\n",
      "3380: accuracy:0.92 loss: 155.643 (lr:0.000635106619579669)\n",
      "3400: accuracy:0.9 loss: 156.72 (lr:0.0006297822197529307)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9272 test loss: 154.492\n",
      "3420: accuracy:0.95 loss: 151.955 (lr:0.0006245107985896541)\n",
      "3440: accuracy:0.94 loss: 153.122 (lr:0.0006192918289433304)\n",
      "3460: accuracy:0.93 loss: 153.868 (lr:0.0006141247889126458)\n",
      "3480: accuracy:0.95 loss: 151.122 (lr:0.000609009161789291)\n",
      "3500: accuracy:0.94 loss: 154.001 (lr:0.000603944436006291)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.927 test loss: 154.433\n",
      "3520: accuracy:0.91 loss: 155.675 (lr:0.0005989301050868467)\n",
      "3540: accuracy:0.85 loss: 160.345 (lr:0.0005939656675936874)\n",
      "3560: accuracy:0.91 loss: 155.374 (lr:0.000589050627078927)\n",
      "3580: accuracy:0.95 loss: 153.462 (lr:0.000584184492034418)\n",
      "3600: accuracy:0.9 loss: 156.58 (lr:0.000579366775842601)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9282 test loss: 154.349\n",
      "3620: accuracy:0.92 loss: 156.607 (lr:0.0005745969967278418)\n",
      "3640: accuracy:0.93 loss: 153.346 (lr:0.0005698746777082543)\n",
      "3660: accuracy:0.95 loss: 152.532 (lr:0.000565199346548001)\n",
      "3680: accuracy:0.93 loss: 154.701 (lr:0.00056057053571007)\n",
      "3700: accuracy:0.97 loss: 151.21 (lr:0.0005559877823095201)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9294 test loss: 154.339\n",
      "3720: accuracy:0.93 loss: 155.065 (lr:0.0005514506280671922)\n",
      "3740: accuracy:0.97 loss: 151.149 (lr:0.0005469586192638811)\n",
      "3760: accuracy:0.95 loss: 152.452 (lr:0.0005425113066949633)\n",
      "3780: accuracy:0.94 loss: 154.027 (lr:0.0005381082456254755)\n",
      "3800: accuracy:0.92 loss: 156.138 (lr:0.0005337489957456418)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9296 test loss: 154.257\n",
      "3820: accuracy:0.92 loss: 155.872 (lr:0.0005294331211268412)\n",
      "3840: accuracy:0.95 loss: 153.764 (lr:0.0005251601901780155)\n",
      "3860: accuracy:0.94 loss: 154.536 (lr:0.0005209297756025089)\n",
      "3880: accuracy:0.96 loss: 151.565 (lr:0.0005167414543553385)\n",
      "3900: accuracy:0.88 loss: 158.935 (lr:0.0005125948076008895)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9303 test loss: 154.189\n",
      "3920: accuracy:0.93 loss: 156.544 (lr:0.0005084894206710306)\n",
      "3940: accuracy:0.89 loss: 158.28 (lr:0.0005044248830236478)\n",
      "3960: accuracy:0.93 loss: 154.63 (lr:0.0005004007882015892)\n",
      "3980: accuracy:0.94 loss: 153.34 (lr:0.0004964167337920192)\n",
      "4000: accuracy:0.92 loss: 156.182 (lr:0.0004924723213861769)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9302 test loss: 154.204\n",
      "4020: accuracy:0.93 loss: 153.983 (lr:0.0004885671565395345)\n",
      "4040: accuracy:0.91 loss: 156.309 (lr:0.00048470084873235297)\n",
      "4060: accuracy:0.94 loss: 152.959 (lr:0.00048087301133063005)\n",
      "4080: accuracy:0.92 loss: 155.175 (lr:0.00047708326154743513)\n",
      "4100: accuracy:0.95 loss: 153.605 (lr:0.0004733312204046323)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9313 test loss: 154.112\n",
      "4120: accuracy:0.96 loss: 151.651 (lr:0.0004696165126949802)\n",
      "4140: accuracy:0.91 loss: 156.632 (lr:0.00046593876694461247)\n",
      "4160: accuracy:0.89 loss: 157.651 (lr:0.000462297615375889)\n",
      "4180: accuracy:0.93 loss: 154.102 (lr:0.000458692693870619)\n",
      "4200: accuracy:0.89 loss: 156.53 (lr:0.00045512364193364755)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9314 test loss: 154.079\n",
      "4220: accuracy:0.92 loss: 155.105 (lr:0.0004515901026568069)\n",
      "4240: accuracy:0.93 loss: 154.521 (lr:0.00044809172268322453)\n",
      "4260: accuracy:0.96 loss: 151.577 (lr:0.00044462815217198804)\n",
      "4280: accuracy:0.91 loss: 154.402 (lr:0.0004411990447631597)\n",
      "4300: accuracy:0.95 loss: 152.326 (lr:0.00043780405754314123)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9316 test loss: 154.077\n",
      "4320: accuracy:0.92 loss: 155.727 (lr:0.0004344428510103813)\n",
      "4340: accuracy:0.95 loss: 152.328 (lr:0.00043111508904142585)\n",
      "4360: accuracy:0.95 loss: 151.785 (lr:0.0004278204388573046)\n",
      "4380: accuracy:0.97 loss: 151.164 (lr:0.0004245585709902538)\n",
      "4400: accuracy:0.9 loss: 156.553 (lr:0.00042132915925076824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400: ********* epoch 8 ********* test accuracy:0.9316 test loss: 154.063\n",
      "4420: accuracy:0.93 loss: 154.076 (lr:0.0004181318806949831)\n",
      "4440: accuracy:0.96 loss: 150.495 (lr:0.0004149664155923781)\n",
      "4460: accuracy:0.91 loss: 156.952 (lr:0.0004118324473938054)\n",
      "4480: accuracy:0.93 loss: 154.12 (lr:0.00040872966269983314)\n",
      "4500: accuracy:0.88 loss: 157.642 (lr:0.00040565775122940656)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9322 test loss: 154.027\n",
      "4520: accuracy:0.92 loss: 154.543 (lr:0.0004026164057888186)\n",
      "4540: accuracy:0.92 loss: 153.301 (lr:0.0003996053222409906)\n",
      "4560: accuracy:0.94 loss: 153.567 (lr:0.0003966241994750587)\n",
      "4580: accuracy:0.94 loss: 152.918 (lr:0.00039367273937626184)\n",
      "4600: accuracy:0.94 loss: 154.191 (lr:0.0003907506467961309)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9314 test loss: 153.988\n",
      "4620: accuracy:0.96 loss: 151.415 (lr:0.0003878576295229724)\n",
      "4640: accuracy:0.94 loss: 151.979 (lr:0.0003849933982526485)\n",
      "4660: accuracy:0.97 loss: 151.275 (lr:0.00038215766655964504)\n",
      "4680: accuracy:0.91 loss: 156.067 (lr:0.0003793501508684298)\n",
      "4700: accuracy:0.96 loss: 150.911 (lr:0.0003765705704250939)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9317 test loss: 153.93\n",
      "4720: accuracy:0.95 loss: 154.103 (lr:0.0003738186472692768)\n",
      "4740: accuracy:0.96 loss: 150.393 (lr:0.0003710941062063696)\n",
      "4760: accuracy:0.92 loss: 154.558 (lr:0.00036839667477999555)\n",
      "4780: accuracy:0.95 loss: 152.823 (lr:0.00036572608324476403)\n",
      "4800: accuracy:0.93 loss: 153.7 (lr:0.0003630820645392963)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9317 test loss: 153.949\n",
      "4820: accuracy:0.93 loss: 153.76 (lr:0.00036046435425951816)\n",
      "4840: accuracy:0.95 loss: 152.282 (lr:0.00035787269063222043)\n",
      "4860: accuracy:0.95 loss: 152.404 (lr:0.0003553068144888804)\n",
      "4880: accuracy:0.95 loss: 152.657 (lr:0.00035276646923974576)\n",
      "4900: accuracy:0.92 loss: 154.065 (lr:0.00035025140084817447)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9321 test loss: 153.947\n",
      "4920: accuracy:0.95 loss: 151.721 (lr:0.0003477613578052316)\n",
      "4940: accuracy:0.94 loss: 153.185 (lr:0.00034529609110453763)\n",
      "4960: accuracy:0.95 loss: 151.455 (lr:0.0003428553542173683)\n",
      "4980: accuracy:0.96 loss: 150.802 (lr:0.00034043890306800073)\n",
      "5000: accuracy:0.9 loss: 157.646 (lr:0.00033804649600930654)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9329 test loss: 153.881\n",
      "5020: accuracy:0.93 loss: 153.885 (lr:0.00033567789379858597)\n",
      "5040: accuracy:0.88 loss: 156.736 (lr:0.0003333328595736441)\n",
      "5060: accuracy:0.91 loss: 156.381 (lr:0.00033101115882910436)\n",
      "5080: accuracy:0.94 loss: 153.204 (lr:0.00032871255939295735)\n",
      "5100: accuracy:0.96 loss: 152.905 (lr:0.0003264368314033442)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9329 test loss: 153.864\n",
      "5120: accuracy:0.95 loss: 153.235 (lr:0.0003241837472855693)\n",
      "5140: accuracy:0.96 loss: 151.869 (lr:0.00032195308172934344)\n",
      "5160: accuracy:0.94 loss: 152.357 (lr:0.00031974461166625194)\n",
      "5180: accuracy:0.94 loss: 153.455 (lr:0.00031755811624744826)\n",
      "5200: accuracy:0.92 loss: 154.296 (lr:0.0003153933768215683)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.933 test loss: 153.857\n",
      "5220: accuracy:0.96 loss: 151.77 (lr:0.0003132501769128656)\n",
      "5240: accuracy:0.93 loss: 154.15 (lr:0.0003111283021995632)\n",
      "5260: accuracy:0.94 loss: 154.179 (lr:0.00030902754049242173)\n",
      "5280: accuracy:0.9 loss: 156.323 (lr:0.0003069476817135196)\n",
      "5300: accuracy:0.96 loss: 151.481 (lr:0.00030488851787524585)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9336 test loss: 153.834\n",
      "5320: accuracy:0.89 loss: 156.506 (lr:0.0003028498430595006)\n",
      "5340: accuracy:0.92 loss: 155.337 (lr:0.0003008314533971034)\n",
      "5360: accuracy:0.94 loss: 153.557 (lr:0.00029883314704740597)\n",
      "5380: accuracy:0.9 loss: 156.863 (lr:0.0002968547241781082)\n",
      "5400: accuracy:0.94 loss: 152.557 (lr:0.0002948959869452743)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9333 test loss: 153.815\n",
      "5420: accuracy:0.97 loss: 149.978 (lr:0.0002929567394735489)\n",
      "5440: accuracy:0.93 loss: 153.24 (lr:0.00029103678783656855)\n",
      "5460: accuracy:0.94 loss: 153.857 (lr:0.00028913594003756986)\n",
      "5480: accuracy:0.93 loss: 154.152 (lr:0.00028725400599018857)\n",
      "5500: accuracy:0.93 loss: 154.571 (lr:0.00028539079749945195)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9334 test loss: 153.795\n",
      "5520: accuracy:0.95 loss: 153.343 (lr:0.00028354612824295816)\n",
      "5540: accuracy:0.96 loss: 151.864 (lr:0.0002817198137522442)\n",
      "5560: accuracy:0.93 loss: 154.069 (lr:0.00027991167139433914)\n",
      "5580: accuracy:0.96 loss: 152.328 (lr:0.0002781215203535004)\n",
      "5600: accuracy:0.92 loss: 155.585 (lr:0.00027634918161313215)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9336 test loss: 153.781\n",
      "5620: accuracy:0.92 loss: 154.287 (lr:0.0002745944779378833)\n",
      "5640: accuracy:0.98 loss: 149.779 (lr:0.0002728572338559242)\n",
      "5660: accuracy:0.94 loss: 155.042 (lr:0.0002711372756413988)\n",
      "5680: accuracy:0.96 loss: 151.777 (lr:0.0002694344312970524)\n",
      "5700: accuracy:0.97 loss: 151.324 (lr:0.0002677485305370315)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9334 test loss: 153.786\n",
      "5720: accuracy:0.95 loss: 152.284 (lr:0.00026607940476985534)\n",
      "5740: accuracy:0.92 loss: 155.91 (lr:0.00026442688708155606)\n",
      "5760: accuracy:0.93 loss: 154.614 (lr:0.0002627908122189878)\n",
      "5780: accuracy:0.94 loss: 152.555 (lr:0.0002611710165733009)\n",
      "5800: accuracy:0.9 loss: 154.748 (lr:0.000259567338163581)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9336 test loss: 153.756\n",
      "5820: accuracy:0.96 loss: 152.247 (lr:0.0002579796166206506)\n",
      "5840: accuracy:0.98 loss: 149.881 (lr:0.00025640769317103245)\n",
      "5860: accuracy:0.97 loss: 150.727 (lr:0.00025485141062107154)\n",
      "5880: accuracy:0.93 loss: 153.623 (lr:0.00025331061334121606)\n",
      "5900: accuracy:0.87 loss: 159.031 (lr:0.00025178514725045394)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9338 test loss: 153.738\n",
      "5920: accuracy:0.95 loss: 152.768 (lr:0.00025027485980090493)\n",
      "5940: accuracy:0.9 loss: 155.732 (lr:0.00024877959996256544)\n",
      "5960: accuracy:0.92 loss: 155.655 (lr:0.00024729921820820566)\n",
      "5980: accuracy:0.89 loss: 158.033 (lr:0.0002458335664984164)\n",
      "6000: accuracy:0.92 loss: 153.623 (lr:0.00024438249826680544)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9347 test loss: 153.722\n",
      "6020: accuracy:0.93 loss: 153.336 (lr:0.0002429458684053403)\n",
      "6040: accuracy:0.96 loss: 152.846 (lr:0.00024152353324983762)\n",
      "6060: accuracy:0.93 loss: 154.673 (lr:0.00024011535056559663)\n",
      "6080: accuracy:0.93 loss: 155.243 (lr:0.00023872117953317528)\n",
      "6100: accuracy:0.96 loss: 150.953 (lr:0.00023734088073430873)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9339 test loss: 153.7\n",
      "6120: accuracy:0.94 loss: 153.453 (lr:0.00023597431613796662)\n",
      "6140: accuracy:0.95 loss: 152.11 (lr:0.0002346213490865507)\n",
      "6160: accuracy:0.92 loss: 154.749 (lr:0.00023328184428222823)\n",
      "6180: accuracy:0.93 loss: 153.342 (lr:0.00023195566777340254)\n",
      "6200: accuracy:0.95 loss: 152.039 (lr:0.00023064268694131766)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9332 test loss: 153.722\n",
      "6220: accuracy:0.92 loss: 155.343 (lr:0.00022934277048679616)\n",
      "6240: accuracy:0.94 loss: 153.861 (lr:0.00022805578841710933)\n",
      "6260: accuracy:0.92 loss: 156.546 (lr:0.00022678161203297775)\n",
      "6280: accuracy:0.93 loss: 155.461 (lr:0.0002255201139157011)\n",
      "6300: accuracy:0.95 loss: 152.859 (lr:0.00022427116791441654)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9353 test loss: 153.657\n",
      "6320: accuracy:0.93 loss: 154.264 (lr:0.00022303464913348302)\n",
      "6340: accuracy:0.95 loss: 152.35 (lr:0.0002218104339199921)\n",
      "6360: accuracy:0.93 loss: 153.228 (lr:0.00022059839985140217)\n",
      "6380: accuracy:0.95 loss: 152.231 (lr:0.00021939842572329645)\n",
      "6400: accuracy:0.97 loss: 150.244 (lr:0.00021821039153726202)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9351 test loss: 153.66\n",
      "6420: accuracy:0.92 loss: 155.091 (lr:0.00021703417848889034)\n",
      "6440: accuracy:0.97 loss: 150.557 (lr:0.0002158696689558963)\n",
      "6460: accuracy:0.92 loss: 153.58 (lr:0.0002147167464863563)\n",
      "6480: accuracy:0.94 loss: 154.098 (lr:0.00021357529578706251)\n",
      "6500: accuracy:0.9 loss: 157.263 (lr:0.00021244520271199385)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9351 test loss: 153.659\n",
      "6520: accuracy:0.93 loss: 153.878 (lr:0.00021132635425090104)\n",
      "6540: accuracy:0.95 loss: 153.273 (lr:0.00021021863851800553)\n",
      "6560: accuracy:0.92 loss: 155.427 (lr:0.00020912194474081107)\n",
      "6580: accuracy:0.92 loss: 155.104 (lr:0.00020803616324902585)\n",
      "6600: accuracy:0.95 loss: 153.834 (lr:0.00020696118546359606)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600: ********* epoch 12 ********* test accuracy:0.9353 test loss: 153.657\n",
      "6620: accuracy:0.93 loss: 154.196 (lr:0.00020589690388584717)\n",
      "6640: accuracy:0.9 loss: 154.79 (lr:0.00020484321208673464)\n",
      "6660: accuracy:0.93 loss: 154.172 (lr:0.00020380000469620037)\n",
      "6680: accuracy:0.96 loss: 151.735 (lr:0.00020276717739263608)\n",
      "6700: accuracy:0.92 loss: 154.758 (lr:0.0002017446268924506)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9349 test loss: 153.633\n",
      "6720: accuracy:0.95 loss: 152.299 (lr:0.00020073225093974185)\n",
      "6740: accuracy:0.93 loss: 154.61 (lr:0.00019972994829607087)\n",
      "6760: accuracy:0.93 loss: 153.274 (lr:0.00019873761873033811)\n",
      "6780: accuracy:0.92 loss: 155.307 (lr:0.00019775516300875995)\n",
      "6800: accuracy:0.99 loss: 149.426 (lr:0.00019678248288494563)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9351 test loss: 153.62\n",
      "6820: accuracy:0.89 loss: 156.891 (lr:0.00019581948109007212)\n",
      "6840: accuracy:0.97 loss: 151.286 (lr:0.0001948660613231575)\n",
      "6860: accuracy:0.96 loss: 151.446 (lr:0.00019392212824143043)\n",
      "6880: accuracy:0.91 loss: 155.667 (lr:0.00019298758745079625)\n",
      "6900: accuracy:0.95 loss: 152.159 (lr:0.00019206234549639704)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9352 test loss: 153.606\n",
      "6920: accuracy:0.96 loss: 153.523 (lr:0.00019114630985326637)\n",
      "6940: accuracy:0.96 loss: 150.21 (lr:0.0001902393889170765)\n",
      "6960: accuracy:0.93 loss: 155.481 (lr:0.0001893414919949781)\n",
      "6980: accuracy:0.96 loss: 150.885 (lr:0.0001884525292965307)\n",
      "7000: accuracy:0.91 loss: 156.709 (lr:0.00018757241192472366)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9349 test loss: 153.608\n",
      "7020: accuracy:0.95 loss: 152.961 (lr:0.00018670105186708636)\n",
      "7040: accuracy:0.93 loss: 154.344 (lr:0.00018583836198688683)\n",
      "7060: accuracy:0.95 loss: 152.669 (lr:0.00018498425601441823)\n",
      "7080: accuracy:0.99 loss: 148.963 (lr:0.00018413864853837148)\n",
      "7100: accuracy:0.94 loss: 151.714 (lr:0.00018330145499729437)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9348 test loss: 153.607\n",
      "7120: accuracy:0.92 loss: 153.66 (lr:0.00018247259167113508)\n",
      "7140: accuracy:0.94 loss: 152.912 (lr:0.0001816519756728703)\n",
      "7160: accuracy:0.9 loss: 158.759 (lr:0.00018083952494021637)\n",
      "7180: accuracy:0.89 loss: 158.281 (lr:0.00018003515822742294)\n",
      "7200: accuracy:0.96 loss: 151.34 (lr:0.00017923879509714843)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9352 test loss: 153.589\n",
      "7220: accuracy:0.93 loss: 154.215 (lr:0.0001784503559124162)\n",
      "7240: accuracy:0.97 loss: 149.589 (lr:0.0001776697618286507)\n",
      "7260: accuracy:0.93 loss: 153.945 (lr:0.00017689693478579313)\n",
      "7280: accuracy:0.94 loss: 152.635 (lr:0.0001761317975004951)\n",
      "7300: accuracy:0.94 loss: 153.462 (lr:0.0001753742734583905)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9361 test loss: 153.572\n",
      "7320: accuracy:0.97 loss: 150.953 (lr:0.00017462428690644382)\n",
      "7340: accuracy:0.95 loss: 151.818 (lr:0.00017388176284537495)\n",
      "7360: accuracy:0.9 loss: 156.581 (lr:0.0001731466270221589)\n",
      "7380: accuracy:0.93 loss: 154.232 (lr:0.00017241880592260087)\n",
      "7400: accuracy:0.96 loss: 151.735 (lr:0.00017169822676398424)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.935 test loss: 153.577\n",
      "7420: accuracy:0.89 loss: 159.096 (lr:0.00017098481748779274)\n",
      "7440: accuracy:0.94 loss: 153.277 (lr:0.00017027850675250424)\n",
      "7460: accuracy:0.9 loss: 157.106 (lr:0.0001695792239264566)\n",
      "7480: accuracy:0.94 loss: 152.516 (lr:0.0001688868990807845)\n",
      "7500: accuracy:0.91 loss: 154.798 (lr:0.00016820146298242642)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9354 test loss: 153.545\n",
      "7520: accuracy:0.93 loss: 153.261 (lr:0.00016752284708720135)\n",
      "7540: accuracy:0.98 loss: 149.156 (lr:0.00016685098353295415)\n",
      "7560: accuracy:0.94 loss: 151.987 (lr:0.00016618580513276965)\n",
      "7580: accuracy:0.92 loss: 154.57 (lr:0.00016552724536825341)\n",
      "7600: accuracy:0.93 loss: 153.748 (lr:0.00016487523838288026)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9353 test loss: 153.549\n",
      "7620: accuracy:0.92 loss: 154.329 (lr:0.00016422971897540824)\n",
      "7640: accuracy:0.92 loss: 154.852 (lr:0.00016359062259335873)\n",
      "7660: accuracy:0.96 loss: 150.674 (lr:0.00016295788532656085)\n",
      "7680: accuracy:0.95 loss: 153.115 (lr:0.00016233144390076078)\n",
      "7700: accuracy:0.97 loss: 152.416 (lr:0.0001617112356712938)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9355 test loss: 153.535\n",
      "7720: accuracy:0.92 loss: 153.291 (lr:0.00016109719861682017)\n",
      "7740: accuracy:0.96 loss: 150.607 (lr:0.00016048927133312268)\n",
      "7760: accuracy:0.95 loss: 152.284 (lr:0.00015988739302696644)\n",
      "7780: accuracy:0.94 loss: 153.157 (lr:0.0001592915035100192)\n",
      "7800: accuracy:0.98 loss: 149.075 (lr:0.00015870154319283275)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9356 test loss: 153.524\n",
      "7820: accuracy:0.95 loss: 152.754 (lr:0.00015811745307888365)\n",
      "7840: accuracy:0.93 loss: 154.058 (lr:0.00015753917475867384)\n",
      "7860: accuracy:0.96 loss: 151.331 (lr:0.00015696665040388936)\n",
      "7880: accuracy:0.95 loss: 152.612 (lr:0.00015639982276161764)\n",
      "7900: accuracy:0.93 loss: 153.413 (lr:0.00015583863514862209)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9355 test loss: 153.522\n",
      "7920: accuracy:0.95 loss: 153.445 (lr:0.00015528303144567374)\n",
      "7940: accuracy:0.92 loss: 154.287 (lr:0.00015473295609193932)\n",
      "7960: accuracy:0.93 loss: 154.375 (lr:0.00015418835407942505)\n",
      "7980: accuracy:0.92 loss: 154.053 (lr:0.0001536491709474758)\n",
      "8000: accuracy:0.92 loss: 155.242 (lr:0.00015311535277732913)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.936 test loss: 153.512\n",
      "8020: accuracy:0.9 loss: 157.828 (lr:0.00015258684618672314)\n",
      "8040: accuracy:0.94 loss: 153.583 (lr:0.00015206359832455833)\n",
      "8060: accuracy:0.97 loss: 150.941 (lr:0.00015154555686561238)\n",
      "8080: accuracy:0.92 loss: 155.269 (lr:0.00015103267000530785)\n",
      "8100: accuracy:0.93 loss: 153.553 (lr:0.0001505248864545312)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9367 test loss: 153.498\n",
      "8120: accuracy:0.95 loss: 152.729 (lr:0.00015002215543450422)\n",
      "8140: accuracy:0.93 loss: 154.338 (lr:0.00014952442667170591)\n",
      "8160: accuracy:0.93 loss: 154.104 (lr:0.0001490316503928453)\n",
      "8180: accuracy:0.9 loss: 157.542 (lr:0.00014854377731988384)\n",
      "8200: accuracy:0.93 loss: 153.687 (lr:0.00014806075866510764)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9355 test loss: 153.501\n",
      "8220: accuracy:0.94 loss: 152.621 (lr:0.00014758254612624869)\n",
      "8240: accuracy:0.95 loss: 152.495 (lr:0.00014710909188165463)\n",
      "8260: accuracy:0.97 loss: 149.207 (lr:0.00014664034858550645)\n",
      "8280: accuracy:0.94 loss: 154.202 (lr:0.00014617626936308393)\n",
      "8300: accuracy:0.99 loss: 149.462 (lr:0.00014571680780607802)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9361 test loss: 153.481\n",
      "8320: accuracy:0.93 loss: 154.856 (lr:0.00014526191796795023)\n",
      "8340: accuracy:0.9 loss: 157.806 (lr:0.0001448115543593376)\n",
      "8360: accuracy:0.9 loss: 155.617 (lr:0.00014436567194350402)\n",
      "8380: accuracy:0.93 loss: 154.123 (lr:0.00014392422613183626)\n",
      "8400: accuracy:0.96 loss: 152.637 (lr:0.00014348717277938534)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9361 test loss: 153.488\n",
      "8420: accuracy:0.93 loss: 154.585 (lr:0.0001430544681804518)\n",
      "8440: accuracy:0.9 loss: 156.584 (lr:0.00014262606906421518)\n",
      "8460: accuracy:0.94 loss: 152.542 (lr:0.00014220193259040676)\n",
      "8480: accuracy:0.89 loss: 158.125 (lr:0.00014178201634502583)\n",
      "8500: accuracy:0.93 loss: 153.755 (lr:0.00014136627833609785)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9358 test loss: 153.484\n",
      "8520: accuracy:0.92 loss: 153.791 (lr:0.00014095467698947548)\n",
      "8540: accuracy:0.91 loss: 155.444 (lr:0.0001405471711446811)\n",
      "8560: accuracy:0.96 loss: 151.483 (lr:0.00014014372005079055)\n",
      "8580: accuracy:0.94 loss: 152.407 (lr:0.00013974428336235834)\n",
      "8600: accuracy:0.96 loss: 151.418 (lr:0.00013934882113538272)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9367 test loss: 153.468\n",
      "8620: accuracy:0.96 loss: 153.74 (lr:0.00013895729382331143)\n",
      "8640: accuracy:0.95 loss: 152.375 (lr:0.00013856966227308694)\n",
      "8660: accuracy:0.92 loss: 154.154 (lr:0.0001381858877212313)\n",
      "8680: accuracy:0.95 loss: 151.303 (lr:0.00013780593178996944)\n",
      "8700: accuracy:0.93 loss: 154.505 (lr:0.00013742975648339164)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9369 test loss: 153.46\n",
      "8720: accuracy:0.93 loss: 155.308 (lr:0.00013705732418365372)\n",
      "8740: accuracy:0.93 loss: 155.311 (lr:0.0001366885976472154)\n",
      "8760: accuracy:0.93 loss: 154.479 (lr:0.00013632354000111572)\n",
      "8780: accuracy:0.92 loss: 154.792 (lr:0.00013596211473928589)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8800: accuracy:0.9 loss: 155.277 (lr:0.00013560428571889846)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9364 test loss: 153.462\n",
      "8820: accuracy:0.91 loss: 156.258 (lr:0.00013525001715675332)\n",
      "8840: accuracy:0.89 loss: 157.56 (lr:0.00013489927362569896)\n",
      "8860: accuracy:0.92 loss: 154.976 (lr:0.00013455202005109)\n",
      "8880: accuracy:0.92 loss: 154.798 (lr:0.00013420822170727954)\n",
      "8900: accuracy:0.93 loss: 152.777 (lr:0.0001338678442141468)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.936 test loss: 153.467\n",
      "8920: accuracy:0.87 loss: 159.46 (lr:0.00013353085353365877)\n",
      "8940: accuracy:0.94 loss: 153.008 (lr:0.00013319721596646657)\n",
      "8960: accuracy:0.95 loss: 151.942 (lr:0.00013286689814853543)\n",
      "8980: accuracy:0.89 loss: 158.188 (lr:0.00013253986704780835)\n",
      "9000: accuracy:0.97 loss: 149.958 (lr:0.0001322160899609027)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9359 test loss: 153.457\n",
      "9020: accuracy:0.96 loss: 151.665 (lr:0.00013189553450983996)\n",
      "9040: accuracy:0.95 loss: 152.829 (lr:0.00013157816863880792)\n",
      "9060: accuracy:0.96 loss: 150.644 (lr:0.00013126396061095494)\n",
      "9080: accuracy:0.91 loss: 155.855 (lr:0.00013095287900521648)\n",
      "9100: accuracy:0.93 loss: 155.262 (lr:0.00013064489271317272)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9361 test loss: 153.448\n",
      "9120: accuracy:0.9 loss: 157.046 (lr:0.00013033997093593773)\n",
      "9140: accuracy:0.97 loss: 151.574 (lr:0.00013003808318107972)\n",
      "9160: accuracy:0.93 loss: 154.685 (lr:0.00012973919925957168)\n",
      "9180: accuracy:0.93 loss: 153.511 (lr:0.00012944328928277232)\n",
      "9200: accuracy:0.98 loss: 150.165 (lr:0.0001291503236594374)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9371 test loss: 153.438\n",
      "9220: accuracy:0.96 loss: 152.054 (lr:0.00012886027309276044)\n",
      "9240: accuracy:0.92 loss: 154.906 (lr:0.00012857310857744306)\n",
      "9260: accuracy:0.95 loss: 152.346 (lr:0.00012828880139679441)\n",
      "9280: accuracy:0.94 loss: 153.384 (lr:0.00012800732311985956)\n",
      "9300: accuracy:0.94 loss: 152.949 (lr:0.00012772864559857617)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9363 test loss: 153.439\n",
      "9320: accuracy:0.91 loss: 155.835 (lr:0.00012745274096495995)\n",
      "9340: accuracy:0.94 loss: 152.889 (lr:0.00012717958162831758)\n",
      "9360: accuracy:0.87 loss: 159.585 (lr:0.00012690914027248777)\n",
      "9380: accuracy:0.91 loss: 156.198 (lr:0.00012664138985310952)\n",
      "9400: accuracy:0.97 loss: 150.488 (lr:0.00012637630359491788)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9373 test loss: 153.415\n",
      "9420: accuracy:0.98 loss: 149.437 (lr:0.00012611385498906603)\n",
      "9440: accuracy:0.94 loss: 153.908 (lr:0.00012585401779047472)\n",
      "9460: accuracy:0.96 loss: 152.574 (lr:0.0001255967660152075)\n",
      "9480: accuracy:0.92 loss: 155.859 (lr:0.00012534207393787256)\n",
      "9500: accuracy:0.92 loss: 155.471 (lr:0.00012508991608904985)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9369 test loss: 153.418\n",
      "9520: accuracy:0.94 loss: 152.887 (lr:0.00012484026725274438)\n",
      "9540: accuracy:0.96 loss: 150.577 (lr:0.00012459310246386448)\n",
      "9560: accuracy:0.92 loss: 153.734 (lr:0.00012434839700572526)\n",
      "9580: accuracy:0.97 loss: 150.155 (lr:0.00012410612640757705)\n",
      "9600: accuracy:0.95 loss: 152.234 (lr:0.0001238662664421581)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9364 test loss: 153.414\n",
      "9620: accuracy:0.94 loss: 152.238 (lr:0.00012362879312327198)\n",
      "9640: accuracy:0.91 loss: 156.786 (lr:0.0001233936827033889)\n",
      "9660: accuracy:0.94 loss: 154.234 (lr:0.00012316091167127095)\n",
      "9680: accuracy:0.94 loss: 153.466 (lr:0.000122930456749621)\n",
      "9700: accuracy:0.94 loss: 154.721 (lr:0.00012270229489275475)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9375 test loss: 153.401\n",
      "9720: accuracy:0.95 loss: 153.103 (lr:0.00012247640328429642)\n",
      "9740: accuracy:0.96 loss: 151.117 (lr:0.00012225275933489693)\n",
      "9760: accuracy:0.99 loss: 149.328 (lr:0.00012203134067997495)\n",
      "9780: accuracy:0.95 loss: 152.272 (lr:0.00012181212517748049)\n",
      "9800: accuracy:0.98 loss: 149.074 (lr:0.00012159509090568058)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9373 test loss: 153.392\n",
      "9820: accuracy:0.92 loss: 155.132 (lr:0.00012138021616096724)\n",
      "9840: accuracy:0.97 loss: 151.078 (lr:0.00012116747945568689)\n",
      "9860: accuracy:0.99 loss: 148.78 (lr:0.00012095685951599174)\n",
      "9880: accuracy:0.92 loss: 154.116 (lr:0.00012074833527971228)\n",
      "9900: accuracy:0.97 loss: 150.26 (lr:0.00012054188589425116)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.937 test loss: 153.398\n",
      "9920: accuracy:0.95 loss: 153.014 (lr:0.00012033749071449773)\n",
      "9940: accuracy:0.98 loss: 149.071 (lr:0.00012013512930076374)\n",
      "9960: accuracy:0.97 loss: 151.034 (lr:0.00011993478141673913)\n",
      "9980: accuracy:0.91 loss: 155.703 (lr:0.00011973642702746858)\n",
      "10000: accuracy:0.9 loss: 156.18 (lr:0.00011954004629734786)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9371 test loss: 153.383\n",
      "CPU times: user 1min 17s, sys: 3.62 s, total: 1min 21s\n",
      "Wall time: 35.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93709999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "%time nn_train_var(min_lr=0.0001, max_lr=0.003, h_dim=100, minibatch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
