{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Bayesian Optimization\n",
    "# Optimizing the SGD Learning Rate when Training a Neural Network\n",
    "\n",
    "### Roger Garrida\n",
    "### Akhil Lohia\n",
    "### Daniel Velasquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "\n",
    "Training a neural network can be a difficult task. In particular, due to the large number of hyperparameters that need to be tunned, e.g. number of layers, number of hidden units, batch size among other. In this project, we focus on one particular hyperparameter that influence directly the success of the learning procedure: The stochastic gradient descent **learning rate **. We use bayesian optimization to tune the learning rate and compare the results with a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import pandas as pd\n",
    "import math as mat\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm #add progress bar to for loops\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1,x2,noise,length): #Generate the kernel (cov) of the Gaussian Process\n",
    "    n1 = x1.shape[0]\n",
    "    n2 = x2.shape[0]\n",
    "    kernel = np.zeros((n1,n2))\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            kernel[i,j] = noise**2*mat.exp(-0.5*((x1[i]-x2[j])/length)**2)\n",
    "    return kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LLH_GP(x,y,m,noise,length, sf = 0): #Compute the likelihood of the data (add sf if consider noise)\n",
    "    ker = gaussian_kernel(x,x, noise, length)\n",
    "    ker = ker+np.diag([sf]*len(x))\n",
    "    return 1/2*(mat.log(np.linalg.det(ker))+np.dot(np.dot(np.transpose(y-m),\n",
    "                                                       np.linalg.inv(ker)),(y-m)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_hyparams(x,y): #Find the hyperparameters that optimize LLH without noise\n",
    "    ini = np.array([0,1,1])\n",
    "    opt = optimize.minimize(lambda params: LLH_GP(x, y, params[0], params[1], params[2]),\n",
    "                            ini)\n",
    "    params = opt.x\n",
    "    m = params[0]\n",
    "    noise = abs(params[1])\n",
    "    length = abs(params[2])\n",
    "    sf = 0.001\n",
    "    return m, noise, length, sf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_hyparams_noise(x,y): #Find the hyperparameters that optimize LLH with noise\n",
    "    ini = np.array([0,1,1,1])\n",
    "    opt = optimize.minimize(lambda params: LLH_GP(x, y, params[0], \n",
    "                                                  params[1], params[2], params[3]),ini)\n",
    "    params = opt.x\n",
    "    m = params[0]\n",
    "    noise = abs(params[1])\n",
    "    length = abs(params[2])\n",
    "    sf = abs(params[3])\n",
    "    return m, noise, length, sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generates the mean and covariance of the posterior distribution in the given grid (xn) \n",
    "#from data (x,y) and the optimized parameters (mean->m, noise, length, noise in y -> sf)\n",
    "def gp_posterior(x, y, xn, m, noise, length, sf = 0): \n",
    "    kxx = gaussian_kernel(x, x, noise = noise, length = length)\n",
    "    kxxn = gaussian_kernel(x, xn, noise = noise, length = length)\n",
    "    kxnx = gaussian_kernel(xn, x, noise = noise, length = length)\n",
    "    kxnxn = gaussian_kernel(xn, xn, noise = noise, length = length)\n",
    "    core = np.linalg.inv(kxx + np.diag([sf]*len(x)))\n",
    "    En = np.dot(np.dot(kxnx, core), y)\n",
    "    covn = kxnxn - np.dot(np.dot(kxnx, core), kxxn)  \n",
    "    \n",
    "    return En, covn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_posterior(x, E, cov):\n",
    "    data = pd.DataFrame({'x': x})\n",
    "    data['Mean'] = E\n",
    "    data['StdDev'] = np.sqrt(np.diag(cov))\n",
    "    #Generate the 5 samples as multivariate normals with 0 mean and covariance sigma\n",
    "    for i in range(5):\n",
    "        data['y'+str(i)] = np.random.multivariate_normal(E, cov)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network Architecture and Dataset\n",
    "\n",
    "Our goal is to perform classification on the MNIST dataset. To do so, we build a neural network with one hidden layer and a fixed number of hidden units. We divide the dataset in training and test sample. We define a function that trains the network as a function of the learning rate and returns a measure of accuracy estimated using the test sample. The accuracy corresponds the number of correctly classified observations divided by the total number of observations within the test sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#MNIST dataset:\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, validation_size=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#placeholder:\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "##Variables:\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    #initial = tf.ones(shape)/10\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def neural_network(input, h_dim):\n",
    "    W0 = weight_variable([784, h_dim])\n",
    "    b0 = bias_variable([h_dim])\n",
    "    h = tf.nn.relu(tf.matmul(input, W0) + b0)\n",
    "\n",
    "    W = weight_variable([h_dim, 10])\n",
    "    b = bias_variable([10])\n",
    "\n",
    "    y = tf.nn.softmax(tf.matmul(h, W) + b)\n",
    "    return y\n",
    "\n",
    "#Network training:\n",
    "def nn_train(learning_rate, h_dim, minibatch = 100):\n",
    "    y = neural_network(x, h_dim)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    #cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))*minibatch\n",
    "    #train_step = tf.train.MomentumOptimizer(learning_rate, 0.5).minimize(cross_entropy)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    #n_samples = len(mnist.train.images)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(10000+1):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(minibatch) #These variables are numpy arrays\n",
    "        \n",
    "        if (i%20 == 0): #Training accuracy update\n",
    "            a, c = sess.run([accuracy, cross_entropy], {x: batch_xs, y_: batch_ys})\n",
    "            print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "        \n",
    "        if (i%100 == 0): #Test accuracy update\n",
    "            a, c = sess.run([accuracy, cross_entropy], {x: mnist.test.images, y_: mnist.test.labels})\n",
    "            print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "\n",
    "    return(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "\n",
    "# Network training with variable learning rate\n",
    "\n",
    "def nn_train_var(min_lr, max_lr, h_dim, minibatch = 100):\n",
    "    \n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    \n",
    "    y = neural_network(x, h_dim)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    #cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))*minibatch\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # training step, the learning rate is a placeholder\n",
    "    train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "    #train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(10000+1):\n",
    "        # learning rate decay\n",
    "        decay_speed = 2000.0\n",
    "        learning_rate = min_lr + (max_lr - min_lr) * math.exp(-i/decay_speed)\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(minibatch) #These variables are numpy arrays\n",
    "        \n",
    "        if (i%20 == 0):\n",
    "            a, c = sess.run([accuracy, cross_entropy], {x: batch_xs, y_: batch_ys})\n",
    "            print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "        \n",
    "        if (i%100 == 0):\n",
    "            a, c = sess.run([accuracy, cross_entropy], {x: mnist.test.images, y_: mnist.test.labels})\n",
    "            print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        \n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, lr:learning_rate})\n",
    "\n",
    "    return(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Adquisition Function\n",
    "\n",
    "We start with a gaussian prior on the hyperparameters. We define an adquisicion function that allows us to select a new learning rate to test. In particular we use *Expected Improvement*. With each new value, we train the network and use the output (i.e. the classification accuracy) to update the prior density. Notice that in this case, we want maximize the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adquisition function\n",
    "\n",
    "def acquisition_fun(x, y, xn, mean_vector, sigma_vector):\n",
    "    x_best = x[np.argmax(y)]\n",
    "    y_best = np.max(y)\n",
    "    ind_cand = np.array([not any(abs(xi-x)<=0.00001) for xi in xn]) #Indicator of candidates\n",
    "    gamma = (mean_vector[ind_cand] - y_best)/sigma_vector[ind_cand]\n",
    "    af = (mean_vector[ind_cand] - y_best)* norm.cdf(gamma) + sigma_vector[ind_cand]*norm.pdf(gamma)\n",
    "\n",
    "    x_next = xn[ind_cand][np.argmax(af)]\n",
    "    return x_next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algorithm\n",
    "\n",
    "\n",
    "Initially, we assume we only observe 2 potential learning rates and their corresponding accuracy. We implement an algorithm that, given some prior on the learning rate, at each iteration the acquisition function selects a new candidate learning rate, then trains the network and estimates the posterior density. The plots below shows inital observations, the mean and 1 standard deviation around the mean. \n",
    "\n",
    "When implementing the algorith we can assume that the observed accuracies contains noise or not. Given that in practice the classification accuracy of the neural network is not a deterministic function of the learning rate, we assume the observe values contain some noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.07 loss: 2.31048 (lr:0.1)\n",
      "0: ********* epoch 1 ********* test accuracy:0.0742 test loss: 2.31189\n",
      "20: accuracy:0.15 loss: 2.30191 (lr:0.1)\n",
      "40: accuracy:0.26 loss: 2.27976 (lr:0.1)\n",
      "60: accuracy:0.28 loss: 2.26954 (lr:0.1)\n",
      "80: accuracy:0.3 loss: 2.16096 (lr:0.1)\n",
      "100: accuracy:0.32 loss: 2.13484 (lr:0.1)\n",
      "100: ********* epoch 1 ********* test accuracy:0.3156 test loss: 2.15654\n",
      "120: accuracy:0.4 loss: 2.10834 (lr:0.1)\n",
      "140: accuracy:0.51 loss: 2.06892 (lr:0.1)\n",
      "160: accuracy:0.55 loss: 2.01661 (lr:0.1)\n",
      "180: accuracy:0.58 loss: 1.9898 (lr:0.1)\n",
      "200: accuracy:0.56 loss: 1.99048 (lr:0.1)\n",
      "200: ********* epoch 1 ********* test accuracy:0.5904 test loss: 1.95428\n",
      "220: accuracy:0.62 loss: 1.93321 (lr:0.1)\n",
      "240: accuracy:0.69 loss: 1.85624 (lr:0.1)\n",
      "260: accuracy:0.72 loss: 1.84226 (lr:0.1)\n",
      "280: accuracy:0.76 loss: 1.77519 (lr:0.1)\n",
      "300: accuracy:0.74 loss: 1.79533 (lr:0.1)\n",
      "300: ********* epoch 1 ********* test accuracy:0.7044 test loss: 1.82272\n",
      "320: accuracy:0.62 loss: 1.87094 (lr:0.1)\n",
      "340: accuracy:0.75 loss: 1.75915 (lr:0.1)\n",
      "360: accuracy:0.73 loss: 1.81307 (lr:0.1)\n",
      "380: accuracy:0.79 loss: 1.74513 (lr:0.1)\n",
      "400: accuracy:0.73 loss: 1.8129 (lr:0.1)\n",
      "400: ********* epoch 1 ********* test accuracy:0.7844 test loss: 1.74951\n",
      "420: accuracy:0.85 loss: 1.66921 (lr:0.1)\n",
      "440: accuracy:0.76 loss: 1.75082 (lr:0.1)\n",
      "460: accuracy:0.73 loss: 1.75752 (lr:0.1)\n",
      "480: accuracy:0.83 loss: 1.68959 (lr:0.1)\n",
      "500: accuracy:0.78 loss: 1.70594 (lr:0.1)\n",
      "500: ********* epoch 1 ********* test accuracy:0.7997 test loss: 1.71291\n",
      "520: accuracy:0.75 loss: 1.72758 (lr:0.1)\n",
      "540: accuracy:0.79 loss: 1.70907 (lr:0.1)\n",
      "560: accuracy:0.81 loss: 1.72395 (lr:0.1)\n",
      "580: accuracy:0.79 loss: 1.71486 (lr:0.1)\n",
      "600: accuracy:0.73 loss: 1.75409 (lr:0.1)\n",
      "600: ********* epoch 2 ********* test accuracy:0.8051 test loss: 1.69418\n",
      "620: accuracy:0.69 loss: 1.79042 (lr:0.1)\n",
      "640: accuracy:0.8 loss: 1.70724 (lr:0.1)\n",
      "660: accuracy:0.81 loss: 1.68199 (lr:0.1)\n",
      "680: accuracy:0.79 loss: 1.7059 (lr:0.1)\n",
      "700: accuracy:0.8 loss: 1.7009 (lr:0.1)\n",
      "700: ********* epoch 2 ********* test accuracy:0.8109 test loss: 1.68251\n",
      "720: accuracy:0.76 loss: 1.72367 (lr:0.1)\n",
      "740: accuracy:0.72 loss: 1.75357 (lr:0.1)\n",
      "760: accuracy:0.78 loss: 1.69198 (lr:0.1)\n",
      "780: accuracy:0.85 loss: 1.64812 (lr:0.1)\n",
      "800: accuracy:0.84 loss: 1.63378 (lr:0.1)\n",
      "800: ********* epoch 2 ********* test accuracy:0.8145 test loss: 1.67441\n",
      "820: accuracy:0.88 loss: 1.61601 (lr:0.1)\n",
      "840: accuracy:0.81 loss: 1.67935 (lr:0.1)\n",
      "860: accuracy:0.83 loss: 1.66572 (lr:0.1)\n",
      "880: accuracy:0.8 loss: 1.69597 (lr:0.1)\n",
      "900: accuracy:0.86 loss: 1.62815 (lr:0.1)\n",
      "900: ********* epoch 2 ********* test accuracy:0.8187 test loss: 1.66691\n",
      "920: accuracy:0.83 loss: 1.67116 (lr:0.1)\n",
      "940: accuracy:0.82 loss: 1.63818 (lr:0.1)\n",
      "960: accuracy:0.85 loss: 1.62512 (lr:0.1)\n",
      "980: accuracy:0.92 loss: 1.56733 (lr:0.1)\n",
      "1000: accuracy:0.82 loss: 1.67192 (lr:0.1)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.821 test loss: 1.66257\n",
      "1020: accuracy:0.75 loss: 1.72662 (lr:0.1)\n",
      "1040: accuracy:0.88 loss: 1.61371 (lr:0.1)\n",
      "1060: accuracy:0.77 loss: 1.69675 (lr:0.1)\n",
      "1080: accuracy:0.82 loss: 1.66201 (lr:0.1)\n",
      "1100: accuracy:0.84 loss: 1.6518 (lr:0.1)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.8222 test loss: 1.65872\n",
      "1120: accuracy:0.81 loss: 1.6549 (lr:0.1)\n",
      "1140: accuracy:0.76 loss: 1.69232 (lr:0.1)\n",
      "1160: accuracy:0.82 loss: 1.67038 (lr:0.1)\n",
      "1180: accuracy:0.81 loss: 1.67175 (lr:0.1)\n",
      "1200: accuracy:0.82 loss: 1.66881 (lr:0.1)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.8241 test loss: 1.65584\n",
      "1220: accuracy:0.79 loss: 1.67968 (lr:0.1)\n",
      "1240: accuracy:0.81 loss: 1.66079 (lr:0.1)\n",
      "1260: accuracy:0.89 loss: 1.59953 (lr:0.1)\n",
      "1280: accuracy:0.86 loss: 1.62291 (lr:0.1)\n",
      "1300: accuracy:0.76 loss: 1.70896 (lr:0.1)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.8258 test loss: 1.65289\n",
      "1320: accuracy:0.82 loss: 1.65363 (lr:0.1)\n",
      "1340: accuracy:0.8 loss: 1.66814 (lr:0.1)\n",
      "1360: accuracy:0.79 loss: 1.6849 (lr:0.1)\n",
      "1380: accuracy:0.84 loss: 1.63776 (lr:0.1)\n",
      "1400: accuracy:0.86 loss: 1.61492 (lr:0.1)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.8264 test loss: 1.65034\n",
      "1420: accuracy:0.82 loss: 1.65406 (lr:0.1)\n",
      "1440: accuracy:0.87 loss: 1.60913 (lr:0.1)\n",
      "1460: accuracy:0.81 loss: 1.64991 (lr:0.1)\n",
      "1480: accuracy:0.83 loss: 1.64605 (lr:0.1)\n",
      "1500: accuracy:0.88 loss: 1.6025 (lr:0.1)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.8267 test loss: 1.64848\n",
      "1520: accuracy:0.81 loss: 1.6723 (lr:0.1)\n",
      "1540: accuracy:0.81 loss: 1.66763 (lr:0.1)\n",
      "1560: accuracy:0.9 loss: 1.57344 (lr:0.1)\n",
      "1580: accuracy:0.78 loss: 1.68786 (lr:0.1)\n",
      "1600: accuracy:0.86 loss: 1.63098 (lr:0.1)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.8301 test loss: 1.6458\n",
      "1620: accuracy:0.82 loss: 1.65278 (lr:0.1)\n",
      "1640: accuracy:0.87 loss: 1.6068 (lr:0.1)\n",
      "1660: accuracy:0.78 loss: 1.69002 (lr:0.1)\n",
      "1680: accuracy:0.77 loss: 1.70037 (lr:0.1)\n",
      "1700: accuracy:0.83 loss: 1.64169 (lr:0.1)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.8304 test loss: 1.64413\n",
      "1720: accuracy:0.85 loss: 1.60515 (lr:0.1)\n",
      "1740: accuracy:0.86 loss: 1.60581 (lr:0.1)\n",
      "1760: accuracy:0.79 loss: 1.66926 (lr:0.1)\n",
      "1780: accuracy:0.89 loss: 1.57915 (lr:0.1)\n",
      "1800: accuracy:0.91 loss: 1.5682 (lr:0.1)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.8309 test loss: 1.64253\n",
      "1820: accuracy:0.87 loss: 1.61527 (lr:0.1)\n",
      "1840: accuracy:0.89 loss: 1.57965 (lr:0.1)\n",
      "1860: accuracy:0.89 loss: 1.59469 (lr:0.1)\n",
      "1880: accuracy:0.79 loss: 1.6817 (lr:0.1)\n",
      "1900: accuracy:0.86 loss: 1.60115 (lr:0.1)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.8318 test loss: 1.64095\n",
      "1920: accuracy:0.82 loss: 1.65989 (lr:0.1)\n",
      "1940: accuracy:0.75 loss: 1.71799 (lr:0.1)\n",
      "1960: accuracy:0.84 loss: 1.6251 (lr:0.1)\n",
      "1980: accuracy:0.9 loss: 1.57329 (lr:0.1)\n",
      "2000: accuracy:0.83 loss: 1.65591 (lr:0.1)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.8314 test loss: 1.63999\n",
      "2020: accuracy:0.83 loss: 1.64563 (lr:0.1)\n",
      "2040: accuracy:0.75 loss: 1.71164 (lr:0.1)\n",
      "2060: accuracy:0.86 loss: 1.61599 (lr:0.1)\n",
      "2080: accuracy:0.79 loss: 1.6808 (lr:0.1)\n",
      "2100: accuracy:0.88 loss: 1.60823 (lr:0.1)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.8345 test loss: 1.63869\n",
      "2120: accuracy:0.89 loss: 1.58897 (lr:0.1)\n",
      "2140: accuracy:0.75 loss: 1.70552 (lr:0.1)\n",
      "2160: accuracy:0.79 loss: 1.6646 (lr:0.1)\n",
      "2180: accuracy:0.83 loss: 1.64224 (lr:0.1)\n",
      "2200: accuracy:0.91 loss: 1.56494 (lr:0.1)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.8338 test loss: 1.63779\n",
      "2220: accuracy:0.87 loss: 1.59836 (lr:0.1)\n",
      "2240: accuracy:0.88 loss: 1.58737 (lr:0.1)\n",
      "2260: accuracy:0.88 loss: 1.59091 (lr:0.1)\n",
      "2280: accuracy:0.83 loss: 1.63359 (lr:0.1)\n",
      "2300: accuracy:0.86 loss: 1.60904 (lr:0.1)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.8357 test loss: 1.63648\n",
      "2320: accuracy:0.83 loss: 1.63549 (lr:0.1)\n",
      "2340: accuracy:0.84 loss: 1.65095 (lr:0.1)\n",
      "2360: accuracy:0.81 loss: 1.66441 (lr:0.1)\n",
      "2380: accuracy:0.87 loss: 1.5953 (lr:0.1)\n",
      "2400: accuracy:0.81 loss: 1.65644 (lr:0.1)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.8344 test loss: 1.6362\n",
      "2420: accuracy:0.82 loss: 1.66144 (lr:0.1)\n",
      "2440: accuracy:0.89 loss: 1.58595 (lr:0.1)\n",
      "2460: accuracy:0.79 loss: 1.68467 (lr:0.1)\n",
      "2480: accuracy:0.86 loss: 1.61177 (lr:0.1)\n",
      "2500: accuracy:0.77 loss: 1.68573 (lr:0.1)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.8361 test loss: 1.63462\n",
      "2520: accuracy:0.85 loss: 1.60926 (lr:0.1)\n",
      "2540: accuracy:0.79 loss: 1.67006 (lr:0.1)\n",
      "2560: accuracy:0.81 loss: 1.65359 (lr:0.1)\n",
      "2580: accuracy:0.85 loss: 1.61358 (lr:0.1)\n",
      "2600: accuracy:0.9 loss: 1.58658 (lr:0.1)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.8371 test loss: 1.63327\n",
      "2620: accuracy:0.84 loss: 1.62861 (lr:0.1)\n",
      "2640: accuracy:0.88 loss: 1.59074 (lr:0.1)\n",
      "2660: accuracy:0.77 loss: 1.68072 (lr:0.1)\n",
      "2680: accuracy:0.81 loss: 1.64702 (lr:0.1)\n",
      "2700: accuracy:0.85 loss: 1.62164 (lr:0.1)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.837 test loss: 1.63288\n",
      "2720: accuracy:0.82 loss: 1.64092 (lr:0.1)\n",
      "2740: accuracy:0.84 loss: 1.60686 (lr:0.1)\n",
      "2760: accuracy:0.86 loss: 1.6116 (lr:0.1)\n",
      "2780: accuracy:0.85 loss: 1.63098 (lr:0.1)\n",
      "2800: accuracy:0.88 loss: 1.57843 (lr:0.1)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.8372 test loss: 1.63186\n",
      "2820: accuracy:0.82 loss: 1.65032 (lr:0.1)\n",
      "2840: accuracy:0.86 loss: 1.61604 (lr:0.1)\n",
      "2860: accuracy:0.81 loss: 1.64212 (lr:0.1)\n",
      "2880: accuracy:0.85 loss: 1.63328 (lr:0.1)\n",
      "2900: accuracy:0.83 loss: 1.64068 (lr:0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900: ********* epoch 5 ********* test accuracy:0.8385 test loss: 1.6311\n",
      "2920: accuracy:0.79 loss: 1.68488 (lr:0.1)\n",
      "2940: accuracy:0.83 loss: 1.64225 (lr:0.1)\n",
      "2960: accuracy:0.87 loss: 1.60485 (lr:0.1)\n",
      "2980: accuracy:0.78 loss: 1.68133 (lr:0.1)\n",
      "3000: accuracy:0.88 loss: 1.59112 (lr:0.1)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.8384 test loss: 1.6306\n",
      "3020: accuracy:0.77 loss: 1.69307 (lr:0.1)\n",
      "3040: accuracy:0.85 loss: 1.62494 (lr:0.1)\n",
      "3060: accuracy:0.89 loss: 1.57257 (lr:0.1)\n",
      "3080: accuracy:0.82 loss: 1.6402 (lr:0.1)\n",
      "3100: accuracy:0.83 loss: 1.64439 (lr:0.1)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.839 test loss: 1.62979\n",
      "3120: accuracy:0.87 loss: 1.61065 (lr:0.1)\n",
      "3140: accuracy:0.85 loss: 1.62278 (lr:0.1)\n",
      "3160: accuracy:0.86 loss: 1.61127 (lr:0.1)\n",
      "3180: accuracy:0.83 loss: 1.66041 (lr:0.1)\n",
      "3200: accuracy:0.88 loss: 1.5859 (lr:0.1)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.8401 test loss: 1.6293\n",
      "3220: accuracy:0.83 loss: 1.64063 (lr:0.1)\n",
      "3240: accuracy:0.86 loss: 1.60694 (lr:0.1)\n",
      "3260: accuracy:0.87 loss: 1.60484 (lr:0.1)\n",
      "3280: accuracy:0.85 loss: 1.61937 (lr:0.1)\n",
      "3300: accuracy:0.84 loss: 1.62329 (lr:0.1)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.8405 test loss: 1.62854\n",
      "3320: accuracy:0.88 loss: 1.5942 (lr:0.1)\n",
      "3340: accuracy:0.88 loss: 1.58698 (lr:0.1)\n",
      "3360: accuracy:0.87 loss: 1.59993 (lr:0.1)\n",
      "3380: accuracy:0.86 loss: 1.60645 (lr:0.1)\n",
      "3400: accuracy:0.86 loss: 1.60954 (lr:0.1)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.8409 test loss: 1.62802\n",
      "3420: accuracy:0.81 loss: 1.65792 (lr:0.1)\n",
      "3440: accuracy:0.88 loss: 1.59351 (lr:0.1)\n",
      "3460: accuracy:0.8 loss: 1.65486 (lr:0.1)\n",
      "3480: accuracy:0.78 loss: 1.68873 (lr:0.1)\n",
      "3500: accuracy:0.88 loss: 1.58543 (lr:0.1)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.8404 test loss: 1.62703\n",
      "3520: accuracy:0.83 loss: 1.63555 (lr:0.1)\n",
      "3540: accuracy:0.86 loss: 1.60405 (lr:0.1)\n",
      "3560: accuracy:0.91 loss: 1.55185 (lr:0.1)\n",
      "3580: accuracy:0.81 loss: 1.65336 (lr:0.1)\n",
      "3600: accuracy:0.85 loss: 1.63056 (lr:0.1)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.8419 test loss: 1.62645\n",
      "3620: accuracy:0.85 loss: 1.62008 (lr:0.1)\n",
      "3640: accuracy:0.86 loss: 1.60523 (lr:0.1)\n",
      "3660: accuracy:0.85 loss: 1.62762 (lr:0.1)\n",
      "3680: accuracy:0.8 loss: 1.67295 (lr:0.1)\n",
      "3700: accuracy:0.87 loss: 1.61729 (lr:0.1)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.8422 test loss: 1.62642\n",
      "3720: accuracy:0.87 loss: 1.5922 (lr:0.1)\n",
      "3740: accuracy:0.81 loss: 1.65718 (lr:0.1)\n",
      "3760: accuracy:0.85 loss: 1.62511 (lr:0.1)\n",
      "3780: accuracy:0.81 loss: 1.66598 (lr:0.1)\n",
      "3800: accuracy:0.81 loss: 1.65154 (lr:0.1)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.8416 test loss: 1.62595\n",
      "3820: accuracy:0.89 loss: 1.58005 (lr:0.1)\n",
      "3840: accuracy:0.81 loss: 1.65746 (lr:0.1)\n",
      "3860: accuracy:0.86 loss: 1.60002 (lr:0.1)\n",
      "3880: accuracy:0.82 loss: 1.63625 (lr:0.1)\n",
      "3900: accuracy:0.85 loss: 1.62142 (lr:0.1)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.8413 test loss: 1.62589\n",
      "3920: accuracy:0.9 loss: 1.5592 (lr:0.1)\n",
      "3940: accuracy:0.81 loss: 1.66441 (lr:0.1)\n",
      "3960: accuracy:0.84 loss: 1.627 (lr:0.1)\n",
      "3980: accuracy:0.85 loss: 1.61815 (lr:0.1)\n",
      "4000: accuracy:0.82 loss: 1.64624 (lr:0.1)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.8414 test loss: 1.62555\n",
      "4020: accuracy:0.81 loss: 1.66432 (lr:0.1)\n",
      "4040: accuracy:0.82 loss: 1.66581 (lr:0.1)\n",
      "4060: accuracy:0.84 loss: 1.6259 (lr:0.1)\n",
      "4080: accuracy:0.78 loss: 1.6797 (lr:0.1)\n",
      "4100: accuracy:0.84 loss: 1.63204 (lr:0.1)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.8433 test loss: 1.62449\n",
      "4120: accuracy:0.8 loss: 1.67895 (lr:0.1)\n",
      "4140: accuracy:0.85 loss: 1.61652 (lr:0.1)\n",
      "4160: accuracy:0.84 loss: 1.64185 (lr:0.1)\n",
      "4180: accuracy:0.86 loss: 1.6082 (lr:0.1)\n",
      "4200: accuracy:0.83 loss: 1.64742 (lr:0.1)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.8439 test loss: 1.62359\n",
      "4220: accuracy:0.85 loss: 1.62475 (lr:0.1)\n",
      "4240: accuracy:0.85 loss: 1.61737 (lr:0.1)\n",
      "4260: accuracy:0.84 loss: 1.62163 (lr:0.1)\n",
      "4280: accuracy:0.8 loss: 1.65835 (lr:0.1)\n",
      "4300: accuracy:0.87 loss: 1.59642 (lr:0.1)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.8447 test loss: 1.62357\n",
      "4320: accuracy:0.85 loss: 1.62405 (lr:0.1)\n",
      "4340: accuracy:0.84 loss: 1.62469 (lr:0.1)\n",
      "4360: accuracy:0.87 loss: 1.6086 (lr:0.1)\n",
      "4380: accuracy:0.83 loss: 1.64606 (lr:0.1)\n",
      "4400: accuracy:0.83 loss: 1.63319 (lr:0.1)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.844 test loss: 1.62332\n",
      "4420: accuracy:0.82 loss: 1.63476 (lr:0.1)\n",
      "4440: accuracy:0.86 loss: 1.61116 (lr:0.1)\n",
      "4460: accuracy:0.92 loss: 1.55669 (lr:0.1)\n",
      "4480: accuracy:0.77 loss: 1.68955 (lr:0.1)\n",
      "4500: accuracy:0.86 loss: 1.62724 (lr:0.1)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.844 test loss: 1.62276\n",
      "4520: accuracy:0.84 loss: 1.6242 (lr:0.1)\n",
      "4540: accuracy:0.87 loss: 1.60947 (lr:0.1)\n",
      "4560: accuracy:0.86 loss: 1.61198 (lr:0.1)\n",
      "4580: accuracy:0.81 loss: 1.6542 (lr:0.1)\n",
      "4600: accuracy:0.86 loss: 1.60797 (lr:0.1)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.8442 test loss: 1.62225\n",
      "4620: accuracy:0.79 loss: 1.67816 (lr:0.1)\n",
      "4640: accuracy:0.83 loss: 1.64017 (lr:0.1)\n",
      "4660: accuracy:0.86 loss: 1.61548 (lr:0.1)\n",
      "4680: accuracy:0.93 loss: 1.54906 (lr:0.1)\n",
      "4700: accuracy:0.82 loss: 1.65105 (lr:0.1)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.8453 test loss: 1.62156\n",
      "4720: accuracy:0.83 loss: 1.62271 (lr:0.1)\n",
      "4740: accuracy:0.79 loss: 1.68453 (lr:0.1)\n",
      "4760: accuracy:0.8 loss: 1.656 (lr:0.1)\n",
      "4780: accuracy:0.86 loss: 1.5969 (lr:0.1)\n",
      "4800: accuracy:0.82 loss: 1.65095 (lr:0.1)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.8437 test loss: 1.62196\n",
      "4820: accuracy:0.78 loss: 1.67852 (lr:0.1)\n",
      "4840: accuracy:0.84 loss: 1.64055 (lr:0.1)\n",
      "4860: accuracy:0.89 loss: 1.59135 (lr:0.1)\n",
      "4880: accuracy:0.8 loss: 1.66743 (lr:0.1)\n",
      "4900: accuracy:0.87 loss: 1.60157 (lr:0.1)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.8456 test loss: 1.62164\n",
      "4920: accuracy:0.88 loss: 1.59215 (lr:0.1)\n",
      "4940: accuracy:0.87 loss: 1.58851 (lr:0.1)\n",
      "4960: accuracy:0.86 loss: 1.61042 (lr:0.1)\n",
      "4980: accuracy:0.85 loss: 1.61459 (lr:0.1)\n",
      "5000: accuracy:0.88 loss: 1.59717 (lr:0.1)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.846 test loss: 1.62096\n",
      "5020: accuracy:0.86 loss: 1.59661 (lr:0.1)\n",
      "5040: accuracy:0.8 loss: 1.6635 (lr:0.1)\n",
      "5060: accuracy:0.84 loss: 1.63022 (lr:0.1)\n",
      "5080: accuracy:0.83 loss: 1.63857 (lr:0.1)\n",
      "5100: accuracy:0.86 loss: 1.6174 (lr:0.1)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.8452 test loss: 1.62088\n",
      "5120: accuracy:0.86 loss: 1.61383 (lr:0.1)\n",
      "5140: accuracy:0.76 loss: 1.679 (lr:0.1)\n",
      "5160: accuracy:0.89 loss: 1.57698 (lr:0.1)\n",
      "5180: accuracy:0.85 loss: 1.61169 (lr:0.1)\n",
      "5200: accuracy:0.79 loss: 1.66622 (lr:0.1)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.8462 test loss: 1.62009\n",
      "5220: accuracy:0.86 loss: 1.61287 (lr:0.1)\n",
      "5240: accuracy:0.8 loss: 1.65055 (lr:0.1)\n",
      "5260: accuracy:0.83 loss: 1.62992 (lr:0.1)\n",
      "5280: accuracy:0.88 loss: 1.58942 (lr:0.1)\n",
      "5300: accuracy:0.82 loss: 1.64813 (lr:0.1)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.8466 test loss: 1.6197\n",
      "5320: accuracy:0.92 loss: 1.53756 (lr:0.1)\n",
      "5340: accuracy:0.88 loss: 1.58754 (lr:0.1)\n",
      "5360: accuracy:0.87 loss: 1.59832 (lr:0.1)\n",
      "5380: accuracy:0.84 loss: 1.62397 (lr:0.1)\n",
      "5400: accuracy:0.86 loss: 1.59086 (lr:0.1)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.847 test loss: 1.61941\n",
      "5420: accuracy:0.85 loss: 1.62469 (lr:0.1)\n",
      "5440: accuracy:0.89 loss: 1.58117 (lr:0.1)\n",
      "5460: accuracy:0.86 loss: 1.60332 (lr:0.1)\n",
      "5480: accuracy:0.86 loss: 1.61002 (lr:0.1)\n",
      "5500: accuracy:0.8 loss: 1.66898 (lr:0.1)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.8475 test loss: 1.61901\n",
      "5520: accuracy:0.88 loss: 1.58196 (lr:0.1)\n",
      "5540: accuracy:0.88 loss: 1.58717 (lr:0.1)\n",
      "5560: accuracy:0.86 loss: 1.61266 (lr:0.1)\n",
      "5580: accuracy:0.86 loss: 1.61078 (lr:0.1)\n",
      "5600: accuracy:0.9 loss: 1.57514 (lr:0.1)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.8473 test loss: 1.61851\n",
      "5620: accuracy:0.84 loss: 1.62109 (lr:0.1)\n",
      "5640: accuracy:0.9 loss: 1.57426 (lr:0.1)\n",
      "5660: accuracy:0.82 loss: 1.63757 (lr:0.1)\n",
      "5680: accuracy:0.87 loss: 1.60199 (lr:0.1)\n",
      "5700: accuracy:0.85 loss: 1.62858 (lr:0.1)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.8472 test loss: 1.61847\n",
      "5720: accuracy:0.8 loss: 1.68377 (lr:0.1)\n",
      "5740: accuracy:0.78 loss: 1.68437 (lr:0.1)\n",
      "5760: accuracy:0.9 loss: 1.56998 (lr:0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5780: accuracy:0.83 loss: 1.63642 (lr:0.1)\n",
      "5800: accuracy:0.81 loss: 1.6547 (lr:0.1)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.8479 test loss: 1.61819\n",
      "5820: accuracy:0.8 loss: 1.65859 (lr:0.1)\n",
      "5840: accuracy:0.8 loss: 1.65328 (lr:0.1)\n",
      "5860: accuracy:0.86 loss: 1.61417 (lr:0.1)\n",
      "5880: accuracy:0.86 loss: 1.60774 (lr:0.1)\n",
      "5900: accuracy:0.77 loss: 1.68884 (lr:0.1)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.8489 test loss: 1.61745\n",
      "5920: accuracy:0.85 loss: 1.62072 (lr:0.1)\n",
      "5940: accuracy:0.83 loss: 1.61628 (lr:0.1)\n",
      "5960: accuracy:0.81 loss: 1.65486 (lr:0.1)\n",
      "5980: accuracy:0.88 loss: 1.57422 (lr:0.1)\n",
      "6000: accuracy:0.86 loss: 1.60443 (lr:0.1)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.8489 test loss: 1.61732\n",
      "6020: accuracy:0.86 loss: 1.5945 (lr:0.1)\n",
      "6040: accuracy:0.82 loss: 1.63735 (lr:0.1)\n",
      "6060: accuracy:0.82 loss: 1.64595 (lr:0.1)\n",
      "6080: accuracy:0.83 loss: 1.62738 (lr:0.1)\n",
      "6100: accuracy:0.82 loss: 1.64234 (lr:0.1)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.8497 test loss: 1.61673\n",
      "6120: accuracy:0.83 loss: 1.63912 (lr:0.1)\n",
      "6140: accuracy:0.8 loss: 1.66484 (lr:0.1)\n",
      "6160: accuracy:0.82 loss: 1.63412 (lr:0.1)\n",
      "6180: accuracy:0.84 loss: 1.6296 (lr:0.1)\n",
      "6200: accuracy:0.8 loss: 1.65856 (lr:0.1)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.8495 test loss: 1.61692\n",
      "6220: accuracy:0.84 loss: 1.62258 (lr:0.1)\n",
      "6240: accuracy:0.89 loss: 1.59111 (lr:0.1)\n",
      "6260: accuracy:0.83 loss: 1.64378 (lr:0.1)\n",
      "6280: accuracy:0.85 loss: 1.6128 (lr:0.1)\n",
      "6300: accuracy:0.85 loss: 1.60811 (lr:0.1)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.8495 test loss: 1.61651\n",
      "6320: accuracy:0.83 loss: 1.63751 (lr:0.1)\n",
      "6340: accuracy:0.92 loss: 1.55215 (lr:0.1)\n",
      "6360: accuracy:0.84 loss: 1.62445 (lr:0.1)\n",
      "6380: accuracy:0.87 loss: 1.5942 (lr:0.1)\n",
      "6400: accuracy:0.85 loss: 1.60359 (lr:0.1)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.8496 test loss: 1.6162\n",
      "6420: accuracy:0.88 loss: 1.58674 (lr:0.1)\n",
      "6440: accuracy:0.87 loss: 1.61095 (lr:0.1)\n",
      "6460: accuracy:0.87 loss: 1.59708 (lr:0.1)\n",
      "6480: accuracy:0.87 loss: 1.6 (lr:0.1)\n",
      "6500: accuracy:0.87 loss: 1.60516 (lr:0.1)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.8493 test loss: 1.61584\n",
      "6520: accuracy:0.83 loss: 1.62978 (lr:0.1)\n",
      "6540: accuracy:0.81 loss: 1.65466 (lr:0.1)\n",
      "6560: accuracy:0.88 loss: 1.58832 (lr:0.1)\n",
      "6580: accuracy:0.9 loss: 1.57059 (lr:0.1)\n",
      "6600: accuracy:0.87 loss: 1.60129 (lr:0.1)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.8493 test loss: 1.61605\n",
      "6620: accuracy:0.84 loss: 1.62606 (lr:0.1)\n",
      "6640: accuracy:0.86 loss: 1.6062 (lr:0.1)\n",
      "6660: accuracy:0.85 loss: 1.61507 (lr:0.1)\n",
      "6680: accuracy:0.81 loss: 1.64967 (lr:0.1)\n",
      "6700: accuracy:0.87 loss: 1.59054 (lr:0.1)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.85 test loss: 1.61534\n",
      "6720: accuracy:0.79 loss: 1.67295 (lr:0.1)\n",
      "6740: accuracy:0.87 loss: 1.59519 (lr:0.1)\n",
      "6760: accuracy:0.77 loss: 1.6827 (lr:0.1)\n",
      "6780: accuracy:0.84 loss: 1.61432 (lr:0.1)\n",
      "6800: accuracy:0.85 loss: 1.6166 (lr:0.1)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.8494 test loss: 1.61543\n",
      "6820: accuracy:0.82 loss: 1.64323 (lr:0.1)\n",
      "6840: accuracy:0.89 loss: 1.58228 (lr:0.1)\n",
      "6860: accuracy:0.8 loss: 1.67527 (lr:0.1)\n",
      "6880: accuracy:0.88 loss: 1.58164 (lr:0.1)\n",
      "6900: accuracy:0.82 loss: 1.64724 (lr:0.1)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.8506 test loss: 1.61491\n",
      "6920: accuracy:0.87 loss: 1.58435 (lr:0.1)\n",
      "6940: accuracy:0.88 loss: 1.58794 (lr:0.1)\n",
      "6960: accuracy:0.84 loss: 1.636 (lr:0.1)\n",
      "6980: accuracy:0.84 loss: 1.63623 (lr:0.1)\n",
      "7000: accuracy:0.82 loss: 1.65455 (lr:0.1)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.8504 test loss: 1.61499\n",
      "7020: accuracy:0.87 loss: 1.6014 (lr:0.1)\n",
      "7040: accuracy:0.82 loss: 1.64593 (lr:0.1)\n",
      "7060: accuracy:0.89 loss: 1.58154 (lr:0.1)\n",
      "7080: accuracy:0.81 loss: 1.6429 (lr:0.1)\n",
      "7100: accuracy:0.86 loss: 1.60282 (lr:0.1)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.8514 test loss: 1.61412\n",
      "7120: accuracy:0.82 loss: 1.64593 (lr:0.1)\n",
      "7140: accuracy:0.9 loss: 1.58916 (lr:0.1)\n",
      "7160: accuracy:0.81 loss: 1.6399 (lr:0.1)\n",
      "7180: accuracy:0.88 loss: 1.59635 (lr:0.1)\n",
      "7200: accuracy:0.81 loss: 1.66083 (lr:0.1)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.8514 test loss: 1.61401\n",
      "7220: accuracy:0.87 loss: 1.59445 (lr:0.1)\n",
      "7240: accuracy:0.83 loss: 1.63158 (lr:0.1)\n",
      "7260: accuracy:0.79 loss: 1.67352 (lr:0.1)\n",
      "7280: accuracy:0.85 loss: 1.60992 (lr:0.1)\n",
      "7300: accuracy:0.89 loss: 1.59689 (lr:0.1)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.8502 test loss: 1.61462\n",
      "7320: accuracy:0.88 loss: 1.59483 (lr:0.1)\n",
      "7340: accuracy:0.81 loss: 1.65557 (lr:0.1)\n",
      "7360: accuracy:0.87 loss: 1.59314 (lr:0.1)\n",
      "7380: accuracy:0.84 loss: 1.62797 (lr:0.1)\n",
      "7400: accuracy:0.86 loss: 1.60852 (lr:0.1)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.8524 test loss: 1.61344\n",
      "7420: accuracy:0.88 loss: 1.57588 (lr:0.1)\n",
      "7440: accuracy:0.79 loss: 1.6878 (lr:0.1)\n",
      "7460: accuracy:0.82 loss: 1.63457 (lr:0.1)\n",
      "7480: accuracy:0.84 loss: 1.62329 (lr:0.1)\n",
      "7500: accuracy:0.83 loss: 1.63245 (lr:0.1)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.8515 test loss: 1.61305\n",
      "7520: accuracy:0.79 loss: 1.66222 (lr:0.1)\n",
      "7540: accuracy:0.87 loss: 1.5978 (lr:0.1)\n",
      "7560: accuracy:0.89 loss: 1.57529 (lr:0.1)\n",
      "7580: accuracy:0.93 loss: 1.54101 (lr:0.1)\n",
      "7600: accuracy:0.92 loss: 1.5579 (lr:0.1)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.8509 test loss: 1.61333\n",
      "7620: accuracy:0.88 loss: 1.58966 (lr:0.1)\n",
      "7640: accuracy:0.86 loss: 1.60701 (lr:0.1)\n",
      "7660: accuracy:0.86 loss: 1.60774 (lr:0.1)\n",
      "7680: accuracy:0.87 loss: 1.59928 (lr:0.1)\n",
      "7700: accuracy:0.89 loss: 1.57502 (lr:0.1)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.8511 test loss: 1.61339\n",
      "7720: accuracy:0.85 loss: 1.61268 (lr:0.1)\n",
      "7740: accuracy:0.85 loss: 1.60454 (lr:0.1)\n",
      "7760: accuracy:0.86 loss: 1.61177 (lr:0.1)\n",
      "7780: accuracy:0.89 loss: 1.57537 (lr:0.1)\n",
      "7800: accuracy:0.87 loss: 1.60473 (lr:0.1)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.8511 test loss: 1.61262\n",
      "7820: accuracy:0.88 loss: 1.59559 (lr:0.1)\n",
      "7840: accuracy:0.8 loss: 1.6598 (lr:0.1)\n",
      "7860: accuracy:0.89 loss: 1.57414 (lr:0.1)\n",
      "7880: accuracy:0.86 loss: 1.6079 (lr:0.1)\n",
      "7900: accuracy:0.87 loss: 1.59905 (lr:0.1)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.8522 test loss: 1.6127\n",
      "7920: accuracy:0.83 loss: 1.63184 (lr:0.1)\n",
      "7940: accuracy:0.86 loss: 1.59278 (lr:0.1)\n",
      "7960: accuracy:0.85 loss: 1.60464 (lr:0.1)\n",
      "7980: accuracy:0.93 loss: 1.53622 (lr:0.1)\n",
      "8000: accuracy:0.9 loss: 1.57376 (lr:0.1)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.8526 test loss: 1.61253\n",
      "8020: accuracy:0.84 loss: 1.62345 (lr:0.1)\n",
      "8040: accuracy:0.86 loss: 1.60485 (lr:0.1)\n",
      "8060: accuracy:0.86 loss: 1.60052 (lr:0.1)\n",
      "8080: accuracy:0.81 loss: 1.65835 (lr:0.1)\n",
      "8100: accuracy:0.82 loss: 1.64443 (lr:0.1)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.8527 test loss: 1.612\n",
      "8120: accuracy:0.84 loss: 1.63082 (lr:0.1)\n",
      "8140: accuracy:0.87 loss: 1.59609 (lr:0.1)\n",
      "8160: accuracy:0.82 loss: 1.65462 (lr:0.1)\n",
      "8180: accuracy:0.92 loss: 1.55068 (lr:0.1)\n",
      "8200: accuracy:0.86 loss: 1.60996 (lr:0.1)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.8533 test loss: 1.61161\n",
      "8220: accuracy:0.83 loss: 1.62922 (lr:0.1)\n",
      "8240: accuracy:0.78 loss: 1.6757 (lr:0.1)\n",
      "8260: accuracy:0.91 loss: 1.55586 (lr:0.1)\n",
      "8280: accuracy:0.81 loss: 1.65135 (lr:0.1)\n",
      "8300: accuracy:0.86 loss: 1.60241 (lr:0.1)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.8528 test loss: 1.61176\n",
      "8320: accuracy:0.83 loss: 1.64751 (lr:0.1)\n",
      "8340: accuracy:0.85 loss: 1.60966 (lr:0.1)\n",
      "8360: accuracy:0.89 loss: 1.58805 (lr:0.1)\n",
      "8380: accuracy:0.84 loss: 1.62266 (lr:0.1)\n",
      "8400: accuracy:0.81 loss: 1.64871 (lr:0.1)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.8539 test loss: 1.61144\n",
      "8420: accuracy:0.87 loss: 1.60234 (lr:0.1)\n",
      "8440: accuracy:0.88 loss: 1.57685 (lr:0.1)\n",
      "8460: accuracy:0.94 loss: 1.53713 (lr:0.1)\n",
      "8480: accuracy:0.84 loss: 1.61918 (lr:0.1)\n",
      "8500: accuracy:0.86 loss: 1.59913 (lr:0.1)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.853 test loss: 1.61142\n",
      "8520: accuracy:0.83 loss: 1.62771 (lr:0.1)\n",
      "8540: accuracy:0.84 loss: 1.62538 (lr:0.1)\n",
      "8560: accuracy:0.87 loss: 1.59327 (lr:0.1)\n",
      "8580: accuracy:0.85 loss: 1.61465 (lr:0.1)\n",
      "8600: accuracy:0.89 loss: 1.58642 (lr:0.1)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.8532 test loss: 1.61099\n",
      "8620: accuracy:0.8 loss: 1.66111 (lr:0.1)\n",
      "8640: accuracy:0.89 loss: 1.58312 (lr:0.1)\n",
      "8660: accuracy:0.8 loss: 1.66026 (lr:0.1)\n",
      "8680: accuracy:0.85 loss: 1.62587 (lr:0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8700: accuracy:0.82 loss: 1.65339 (lr:0.1)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.854 test loss: 1.61078\n",
      "8720: accuracy:0.9 loss: 1.56724 (lr:0.1)\n",
      "8740: accuracy:0.85 loss: 1.62144 (lr:0.1)\n",
      "8760: accuracy:0.81 loss: 1.65183 (lr:0.1)\n",
      "8780: accuracy:0.91 loss: 1.55472 (lr:0.1)\n",
      "8800: accuracy:0.87 loss: 1.59935 (lr:0.1)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.8536 test loss: 1.61081\n",
      "8820: accuracy:0.85 loss: 1.6162 (lr:0.1)\n",
      "8840: accuracy:0.83 loss: 1.63476 (lr:0.1)\n",
      "8860: accuracy:0.85 loss: 1.61358 (lr:0.1)\n",
      "8880: accuracy:0.84 loss: 1.61955 (lr:0.1)\n",
      "8900: accuracy:0.85 loss: 1.61509 (lr:0.1)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.8552 test loss: 1.6101\n",
      "8920: accuracy:0.89 loss: 1.58323 (lr:0.1)\n",
      "8940: accuracy:0.84 loss: 1.61237 (lr:0.1)\n",
      "8960: accuracy:0.91 loss: 1.56357 (lr:0.1)\n",
      "8980: accuracy:0.8 loss: 1.65379 (lr:0.1)\n",
      "9000: accuracy:0.89 loss: 1.57475 (lr:0.1)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.8544 test loss: 1.60991\n",
      "9020: accuracy:0.89 loss: 1.5773 (lr:0.1)\n",
      "9040: accuracy:0.86 loss: 1.60363 (lr:0.1)\n",
      "9060: accuracy:0.85 loss: 1.62191 (lr:0.1)\n",
      "9080: accuracy:0.93 loss: 1.54558 (lr:0.1)\n",
      "9100: accuracy:0.81 loss: 1.65181 (lr:0.1)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.8542 test loss: 1.61013\n",
      "9120: accuracy:0.82 loss: 1.65054 (lr:0.1)\n",
      "9140: accuracy:0.88 loss: 1.58199 (lr:0.1)\n",
      "9160: accuracy:0.89 loss: 1.57541 (lr:0.1)\n",
      "9180: accuracy:0.88 loss: 1.58247 (lr:0.1)\n",
      "9200: accuracy:0.85 loss: 1.61049 (lr:0.1)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.8552 test loss: 1.60969\n",
      "9220: accuracy:0.85 loss: 1.6168 (lr:0.1)\n",
      "9240: accuracy:0.91 loss: 1.564 (lr:0.1)\n",
      "9260: accuracy:0.83 loss: 1.63519 (lr:0.1)\n",
      "9280: accuracy:0.88 loss: 1.58511 (lr:0.1)\n",
      "9300: accuracy:0.87 loss: 1.59643 (lr:0.1)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.8544 test loss: 1.60979\n",
      "9320: accuracy:0.82 loss: 1.64866 (lr:0.1)\n",
      "9340: accuracy:0.8 loss: 1.65872 (lr:0.1)\n",
      "9360: accuracy:0.88 loss: 1.58621 (lr:0.1)\n",
      "9380: accuracy:0.82 loss: 1.64446 (lr:0.1)\n",
      "9400: accuracy:0.87 loss: 1.60245 (lr:0.1)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.8545 test loss: 1.60935\n",
      "9420: accuracy:0.81 loss: 1.64842 (lr:0.1)\n",
      "9440: accuracy:0.93 loss: 1.54016 (lr:0.1)\n",
      "9460: accuracy:0.84 loss: 1.62518 (lr:0.1)\n",
      "9480: accuracy:0.83 loss: 1.63469 (lr:0.1)\n",
      "9500: accuracy:0.86 loss: 1.60283 (lr:0.1)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.8553 test loss: 1.60905\n",
      "9520: accuracy:0.9 loss: 1.56884 (lr:0.1)\n",
      "9540: accuracy:0.76 loss: 1.68695 (lr:0.1)\n",
      "9560: accuracy:0.85 loss: 1.61274 (lr:0.1)\n",
      "9580: accuracy:0.85 loss: 1.60669 (lr:0.1)\n",
      "9600: accuracy:0.88 loss: 1.59659 (lr:0.1)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.8558 test loss: 1.60866\n",
      "9620: accuracy:0.86 loss: 1.60849 (lr:0.1)\n",
      "9640: accuracy:0.89 loss: 1.58831 (lr:0.1)\n",
      "9660: accuracy:0.84 loss: 1.62159 (lr:0.1)\n",
      "9680: accuracy:0.9 loss: 1.56706 (lr:0.1)\n",
      "9700: accuracy:0.94 loss: 1.52651 (lr:0.1)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.8554 test loss: 1.60923\n",
      "9720: accuracy:0.83 loss: 1.6279 (lr:0.1)\n",
      "9740: accuracy:0.86 loss: 1.60786 (lr:0.1)\n",
      "9760: accuracy:0.82 loss: 1.64496 (lr:0.1)\n",
      "9780: accuracy:0.79 loss: 1.67277 (lr:0.1)\n",
      "9800: accuracy:0.86 loss: 1.59781 (lr:0.1)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.8561 test loss: 1.60852\n",
      "9820: accuracy:0.87 loss: 1.59983 (lr:0.1)\n",
      "9840: accuracy:0.78 loss: 1.68448 (lr:0.1)\n",
      "9860: accuracy:0.89 loss: 1.5779 (lr:0.1)\n",
      "9880: accuracy:0.87 loss: 1.58639 (lr:0.1)\n",
      "9900: accuracy:0.9 loss: 1.56218 (lr:0.1)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.8558 test loss: 1.60824\n",
      "9920: accuracy:0.87 loss: 1.60089 (lr:0.1)\n",
      "9940: accuracy:0.84 loss: 1.62184 (lr:0.1)\n",
      "9960: accuracy:0.79 loss: 1.66272 (lr:0.1)\n",
      "9980: accuracy:0.85 loss: 1.61592 (lr:0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:27<00:27, 27.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: accuracy:0.91 loss: 1.56432 (lr:0.1)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.8563 test loss: 1.60805\n",
      "0: accuracy:0.09 loss: 2.31029 (lr:8.0)\n",
      "0: ********* epoch 1 ********* test accuracy:0.0882 test loss: 2.31303\n",
      "20: accuracy:0.54 loss: 1.917 (lr:8.0)\n",
      "40: accuracy:0.72 loss: 1.7228 (lr:8.0)\n",
      "60: accuracy:0.64 loss: 1.81264 (lr:8.0)\n",
      "80: accuracy:0.86 loss: 1.60035 (lr:8.0)\n",
      "100: accuracy:0.89 loss: 1.58409 (lr:8.0)\n",
      "100: ********* epoch 1 ********* test accuracy:0.8727 test loss: 1.59142\n",
      "120: accuracy:0.87 loss: 1.597 (lr:8.0)\n",
      "140: accuracy:0.88 loss: 1.58075 (lr:8.0)\n",
      "160: accuracy:0.85 loss: 1.62107 (lr:8.0)\n",
      "180: accuracy:0.83 loss: 1.6366 (lr:8.0)\n",
      "200: accuracy:0.84 loss: 1.6239 (lr:8.0)\n",
      "200: ********* epoch 1 ********* test accuracy:0.8685 test loss: 1.59294\n",
      "220: accuracy:0.9 loss: 1.55695 (lr:8.0)\n",
      "240: accuracy:0.83 loss: 1.61964 (lr:8.0)\n",
      "260: accuracy:0.9 loss: 1.55555 (lr:8.0)\n",
      "280: accuracy:0.85 loss: 1.60432 (lr:8.0)\n",
      "300: accuracy:0.9 loss: 1.56097 (lr:8.0)\n",
      "300: ********* epoch 1 ********* test accuracy:0.9096 test loss: 1.5521\n",
      "320: accuracy:0.92 loss: 1.54454 (lr:8.0)\n",
      "340: accuracy:0.89 loss: 1.5743 (lr:8.0)\n",
      "360: accuracy:0.93 loss: 1.52822 (lr:8.0)\n",
      "380: accuracy:0.89 loss: 1.56588 (lr:8.0)\n",
      "400: accuracy:0.94 loss: 1.52637 (lr:8.0)\n",
      "400: ********* epoch 1 ********* test accuracy:0.9224 test loss: 1.53862\n",
      "420: accuracy:0.97 loss: 1.49202 (lr:8.0)\n",
      "440: accuracy:0.88 loss: 1.57125 (lr:8.0)\n",
      "460: accuracy:0.94 loss: 1.52663 (lr:8.0)\n",
      "480: accuracy:0.97 loss: 1.49596 (lr:8.0)\n",
      "500: accuracy:0.91 loss: 1.55053 (lr:8.0)\n",
      "500: ********* epoch 1 ********* test accuracy:0.9233 test loss: 1.53799\n",
      "520: accuracy:0.89 loss: 1.57379 (lr:8.0)\n",
      "540: accuracy:0.9 loss: 1.55777 (lr:8.0)\n",
      "560: accuracy:0.89 loss: 1.57163 (lr:8.0)\n",
      "580: accuracy:0.89 loss: 1.57229 (lr:8.0)\n",
      "600: accuracy:0.88 loss: 1.57424 (lr:8.0)\n",
      "600: ********* epoch 2 ********* test accuracy:0.9235 test loss: 1.53676\n",
      "620: accuracy:0.93 loss: 1.52952 (lr:8.0)\n",
      "640: accuracy:0.93 loss: 1.52966 (lr:8.0)\n",
      "660: accuracy:0.91 loss: 1.55769 (lr:8.0)\n",
      "680: accuracy:0.94 loss: 1.52 (lr:8.0)\n",
      "700: accuracy:0.98 loss: 1.48805 (lr:8.0)\n",
      "700: ********* epoch 2 ********* test accuracy:0.9296 test loss: 1.53163\n",
      "720: accuracy:0.98 loss: 1.48214 (lr:8.0)\n",
      "740: accuracy:0.93 loss: 1.53641 (lr:8.0)\n",
      "760: accuracy:0.89 loss: 1.56811 (lr:8.0)\n",
      "780: accuracy:0.89 loss: 1.56787 (lr:8.0)\n",
      "800: accuracy:0.93 loss: 1.52927 (lr:8.0)\n",
      "800: ********* epoch 2 ********* test accuracy:0.9316 test loss: 1.52976\n",
      "820: accuracy:0.89 loss: 1.56727 (lr:8.0)\n",
      "840: accuracy:0.92 loss: 1.53742 (lr:8.0)\n",
      "860: accuracy:0.93 loss: 1.53505 (lr:8.0)\n",
      "880: accuracy:0.92 loss: 1.5393 (lr:8.0)\n",
      "900: accuracy:0.95 loss: 1.50925 (lr:8.0)\n",
      "900: ********* epoch 2 ********* test accuracy:0.9405 test loss: 1.52055\n",
      "920: accuracy:0.97 loss: 1.49405 (lr:8.0)\n",
      "940: accuracy:0.96 loss: 1.49948 (lr:8.0)\n",
      "960: accuracy:0.91 loss: 1.54854 (lr:8.0)\n",
      "980: accuracy:0.95 loss: 1.51036 (lr:8.0)\n",
      "1000: accuracy:0.95 loss: 1.50778 (lr:8.0)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9399 test loss: 1.52096\n",
      "1020: accuracy:0.92 loss: 1.54326 (lr:8.0)\n",
      "1040: accuracy:0.93 loss: 1.52977 (lr:8.0)\n",
      "1060: accuracy:0.93 loss: 1.52587 (lr:8.0)\n",
      "1080: accuracy:0.93 loss: 1.54791 (lr:8.0)\n",
      "1100: accuracy:0.91 loss: 1.55062 (lr:8.0)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9327 test loss: 1.52737\n",
      "1120: accuracy:0.89 loss: 1.57604 (lr:8.0)\n",
      "1140: accuracy:0.94 loss: 1.52501 (lr:8.0)\n",
      "1160: accuracy:0.96 loss: 1.50351 (lr:8.0)\n",
      "1180: accuracy:0.97 loss: 1.49695 (lr:8.0)\n",
      "1200: accuracy:0.91 loss: 1.5429 (lr:8.0)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9396 test loss: 1.52127\n",
      "1220: accuracy:0.9 loss: 1.56581 (lr:8.0)\n",
      "1240: accuracy:0.97 loss: 1.49613 (lr:8.0)\n",
      "1260: accuracy:0.97 loss: 1.49219 (lr:8.0)\n",
      "1280: accuracy:0.93 loss: 1.52924 (lr:8.0)\n",
      "1300: accuracy:0.92 loss: 1.5417 (lr:8.0)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9407 test loss: 1.5195\n",
      "1320: accuracy:0.93 loss: 1.53388 (lr:8.0)\n",
      "1340: accuracy:0.97 loss: 1.48935 (lr:8.0)\n",
      "1360: accuracy:0.92 loss: 1.5404 (lr:8.0)\n",
      "1380: accuracy:0.9 loss: 1.5569 (lr:8.0)\n",
      "1400: accuracy:0.92 loss: 1.54075 (lr:8.0)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9342 test loss: 1.52677\n",
      "1420: accuracy:0.96 loss: 1.50083 (lr:8.0)\n",
      "1440: accuracy:0.94 loss: 1.51669 (lr:8.0)\n",
      "1460: accuracy:0.95 loss: 1.50855 (lr:8.0)\n",
      "1480: accuracy:0.91 loss: 1.5459 (lr:8.0)\n",
      "1500: accuracy:0.94 loss: 1.52065 (lr:8.0)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9406 test loss: 1.52031\n",
      "1520: accuracy:0.89 loss: 1.57411 (lr:8.0)\n",
      "1540: accuracy:0.94 loss: 1.51509 (lr:8.0)\n",
      "1560: accuracy:0.94 loss: 1.52071 (lr:8.0)\n",
      "1580: accuracy:0.99 loss: 1.47188 (lr:8.0)\n",
      "1600: accuracy:0.92 loss: 1.5411 (lr:8.0)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9398 test loss: 1.52127\n",
      "1620: accuracy:0.83 loss: 1.62343 (lr:8.0)\n",
      "1640: accuracy:0.94 loss: 1.51907 (lr:8.0)\n",
      "1660: accuracy:0.92 loss: 1.53798 (lr:8.0)\n",
      "1680: accuracy:0.92 loss: 1.5369 (lr:8.0)\n",
      "1700: accuracy:0.91 loss: 1.55057 (lr:8.0)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9304 test loss: 1.53117\n",
      "1720: accuracy:0.93 loss: 1.53946 (lr:8.0)\n",
      "1740: accuracy:0.96 loss: 1.50432 (lr:8.0)\n",
      "1760: accuracy:0.98 loss: 1.48003 (lr:8.0)\n",
      "1780: accuracy:0.91 loss: 1.54968 (lr:8.0)\n",
      "1800: accuracy:0.94 loss: 1.52265 (lr:8.0)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9361 test loss: 1.52466\n",
      "1820: accuracy:0.95 loss: 1.51005 (lr:8.0)\n",
      "1840: accuracy:0.94 loss: 1.5247 (lr:8.0)\n",
      "1860: accuracy:0.93 loss: 1.52804 (lr:8.0)\n",
      "1880: accuracy:0.93 loss: 1.53021 (lr:8.0)\n",
      "1900: accuracy:0.89 loss: 1.56915 (lr:8.0)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9452 test loss: 1.51617\n",
      "1920: accuracy:0.95 loss: 1.51113 (lr:8.0)\n",
      "1940: accuracy:0.97 loss: 1.49053 (lr:8.0)\n",
      "1960: accuracy:0.95 loss: 1.5131 (lr:8.0)\n",
      "1980: accuracy:0.95 loss: 1.5084 (lr:8.0)\n",
      "2000: accuracy:0.93 loss: 1.53114 (lr:8.0)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9339 test loss: 1.5275\n",
      "2020: accuracy:0.96 loss: 1.50066 (lr:8.0)\n",
      "2040: accuracy:0.92 loss: 1.54064 (lr:8.0)\n",
      "2060: accuracy:0.94 loss: 1.52467 (lr:8.0)\n",
      "2080: accuracy:0.94 loss: 1.5245 (lr:8.0)\n",
      "2100: accuracy:0.93 loss: 1.53146 (lr:8.0)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9378 test loss: 1.52312\n",
      "2120: accuracy:0.98 loss: 1.48269 (lr:8.0)\n",
      "2140: accuracy:0.93 loss: 1.52961 (lr:8.0)\n",
      "2160: accuracy:0.94 loss: 1.52221 (lr:8.0)\n",
      "2180: accuracy:0.97 loss: 1.4905 (lr:8.0)\n",
      "2200: accuracy:0.95 loss: 1.5082 (lr:8.0)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9441 test loss: 1.51675\n",
      "2220: accuracy:0.9 loss: 1.56017 (lr:8.0)\n",
      "2240: accuracy:0.93 loss: 1.5311 (lr:8.0)\n",
      "2260: accuracy:0.93 loss: 1.52659 (lr:8.0)\n",
      "2280: accuracy:0.99 loss: 1.47126 (lr:8.0)\n",
      "2300: accuracy:0.96 loss: 1.50504 (lr:8.0)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9418 test loss: 1.51871\n",
      "2320: accuracy:0.94 loss: 1.52365 (lr:8.0)\n",
      "2340: accuracy:0.95 loss: 1.51046 (lr:8.0)\n",
      "2360: accuracy:0.97 loss: 1.49099 (lr:8.0)\n",
      "2380: accuracy:0.9 loss: 1.56545 (lr:8.0)\n",
      "2400: accuracy:0.97 loss: 1.49117 (lr:8.0)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9516 test loss: 1.50937\n",
      "2420: accuracy:0.93 loss: 1.53333 (lr:8.0)\n",
      "2440: accuracy:0.92 loss: 1.54188 (lr:8.0)\n",
      "2460: accuracy:0.95 loss: 1.51565 (lr:8.0)\n",
      "2480: accuracy:0.92 loss: 1.54462 (lr:8.0)\n",
      "2500: accuracy:0.97 loss: 1.49224 (lr:8.0)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9448 test loss: 1.51584\n",
      "2520: accuracy:0.93 loss: 1.53024 (lr:8.0)\n",
      "2540: accuracy:0.97 loss: 1.49128 (lr:8.0)\n",
      "2560: accuracy:0.92 loss: 1.54107 (lr:8.0)\n",
      "2580: accuracy:0.9 loss: 1.56159 (lr:8.0)\n",
      "2600: accuracy:0.94 loss: 1.52064 (lr:8.0)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9346 test loss: 1.52645\n",
      "2620: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "2640: accuracy:0.97 loss: 1.49117 (lr:8.0)\n",
      "2660: accuracy:0.92 loss: 1.54051 (lr:8.0)\n",
      "2680: accuracy:0.96 loss: 1.50111 (lr:8.0)\n",
      "2700: accuracy:0.95 loss: 1.51767 (lr:8.0)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9413 test loss: 1.5196\n",
      "2720: accuracy:0.93 loss: 1.53101 (lr:8.0)\n",
      "2740: accuracy:0.94 loss: 1.52091 (lr:8.0)\n",
      "2760: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "2780: accuracy:0.96 loss: 1.5006 (lr:8.0)\n",
      "2800: accuracy:0.95 loss: 1.5081 (lr:8.0)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9424 test loss: 1.51852\n",
      "2820: accuracy:0.95 loss: 1.51118 (lr:8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2840: accuracy:0.96 loss: 1.50066 (lr:8.0)\n",
      "2860: accuracy:0.88 loss: 1.58186 (lr:8.0)\n",
      "2880: accuracy:0.94 loss: 1.51996 (lr:8.0)\n",
      "2900: accuracy:0.93 loss: 1.53599 (lr:8.0)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9535 test loss: 1.50756\n",
      "2920: accuracy:0.98 loss: 1.48153 (lr:8.0)\n",
      "2940: accuracy:0.92 loss: 1.54108 (lr:8.0)\n",
      "2960: accuracy:0.95 loss: 1.5111 (lr:8.0)\n",
      "2980: accuracy:0.94 loss: 1.52154 (lr:8.0)\n",
      "3000: accuracy:0.96 loss: 1.49918 (lr:8.0)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.953 test loss: 1.50796\n",
      "3020: accuracy:0.92 loss: 1.54122 (lr:8.0)\n",
      "3040: accuracy:0.96 loss: 1.49727 (lr:8.0)\n",
      "3060: accuracy:0.93 loss: 1.53143 (lr:8.0)\n",
      "3080: accuracy:0.93 loss: 1.52743 (lr:8.0)\n",
      "3100: accuracy:0.97 loss: 1.49135 (lr:8.0)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9537 test loss: 1.5068\n",
      "3120: accuracy:0.96 loss: 1.50117 (lr:8.0)\n",
      "3140: accuracy:0.94 loss: 1.51825 (lr:8.0)\n",
      "3160: accuracy:0.96 loss: 1.49952 (lr:8.0)\n",
      "3180: accuracy:0.95 loss: 1.51121 (lr:8.0)\n",
      "3200: accuracy:0.93 loss: 1.53118 (lr:8.0)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9391 test loss: 1.52181\n",
      "3220: accuracy:0.97 loss: 1.4978 (lr:8.0)\n",
      "3240: accuracy:0.88 loss: 1.58276 (lr:8.0)\n",
      "3260: accuracy:0.94 loss: 1.5202 (lr:8.0)\n",
      "3280: accuracy:0.95 loss: 1.51091 (lr:8.0)\n",
      "3300: accuracy:0.93 loss: 1.53115 (lr:8.0)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9474 test loss: 1.51369\n",
      "3320: accuracy:0.93 loss: 1.53474 (lr:8.0)\n",
      "3340: accuracy:0.94 loss: 1.52109 (lr:8.0)\n",
      "3360: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "3380: accuracy:0.96 loss: 1.50439 (lr:8.0)\n",
      "3400: accuracy:0.96 loss: 1.50078 (lr:8.0)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9531 test loss: 1.50799\n",
      "3420: accuracy:0.95 loss: 1.5097 (lr:8.0)\n",
      "3440: accuracy:0.95 loss: 1.51195 (lr:8.0)\n",
      "3460: accuracy:0.98 loss: 1.48147 (lr:8.0)\n",
      "3480: accuracy:0.95 loss: 1.51114 (lr:8.0)\n",
      "3500: accuracy:0.96 loss: 1.50197 (lr:8.0)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9536 test loss: 1.50724\n",
      "3520: accuracy:0.96 loss: 1.50456 (lr:8.0)\n",
      "3540: accuracy:0.95 loss: 1.51136 (lr:8.0)\n",
      "3560: accuracy:0.94 loss: 1.52516 (lr:8.0)\n",
      "3580: accuracy:0.93 loss: 1.53063 (lr:8.0)\n",
      "3600: accuracy:0.96 loss: 1.50106 (lr:8.0)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9487 test loss: 1.51209\n",
      "3620: accuracy:0.91 loss: 1.54495 (lr:8.0)\n",
      "3640: accuracy:0.96 loss: 1.50017 (lr:8.0)\n",
      "3660: accuracy:0.96 loss: 1.50019 (lr:8.0)\n",
      "3680: accuracy:0.94 loss: 1.52115 (lr:8.0)\n",
      "3700: accuracy:0.95 loss: 1.51116 (lr:8.0)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9453 test loss: 1.51616\n",
      "3720: accuracy:0.95 loss: 1.51327 (lr:8.0)\n",
      "3740: accuracy:0.92 loss: 1.54092 (lr:8.0)\n",
      "3760: accuracy:0.96 loss: 1.49439 (lr:8.0)\n",
      "3780: accuracy:0.94 loss: 1.52027 (lr:8.0)\n",
      "3800: accuracy:0.93 loss: 1.52693 (lr:8.0)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.952 test loss: 1.50876\n",
      "3820: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "3840: accuracy:0.92 loss: 1.54317 (lr:8.0)\n",
      "3860: accuracy:0.93 loss: 1.52702 (lr:8.0)\n",
      "3880: accuracy:0.99 loss: 1.47118 (lr:8.0)\n",
      "3900: accuracy:0.99 loss: 1.46992 (lr:8.0)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.945 test loss: 1.51588\n",
      "3920: accuracy:0.95 loss: 1.51238 (lr:8.0)\n",
      "3940: accuracy:0.99 loss: 1.47395 (lr:8.0)\n",
      "3960: accuracy:0.95 loss: 1.50716 (lr:8.0)\n",
      "3980: accuracy:0.97 loss: 1.49081 (lr:8.0)\n",
      "4000: accuracy:0.97 loss: 1.49207 (lr:8.0)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9421 test loss: 1.51897\n",
      "4020: accuracy:0.96 loss: 1.49663 (lr:8.0)\n",
      "4040: accuracy:0.92 loss: 1.54116 (lr:8.0)\n",
      "4060: accuracy:0.98 loss: 1.4813 (lr:8.0)\n",
      "4080: accuracy:0.95 loss: 1.50635 (lr:8.0)\n",
      "4100: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9528 test loss: 1.50798\n",
      "4120: accuracy:0.94 loss: 1.52455 (lr:8.0)\n",
      "4140: accuracy:0.93 loss: 1.53078 (lr:8.0)\n",
      "4160: accuracy:0.96 loss: 1.50119 (lr:8.0)\n",
      "4180: accuracy:0.98 loss: 1.4823 (lr:8.0)\n",
      "4200: accuracy:0.97 loss: 1.49116 (lr:8.0)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9509 test loss: 1.50997\n",
      "4220: accuracy:0.95 loss: 1.50755 (lr:8.0)\n",
      "4240: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "4260: accuracy:0.94 loss: 1.5155 (lr:8.0)\n",
      "4280: accuracy:0.95 loss: 1.51084 (lr:8.0)\n",
      "4300: accuracy:0.98 loss: 1.48503 (lr:8.0)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.945 test loss: 1.51601\n",
      "4320: accuracy:0.93 loss: 1.52967 (lr:8.0)\n",
      "4340: accuracy:0.97 loss: 1.49143 (lr:8.0)\n",
      "4360: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "4380: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "4400: accuracy:0.95 loss: 1.51747 (lr:8.0)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9403 test loss: 1.52082\n",
      "4420: accuracy:0.93 loss: 1.53031 (lr:8.0)\n",
      "4440: accuracy:0.89 loss: 1.5672 (lr:8.0)\n",
      "4460: accuracy:0.98 loss: 1.48257 (lr:8.0)\n",
      "4480: accuracy:0.94 loss: 1.52283 (lr:8.0)\n",
      "4500: accuracy:0.93 loss: 1.53115 (lr:8.0)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9518 test loss: 1.50938\n",
      "4520: accuracy:0.94 loss: 1.5211 (lr:8.0)\n",
      "4540: accuracy:0.96 loss: 1.49912 (lr:8.0)\n",
      "4560: accuracy:0.95 loss: 1.51102 (lr:8.0)\n",
      "4580: accuracy:0.98 loss: 1.48182 (lr:8.0)\n",
      "4600: accuracy:0.99 loss: 1.471 (lr:8.0)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.958 test loss: 1.50325\n",
      "4620: accuracy:0.93 loss: 1.53099 (lr:8.0)\n",
      "4640: accuracy:0.95 loss: 1.51158 (lr:8.0)\n",
      "4660: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "4680: accuracy:0.91 loss: 1.54757 (lr:8.0)\n",
      "4700: accuracy:0.94 loss: 1.52206 (lr:8.0)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9497 test loss: 1.51134\n",
      "4720: accuracy:0.98 loss: 1.48043 (lr:8.0)\n",
      "4740: accuracy:0.97 loss: 1.49114 (lr:8.0)\n",
      "4760: accuracy:0.91 loss: 1.55115 (lr:8.0)\n",
      "4780: accuracy:0.94 loss: 1.52093 (lr:8.0)\n",
      "4800: accuracy:0.9 loss: 1.55485 (lr:8.0)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9457 test loss: 1.51528\n",
      "4820: accuracy:0.95 loss: 1.51096 (lr:8.0)\n",
      "4840: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "4860: accuracy:0.95 loss: 1.50718 (lr:8.0)\n",
      "4880: accuracy:0.98 loss: 1.48137 (lr:8.0)\n",
      "4900: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9495 test loss: 1.51162\n",
      "4920: accuracy:0.93 loss: 1.53475 (lr:8.0)\n",
      "4940: accuracy:0.99 loss: 1.47115 (lr:8.0)\n",
      "4960: accuracy:0.94 loss: 1.52325 (lr:8.0)\n",
      "4980: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "5000: accuracy:0.97 loss: 1.49044 (lr:8.0)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9486 test loss: 1.51255\n",
      "5020: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "5040: accuracy:0.91 loss: 1.55326 (lr:8.0)\n",
      "5060: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "5080: accuracy:0.93 loss: 1.52742 (lr:8.0)\n",
      "5100: accuracy:0.9 loss: 1.55553 (lr:8.0)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9458 test loss: 1.5151\n",
      "5120: accuracy:0.94 loss: 1.5206 (lr:8.0)\n",
      "5140: accuracy:0.97 loss: 1.49244 (lr:8.0)\n",
      "5160: accuracy:1.0 loss: 1.46115 (lr:8.0)\n",
      "5180: accuracy:0.95 loss: 1.51381 (lr:8.0)\n",
      "5200: accuracy:0.93 loss: 1.53114 (lr:8.0)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9291 test loss: 1.53202\n",
      "5220: accuracy:0.94 loss: 1.52184 (lr:8.0)\n",
      "5240: accuracy:0.93 loss: 1.53128 (lr:8.0)\n",
      "5260: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "5280: accuracy:0.96 loss: 1.50087 (lr:8.0)\n",
      "5300: accuracy:0.99 loss: 1.47066 (lr:8.0)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9545 test loss: 1.50635\n",
      "5320: accuracy:0.94 loss: 1.52137 (lr:8.0)\n",
      "5340: accuracy:0.99 loss: 1.47115 (lr:8.0)\n",
      "5360: accuracy:0.96 loss: 1.50198 (lr:8.0)\n",
      "5380: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "5400: accuracy:0.94 loss: 1.52185 (lr:8.0)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9535 test loss: 1.50754\n",
      "5420: accuracy:0.94 loss: 1.52125 (lr:8.0)\n",
      "5440: accuracy:0.99 loss: 1.47117 (lr:8.0)\n",
      "5460: accuracy:0.94 loss: 1.51826 (lr:8.0)\n",
      "5480: accuracy:0.96 loss: 1.50103 (lr:8.0)\n",
      "5500: accuracy:0.96 loss: 1.49963 (lr:8.0)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9494 test loss: 1.51151\n",
      "5520: accuracy:0.97 loss: 1.49312 (lr:8.0)\n",
      "5540: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "5560: accuracy:0.94 loss: 1.52079 (lr:8.0)\n",
      "5580: accuracy:0.94 loss: 1.51979 (lr:8.0)\n",
      "5600: accuracy:0.96 loss: 1.49759 (lr:8.0)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9424 test loss: 1.51839\n",
      "5620: accuracy:0.93 loss: 1.52617 (lr:8.0)\n",
      "5640: accuracy:0.95 loss: 1.51222 (lr:8.0)\n",
      "5660: accuracy:0.96 loss: 1.5012 (lr:8.0)\n",
      "5680: accuracy:0.93 loss: 1.53112 (lr:8.0)\n",
      "5700: accuracy:0.9 loss: 1.56094 (lr:8.0)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9396 test loss: 1.52136\n",
      "5720: accuracy:0.94 loss: 1.52113 (lr:8.0)\n",
      "5740: accuracy:0.93 loss: 1.53115 (lr:8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5760: accuracy:0.94 loss: 1.5238 (lr:8.0)\n",
      "5780: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "5800: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.938 test loss: 1.52303\n",
      "5820: accuracy:0.95 loss: 1.5112 (lr:8.0)\n",
      "5840: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "5860: accuracy:0.93 loss: 1.53115 (lr:8.0)\n",
      "5880: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "5900: accuracy:0.96 loss: 1.50187 (lr:8.0)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9541 test loss: 1.50686\n",
      "5920: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "5940: accuracy:0.97 loss: 1.48882 (lr:8.0)\n",
      "5960: accuracy:0.95 loss: 1.51109 (lr:8.0)\n",
      "5980: accuracy:0.97 loss: 1.49157 (lr:8.0)\n",
      "6000: accuracy:0.94 loss: 1.52115 (lr:8.0)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9488 test loss: 1.51243\n",
      "6020: accuracy:0.94 loss: 1.52116 (lr:8.0)\n",
      "6040: accuracy:0.93 loss: 1.53118 (lr:8.0)\n",
      "6060: accuracy:0.96 loss: 1.50113 (lr:8.0)\n",
      "6080: accuracy:0.99 loss: 1.47122 (lr:8.0)\n",
      "6100: accuracy:0.97 loss: 1.49681 (lr:8.0)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9395 test loss: 1.52174\n",
      "6120: accuracy:0.94 loss: 1.52107 (lr:8.0)\n",
      "6140: accuracy:0.95 loss: 1.51485 (lr:8.0)\n",
      "6160: accuracy:0.96 loss: 1.5011 (lr:8.0)\n",
      "6180: accuracy:0.97 loss: 1.49086 (lr:8.0)\n",
      "6200: accuracy:0.94 loss: 1.52103 (lr:8.0)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9512 test loss: 1.51005\n",
      "6220: accuracy:0.98 loss: 1.48181 (lr:8.0)\n",
      "6240: accuracy:0.95 loss: 1.50983 (lr:8.0)\n",
      "6260: accuracy:0.96 loss: 1.50125 (lr:8.0)\n",
      "6280: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "6300: accuracy:0.97 loss: 1.49117 (lr:8.0)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.936 test loss: 1.52545\n",
      "6320: accuracy:0.99 loss: 1.47667 (lr:8.0)\n",
      "6340: accuracy:0.96 loss: 1.50142 (lr:8.0)\n",
      "6360: accuracy:0.95 loss: 1.51035 (lr:8.0)\n",
      "6380: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "6400: accuracy:0.98 loss: 1.48213 (lr:8.0)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.956 test loss: 1.50544\n",
      "6420: accuracy:0.99 loss: 1.47551 (lr:8.0)\n",
      "6440: accuracy:0.92 loss: 1.54101 (lr:8.0)\n",
      "6460: accuracy:0.97 loss: 1.49146 (lr:8.0)\n",
      "6480: accuracy:0.94 loss: 1.52115 (lr:8.0)\n",
      "6500: accuracy:0.92 loss: 1.54493 (lr:8.0)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9418 test loss: 1.51909\n",
      "6520: accuracy:0.91 loss: 1.5517 (lr:8.0)\n",
      "6540: accuracy:0.95 loss: 1.50877 (lr:8.0)\n",
      "6560: accuracy:0.97 loss: 1.49078 (lr:8.0)\n",
      "6580: accuracy:0.93 loss: 1.53115 (lr:8.0)\n",
      "6600: accuracy:0.99 loss: 1.47115 (lr:8.0)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.956 test loss: 1.50496\n",
      "6620: accuracy:0.97 loss: 1.49116 (lr:8.0)\n",
      "6640: accuracy:0.94 loss: 1.52111 (lr:8.0)\n",
      "6660: accuracy:0.92 loss: 1.54134 (lr:8.0)\n",
      "6680: accuracy:0.93 loss: 1.52982 (lr:8.0)\n",
      "6700: accuracy:0.98 loss: 1.48126 (lr:8.0)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9531 test loss: 1.50788\n",
      "6720: accuracy:0.94 loss: 1.52103 (lr:8.0)\n",
      "6740: accuracy:0.96 loss: 1.50183 (lr:8.0)\n",
      "6760: accuracy:0.95 loss: 1.51191 (lr:8.0)\n",
      "6780: accuracy:0.94 loss: 1.51825 (lr:8.0)\n",
      "6800: accuracy:0.95 loss: 1.51144 (lr:8.0)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9566 test loss: 1.50441\n",
      "6820: accuracy:0.95 loss: 1.51084 (lr:8.0)\n",
      "6840: accuracy:0.96 loss: 1.50251 (lr:8.0)\n",
      "6860: accuracy:0.98 loss: 1.4812 (lr:8.0)\n",
      "6880: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "6900: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9504 test loss: 1.51073\n",
      "6920: accuracy:0.98 loss: 1.48105 (lr:8.0)\n",
      "6940: accuracy:0.99 loss: 1.47115 (lr:8.0)\n",
      "6960: accuracy:0.99 loss: 1.47092 (lr:8.0)\n",
      "6980: accuracy:0.97 loss: 1.49221 (lr:8.0)\n",
      "7000: accuracy:0.99 loss: 1.47139 (lr:8.0)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9535 test loss: 1.50747\n",
      "7020: accuracy:0.96 loss: 1.50105 (lr:8.0)\n",
      "7040: accuracy:0.97 loss: 1.49118 (lr:8.0)\n",
      "7060: accuracy:0.94 loss: 1.52378 (lr:8.0)\n",
      "7080: accuracy:0.97 loss: 1.48936 (lr:8.0)\n",
      "7100: accuracy:0.92 loss: 1.54088 (lr:8.0)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9496 test loss: 1.51148\n",
      "7120: accuracy:0.94 loss: 1.52115 (lr:8.0)\n",
      "7140: accuracy:0.94 loss: 1.52289 (lr:8.0)\n",
      "7160: accuracy:0.95 loss: 1.51076 (lr:8.0)\n",
      "7180: accuracy:0.97 loss: 1.49448 (lr:8.0)\n",
      "7200: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9523 test loss: 1.50873\n",
      "7220: accuracy:0.95 loss: 1.51088 (lr:8.0)\n",
      "7240: accuracy:0.96 loss: 1.50073 (lr:8.0)\n",
      "7260: accuracy:0.94 loss: 1.51999 (lr:8.0)\n",
      "7280: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "7300: accuracy:0.95 loss: 1.50745 (lr:8.0)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9558 test loss: 1.50526\n",
      "7320: accuracy:0.99 loss: 1.47115 (lr:8.0)\n",
      "7340: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "7360: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "7380: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "7400: accuracy:0.94 loss: 1.51597 (lr:8.0)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9539 test loss: 1.50729\n",
      "7420: accuracy:0.97 loss: 1.49102 (lr:8.0)\n",
      "7440: accuracy:0.94 loss: 1.52115 (lr:8.0)\n",
      "7460: accuracy:0.93 loss: 1.53115 (lr:8.0)\n",
      "7480: accuracy:0.97 loss: 1.49254 (lr:8.0)\n",
      "7500: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9586 test loss: 1.50243\n",
      "7520: accuracy:0.97 loss: 1.48733 (lr:8.0)\n",
      "7540: accuracy:0.94 loss: 1.52146 (lr:8.0)\n",
      "7560: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "7580: accuracy:0.95 loss: 1.5111 (lr:8.0)\n",
      "7600: accuracy:0.95 loss: 1.51123 (lr:8.0)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9571 test loss: 1.50391\n",
      "7620: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "7640: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "7660: accuracy:0.97 loss: 1.49123 (lr:8.0)\n",
      "7680: accuracy:0.98 loss: 1.48133 (lr:8.0)\n",
      "7700: accuracy:0.94 loss: 1.52115 (lr:8.0)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9601 test loss: 1.50116\n",
      "7720: accuracy:0.93 loss: 1.53365 (lr:8.0)\n",
      "7740: accuracy:0.94 loss: 1.52124 (lr:8.0)\n",
      "7760: accuracy:0.95 loss: 1.51026 (lr:8.0)\n",
      "7780: accuracy:0.95 loss: 1.51116 (lr:8.0)\n",
      "7800: accuracy:0.91 loss: 1.55082 (lr:8.0)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9515 test loss: 1.50962\n",
      "7820: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "7840: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "7860: accuracy:0.95 loss: 1.50969 (lr:8.0)\n",
      "7880: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "7900: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9496 test loss: 1.51145\n",
      "7920: accuracy:0.96 loss: 1.50082 (lr:8.0)\n",
      "7940: accuracy:0.95 loss: 1.51109 (lr:8.0)\n",
      "7960: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "7980: accuracy:0.95 loss: 1.51038 (lr:8.0)\n",
      "8000: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9495 test loss: 1.51134\n",
      "8020: accuracy:0.9 loss: 1.56119 (lr:8.0)\n",
      "8040: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "8060: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "8080: accuracy:0.97 loss: 1.49219 (lr:8.0)\n",
      "8100: accuracy:0.95 loss: 1.50817 (lr:8.0)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9545 test loss: 1.50647\n",
      "8120: accuracy:0.97 loss: 1.49323 (lr:8.0)\n",
      "8140: accuracy:0.95 loss: 1.51079 (lr:8.0)\n",
      "8160: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "8180: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "8200: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9564 test loss: 1.50455\n",
      "8220: accuracy:0.96 loss: 1.49918 (lr:8.0)\n",
      "8240: accuracy:1.0 loss: 1.46149 (lr:8.0)\n",
      "8260: accuracy:0.96 loss: 1.50512 (lr:8.0)\n",
      "8280: accuracy:0.94 loss: 1.52191 (lr:8.0)\n",
      "8300: accuracy:0.96 loss: 1.50329 (lr:8.0)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9495 test loss: 1.51165\n",
      "8320: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "8340: accuracy:0.97 loss: 1.49205 (lr:8.0)\n",
      "8360: accuracy:0.94 loss: 1.52112 (lr:8.0)\n",
      "8380: accuracy:0.96 loss: 1.50398 (lr:8.0)\n",
      "8400: accuracy:1.0 loss: 1.46115 (lr:8.0)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9549 test loss: 1.506\n",
      "8420: accuracy:0.88 loss: 1.57548 (lr:8.0)\n",
      "8440: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "8460: accuracy:0.97 loss: 1.49117 (lr:8.0)\n",
      "8480: accuracy:0.96 loss: 1.50216 (lr:8.0)\n",
      "8500: accuracy:0.97 loss: 1.48668 (lr:8.0)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9483 test loss: 1.51283\n",
      "8520: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "8540: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "8560: accuracy:0.93 loss: 1.53374 (lr:8.0)\n",
      "8580: accuracy:1.0 loss: 1.46241 (lr:8.0)\n",
      "8600: accuracy:0.95 loss: 1.51115 (lr:8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8600: ********* epoch 15 ********* test accuracy:0.955 test loss: 1.50625\n",
      "8620: accuracy:1.0 loss: 1.46115 (lr:8.0)\n",
      "8640: accuracy:0.97 loss: 1.49245 (lr:8.0)\n",
      "8660: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "8680: accuracy:0.93 loss: 1.53114 (lr:8.0)\n",
      "8700: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9536 test loss: 1.50751\n",
      "8720: accuracy:0.97 loss: 1.49122 (lr:8.0)\n",
      "8740: accuracy:0.95 loss: 1.51204 (lr:8.0)\n",
      "8760: accuracy:0.98 loss: 1.48127 (lr:8.0)\n",
      "8780: accuracy:0.94 loss: 1.52114 (lr:8.0)\n",
      "8800: accuracy:0.99 loss: 1.47116 (lr:8.0)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9574 test loss: 1.50325\n",
      "8820: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "8840: accuracy:0.93 loss: 1.53047 (lr:8.0)\n",
      "8860: accuracy:0.98 loss: 1.48418 (lr:8.0)\n",
      "8880: accuracy:0.94 loss: 1.52108 (lr:8.0)\n",
      "8900: accuracy:0.98 loss: 1.48178 (lr:8.0)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9579 test loss: 1.50315\n",
      "8920: accuracy:0.9 loss: 1.56108 (lr:8.0)\n",
      "8940: accuracy:1.0 loss: 1.46116 (lr:8.0)\n",
      "8960: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "8980: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "9000: accuracy:0.94 loss: 1.52115 (lr:8.0)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9521 test loss: 1.50883\n",
      "9020: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "9040: accuracy:0.94 loss: 1.51901 (lr:8.0)\n",
      "9060: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "9080: accuracy:0.97 loss: 1.48676 (lr:8.0)\n",
      "9100: accuracy:0.97 loss: 1.49263 (lr:8.0)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9541 test loss: 1.50702\n",
      "9120: accuracy:0.95 loss: 1.51189 (lr:8.0)\n",
      "9140: accuracy:0.93 loss: 1.53098 (lr:8.0)\n",
      "9160: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "9180: accuracy:0.98 loss: 1.48058 (lr:8.0)\n",
      "9200: accuracy:0.94 loss: 1.52551 (lr:8.0)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9553 test loss: 1.5057\n",
      "9220: accuracy:0.97 loss: 1.49114 (lr:8.0)\n",
      "9240: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "9260: accuracy:0.96 loss: 1.50114 (lr:8.0)\n",
      "9280: accuracy:0.98 loss: 1.48109 (lr:8.0)\n",
      "9300: accuracy:0.98 loss: 1.4809 (lr:8.0)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9502 test loss: 1.51067\n",
      "9320: accuracy:0.93 loss: 1.53115 (lr:8.0)\n",
      "9340: accuracy:0.95 loss: 1.51331 (lr:8.0)\n",
      "9360: accuracy:0.99 loss: 1.47115 (lr:8.0)\n",
      "9380: accuracy:0.97 loss: 1.49657 (lr:8.0)\n",
      "9400: accuracy:0.96 loss: 1.50181 (lr:8.0)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9562 test loss: 1.50501\n",
      "9420: accuracy:0.98 loss: 1.48147 (lr:8.0)\n",
      "9440: accuracy:0.96 loss: 1.50076 (lr:8.0)\n",
      "9460: accuracy:0.93 loss: 1.52587 (lr:8.0)\n",
      "9480: accuracy:0.96 loss: 1.50116 (lr:8.0)\n",
      "9500: accuracy:0.98 loss: 1.47994 (lr:8.0)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9587 test loss: 1.50201\n",
      "9520: accuracy:0.96 loss: 1.50128 (lr:8.0)\n",
      "9540: accuracy:0.94 loss: 1.52118 (lr:8.0)\n",
      "9560: accuracy:0.93 loss: 1.53115 (lr:8.0)\n",
      "9580: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "9600: accuracy:0.96 loss: 1.50355 (lr:8.0)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9583 test loss: 1.50259\n",
      "9620: accuracy:0.97 loss: 1.49113 (lr:8.0)\n",
      "9640: accuracy:0.96 loss: 1.50116 (lr:8.0)\n",
      "9660: accuracy:0.97 loss: 1.48972 (lr:8.0)\n",
      "9680: accuracy:0.95 loss: 1.51115 (lr:8.0)\n",
      "9700: accuracy:0.99 loss: 1.47149 (lr:8.0)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9602 test loss: 1.50099\n",
      "9720: accuracy:0.96 loss: 1.50115 (lr:8.0)\n",
      "9740: accuracy:0.94 loss: 1.52118 (lr:8.0)\n",
      "9760: accuracy:0.93 loss: 1.531 (lr:8.0)\n",
      "9780: accuracy:0.92 loss: 1.54111 (lr:8.0)\n",
      "9800: accuracy:0.94 loss: 1.52115 (lr:8.0)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.949 test loss: 1.51193\n",
      "9820: accuracy:0.97 loss: 1.49115 (lr:8.0)\n",
      "9840: accuracy:0.94 loss: 1.5213 (lr:8.0)\n",
      "9860: accuracy:0.98 loss: 1.48115 (lr:8.0)\n",
      "9880: accuracy:0.97 loss: 1.4865 (lr:8.0)\n",
      "9900: accuracy:0.94 loss: 1.5215 (lr:8.0)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9577 test loss: 1.50337\n",
      "9920: accuracy:0.92 loss: 1.54116 (lr:8.0)\n",
      "9940: accuracy:0.94 loss: 1.52148 (lr:8.0)\n",
      "9960: accuracy:0.96 loss: 1.50126 (lr:8.0)\n",
      "9980: accuracy:0.94 loss: 1.52224 (lr:8.0)\n",
      "10000: accuracy:0.94 loss: 1.52116 (lr:8.0)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.956 test loss: 1.50488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:09<00:00, 31.87s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0P8D+ZjBlfU10rIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fecd5a6e860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning rates: [ 0.1  8. ]\n",
      "Accuracy: [0.85659999, 0.9562]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "h_dim = 100 # Fixed number of hidden dimensions\n",
    "n = 500 # Number of point in the grid.\n",
    "xn = np.arange(0,10,10/n)\n",
    "\n",
    "l_rates = np.array([0.1,8]) # Initial learning rates\n",
    "f = [nn_train(l, h_dim) for l in tqdm(l_rates)] #Accuracy\n",
    "\n",
    "noise = 1;length =1; m=0\n",
    "m, noise, length, sf = opt_hyparams(l_rates,f)\n",
    "E, cov = gp_posterior(l_rates, f, xn, m, noise, length, sf)\n",
    "data = data_posterior(xn, E, cov)\n",
    "\n",
    "#This plot shows the mean and variance with two initial observations:\n",
    "plt.plot(data['x'],data['Mean'], color = 'black', label = 'Mean')\n",
    "plt.plot(l_rates,f, 'ro', label = 'Obs')\n",
    "plt.fill_between(data['x'], data['Mean']-data['StdDev'], data['Mean']+data['StdDev'],color = 'lightgrey')\n",
    "plt.show()\n",
    "\n",
    "print ('Initial Learning rates:', l_rates)\n",
    "print ('Accuracy:', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we perform 5 interations. The figure below exhibits the learning rates selected by the algorithm with their corresponding accuracies. Additionally it shows the new mean and 1 standard deviation around the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.09 loss: 2.32158 (lr:7.14)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1439 test loss: 2.30521\n",
      "20: accuracy:0.49 loss: 1.96123 (lr:7.14)\n",
      "40: accuracy:0.41 loss: 2.05422 (lr:7.14)\n",
      "60: accuracy:0.45 loss: 2.01006 (lr:7.14)\n",
      "80: accuracy:0.71 loss: 1.7428 (lr:7.14)\n",
      "100: accuracy:0.64 loss: 1.81776 (lr:7.14)\n",
      "100: ********* epoch 1 ********* test accuracy:0.6998 test loss: 1.7603\n",
      "120: accuracy:0.78 loss: 1.67732 (lr:7.14)\n",
      "140: accuracy:0.79 loss: 1.6713 (lr:7.14)\n",
      "160: accuracy:0.78 loss: 1.66951 (lr:7.14)\n",
      "180: accuracy:0.78 loss: 1.67632 (lr:7.14)\n",
      "200: accuracy:0.86 loss: 1.61104 (lr:7.14)\n",
      "200: ********* epoch 1 ********* test accuracy:0.8282 test loss: 1.6319\n",
      "220: accuracy:0.86 loss: 1.60522 (lr:7.14)\n",
      "240: accuracy:0.68 loss: 1.77521 (lr:7.14)\n",
      "260: accuracy:0.8 loss: 1.65928 (lr:7.14)\n",
      "280: accuracy:0.86 loss: 1.60296 (lr:7.14)\n",
      "300: accuracy:0.82 loss: 1.63672 (lr:7.14)\n",
      "300: ********* epoch 1 ********* test accuracy:0.8204 test loss: 1.63954\n",
      "320: accuracy:0.77 loss: 1.69114 (lr:7.14)\n",
      "340: accuracy:0.89 loss: 1.57174 (lr:7.14)\n",
      "360: accuracy:0.85 loss: 1.60507 (lr:7.14)\n",
      "380: accuracy:0.85 loss: 1.60205 (lr:7.14)\n",
      "400: accuracy:0.84 loss: 1.61897 (lr:7.14)\n",
      "400: ********* epoch 1 ********* test accuracy:0.8373 test loss: 1.6237\n",
      "420: accuracy:0.82 loss: 1.64187 (lr:7.14)\n",
      "440: accuracy:0.83 loss: 1.62411 (lr:7.14)\n",
      "460: accuracy:0.91 loss: 1.54638 (lr:7.14)\n",
      "480: accuracy:0.86 loss: 1.60219 (lr:7.14)\n",
      "500: accuracy:0.84 loss: 1.61026 (lr:7.14)\n",
      "500: ********* epoch 1 ********* test accuracy:0.8494 test loss: 1.61129\n",
      "520: accuracy:0.79 loss: 1.67344 (lr:7.14)\n",
      "540: accuracy:0.85 loss: 1.60656 (lr:7.14)\n",
      "560: accuracy:0.9 loss: 1.56226 (lr:7.14)\n",
      "580: accuracy:0.82 loss: 1.64291 (lr:7.14)\n",
      "600: accuracy:0.92 loss: 1.54576 (lr:7.14)\n",
      "600: ********* epoch 2 ********* test accuracy:0.8425 test loss: 1.61706\n",
      "620: accuracy:0.81 loss: 1.65401 (lr:7.14)\n",
      "640: accuracy:0.82 loss: 1.64302 (lr:7.14)\n",
      "660: accuracy:0.82 loss: 1.63858 (lr:7.14)\n",
      "680: accuracy:0.8 loss: 1.66329 (lr:7.14)\n",
      "700: accuracy:0.83 loss: 1.62627 (lr:7.14)\n",
      "700: ********* epoch 2 ********* test accuracy:0.8471 test loss: 1.61516\n",
      "720: accuracy:0.88 loss: 1.58433 (lr:7.14)\n",
      "740: accuracy:0.79 loss: 1.67074 (lr:7.14)\n",
      "760: accuracy:0.84 loss: 1.6228 (lr:7.14)\n",
      "780: accuracy:0.86 loss: 1.5951 (lr:7.14)\n",
      "800: accuracy:0.85 loss: 1.6119 (lr:7.14)\n",
      "800: ********* epoch 2 ********* test accuracy:0.8492 test loss: 1.61144\n",
      "820: accuracy:0.88 loss: 1.58758 (lr:7.14)\n",
      "840: accuracy:0.82 loss: 1.63896 (lr:7.14)\n",
      "860: accuracy:0.85 loss: 1.60824 (lr:7.14)\n",
      "880: accuracy:0.86 loss: 1.59886 (lr:7.14)\n",
      "900: accuracy:0.8 loss: 1.65405 (lr:7.14)\n",
      "900: ********* epoch 2 ********* test accuracy:0.8451 test loss: 1.615\n",
      "920: accuracy:0.89 loss: 1.57029 (lr:7.14)\n",
      "940: accuracy:0.87 loss: 1.585 (lr:7.14)\n",
      "960: accuracy:0.9 loss: 1.56058 (lr:7.14)\n",
      "980: accuracy:0.89 loss: 1.57229 (lr:7.14)\n",
      "1000: accuracy:0.8 loss: 1.65461 (lr:7.14)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.8601 test loss: 1.60046\n",
      "1020: accuracy:0.79 loss: 1.66428 (lr:7.14)\n",
      "1040: accuracy:0.84 loss: 1.61887 (lr:7.14)\n",
      "1060: accuracy:0.82 loss: 1.63679 (lr:7.14)\n",
      "1080: accuracy:0.86 loss: 1.59848 (lr:7.14)\n",
      "1100: accuracy:0.89 loss: 1.56593 (lr:7.14)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.8616 test loss: 1.59882\n",
      "1120: accuracy:0.84 loss: 1.62248 (lr:7.14)\n",
      "1140: accuracy:0.86 loss: 1.60555 (lr:7.14)\n",
      "1160: accuracy:0.87 loss: 1.58779 (lr:7.14)\n",
      "1180: accuracy:0.84 loss: 1.62261 (lr:7.14)\n",
      "1200: accuracy:0.81 loss: 1.65106 (lr:7.14)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.8526 test loss: 1.60764\n",
      "1220: accuracy:0.83 loss: 1.62162 (lr:7.14)\n",
      "1240: accuracy:0.85 loss: 1.61773 (lr:7.14)\n",
      "1260: accuracy:0.91 loss: 1.5496 (lr:7.14)\n",
      "1280: accuracy:0.81 loss: 1.65161 (lr:7.14)\n",
      "1300: accuracy:0.85 loss: 1.60794 (lr:7.14)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.8272 test loss: 1.63276\n",
      "1320: accuracy:0.76 loss: 1.70219 (lr:7.14)\n",
      "1340: accuracy:0.89 loss: 1.57171 (lr:7.14)\n",
      "1360: accuracy:0.88 loss: 1.58061 (lr:7.14)\n",
      "1380: accuracy:0.8 loss: 1.66434 (lr:7.14)\n",
      "1400: accuracy:0.94 loss: 1.51859 (lr:7.14)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.8617 test loss: 1.5986\n",
      "1420: accuracy:0.88 loss: 1.58097 (lr:7.14)\n",
      "1440: accuracy:0.87 loss: 1.59147 (lr:7.14)\n",
      "1460: accuracy:0.89 loss: 1.57079 (lr:7.14)\n",
      "1480: accuracy:0.79 loss: 1.67029 (lr:7.14)\n",
      "1500: accuracy:0.84 loss: 1.61964 (lr:7.14)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.854 test loss: 1.60741\n",
      "1520: accuracy:0.9 loss: 1.561 (lr:7.14)\n",
      "1540: accuracy:0.89 loss: 1.57011 (lr:7.14)\n",
      "1560: accuracy:0.87 loss: 1.5929 (lr:7.14)\n",
      "1580: accuracy:0.85 loss: 1.61117 (lr:7.14)\n",
      "1600: accuracy:0.92 loss: 1.54469 (lr:7.14)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.8567 test loss: 1.60362\n",
      "1620: accuracy:0.83 loss: 1.63203 (lr:7.14)\n",
      "1640: accuracy:0.9 loss: 1.55985 (lr:7.14)\n",
      "1660: accuracy:0.88 loss: 1.58073 (lr:7.14)\n",
      "1680: accuracy:0.82 loss: 1.64056 (lr:7.14)\n",
      "1700: accuracy:0.86 loss: 1.59529 (lr:7.14)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.8616 test loss: 1.59911\n",
      "1720: accuracy:0.9 loss: 1.55869 (lr:7.14)\n",
      "1740: accuracy:0.86 loss: 1.60352 (lr:7.14)\n",
      "1760: accuracy:0.81 loss: 1.64509 (lr:7.14)\n",
      "1780: accuracy:0.84 loss: 1.6261 (lr:7.14)\n",
      "1800: accuracy:0.87 loss: 1.58761 (lr:7.14)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.8632 test loss: 1.59758\n",
      "1820: accuracy:0.87 loss: 1.59623 (lr:7.14)\n",
      "1840: accuracy:0.85 loss: 1.61048 (lr:7.14)\n",
      "1860: accuracy:0.79 loss: 1.66651 (lr:7.14)\n",
      "1880: accuracy:0.86 loss: 1.59877 (lr:7.14)\n",
      "1900: accuracy:0.87 loss: 1.58724 (lr:7.14)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.8644 test loss: 1.5961\n",
      "1920: accuracy:0.85 loss: 1.60259 (lr:7.14)\n",
      "1940: accuracy:0.85 loss: 1.60718 (lr:7.14)\n",
      "1960: accuracy:0.87 loss: 1.58578 (lr:7.14)\n",
      "1980: accuracy:0.86 loss: 1.60466 (lr:7.14)\n",
      "2000: accuracy:0.85 loss: 1.60529 (lr:7.14)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.8616 test loss: 1.59917\n",
      "2020: accuracy:0.89 loss: 1.57434 (lr:7.14)\n",
      "2040: accuracy:0.87 loss: 1.59311 (lr:7.14)\n",
      "2060: accuracy:0.89 loss: 1.57119 (lr:7.14)\n",
      "2080: accuracy:0.94 loss: 1.52248 (lr:7.14)\n",
      "2100: accuracy:0.92 loss: 1.54534 (lr:7.14)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.8597 test loss: 1.60077\n",
      "2120: accuracy:0.84 loss: 1.62079 (lr:7.14)\n",
      "2140: accuracy:0.88 loss: 1.57452 (lr:7.14)\n",
      "2160: accuracy:0.91 loss: 1.55305 (lr:7.14)\n",
      "2180: accuracy:0.83 loss: 1.62408 (lr:7.14)\n",
      "2200: accuracy:0.87 loss: 1.58773 (lr:7.14)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.8573 test loss: 1.60287\n",
      "2220: accuracy:0.87 loss: 1.58801 (lr:7.14)\n",
      "2240: accuracy:0.9 loss: 1.56104 (lr:7.14)\n",
      "2260: accuracy:0.88 loss: 1.5814 (lr:7.14)\n",
      "2280: accuracy:0.84 loss: 1.62247 (lr:7.14)\n",
      "2300: accuracy:0.87 loss: 1.59387 (lr:7.14)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.8598 test loss: 1.60066\n",
      "2320: accuracy:0.83 loss: 1.63323 (lr:7.14)\n",
      "2340: accuracy:0.86 loss: 1.60091 (lr:7.14)\n",
      "2360: accuracy:0.84 loss: 1.61977 (lr:7.14)\n",
      "2380: accuracy:0.87 loss: 1.58641 (lr:7.14)\n",
      "2400: accuracy:0.84 loss: 1.61648 (lr:7.14)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.8615 test loss: 1.59929\n",
      "2420: accuracy:0.84 loss: 1.6252 (lr:7.14)\n",
      "2440: accuracy:0.85 loss: 1.61093 (lr:7.14)\n",
      "2460: accuracy:0.86 loss: 1.60486 (lr:7.14)\n",
      "2480: accuracy:0.84 loss: 1.61569 (lr:7.14)\n",
      "2500: accuracy:0.86 loss: 1.60034 (lr:7.14)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.8631 test loss: 1.59803\n",
      "2520: accuracy:0.85 loss: 1.60883 (lr:7.14)\n",
      "2540: accuracy:0.8 loss: 1.66288 (lr:7.14)\n",
      "2560: accuracy:0.88 loss: 1.58461 (lr:7.14)\n",
      "2580: accuracy:0.86 loss: 1.60356 (lr:7.14)\n",
      "2600: accuracy:0.83 loss: 1.62984 (lr:7.14)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.8645 test loss: 1.59609\n",
      "2620: accuracy:0.88 loss: 1.58181 (lr:7.14)\n",
      "2640: accuracy:0.87 loss: 1.59303 (lr:7.14)\n",
      "2660: accuracy:0.85 loss: 1.61335 (lr:7.14)\n",
      "2680: accuracy:0.89 loss: 1.56783 (lr:7.14)\n",
      "2700: accuracy:0.92 loss: 1.54896 (lr:7.14)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.8654 test loss: 1.59512\n",
      "2720: accuracy:0.84 loss: 1.62088 (lr:7.14)\n",
      "2740: accuracy:0.79 loss: 1.67045 (lr:7.14)\n",
      "2760: accuracy:0.89 loss: 1.57327 (lr:7.14)\n",
      "2780: accuracy:0.89 loss: 1.57043 (lr:7.14)\n",
      "2800: accuracy:0.82 loss: 1.63667 (lr:7.14)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.8644 test loss: 1.59573\n",
      "2820: accuracy:0.81 loss: 1.65007 (lr:7.14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2840: accuracy:0.81 loss: 1.65053 (lr:7.14)\n",
      "2860: accuracy:0.87 loss: 1.58971 (lr:7.14)\n",
      "2880: accuracy:0.83 loss: 1.62545 (lr:7.14)\n",
      "2900: accuracy:0.91 loss: 1.55078 (lr:7.14)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.862 test loss: 1.59854\n",
      "2920: accuracy:0.86 loss: 1.60058 (lr:7.14)\n",
      "2940: accuracy:0.91 loss: 1.55208 (lr:7.14)\n",
      "2960: accuracy:0.81 loss: 1.65093 (lr:7.14)\n",
      "2980: accuracy:0.86 loss: 1.60095 (lr:7.14)\n",
      "3000: accuracy:0.82 loss: 1.64089 (lr:7.14)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.8716 test loss: 1.58926\n",
      "3020: accuracy:0.84 loss: 1.62276 (lr:7.14)\n",
      "3040: accuracy:0.87 loss: 1.59101 (lr:7.14)\n",
      "3060: accuracy:0.87 loss: 1.58795 (lr:7.14)\n",
      "3080: accuracy:0.87 loss: 1.59475 (lr:7.14)\n",
      "3100: accuracy:0.86 loss: 1.59799 (lr:7.14)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.8661 test loss: 1.59433\n",
      "3120: accuracy:0.88 loss: 1.57796 (lr:7.14)\n",
      "3140: accuracy:0.86 loss: 1.59655 (lr:7.14)\n",
      "3160: accuracy:0.93 loss: 1.53099 (lr:7.14)\n",
      "3180: accuracy:0.89 loss: 1.57014 (lr:7.14)\n",
      "3200: accuracy:0.82 loss: 1.63872 (lr:7.14)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.8676 test loss: 1.59311\n",
      "3220: accuracy:0.81 loss: 1.65305 (lr:7.14)\n",
      "3240: accuracy:0.89 loss: 1.56853 (lr:7.14)\n",
      "3260: accuracy:0.88 loss: 1.58061 (lr:7.14)\n",
      "3280: accuracy:0.89 loss: 1.57171 (lr:7.14)\n",
      "3300: accuracy:0.92 loss: 1.54083 (lr:7.14)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.868 test loss: 1.59229\n",
      "3320: accuracy:0.89 loss: 1.57166 (lr:7.14)\n",
      "3340: accuracy:0.9 loss: 1.55703 (lr:7.14)\n",
      "3360: accuracy:0.88 loss: 1.58208 (lr:7.14)\n",
      "3380: accuracy:0.85 loss: 1.61112 (lr:7.14)\n",
      "3400: accuracy:0.89 loss: 1.57189 (lr:7.14)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.8626 test loss: 1.59808\n",
      "3420: accuracy:0.88 loss: 1.58164 (lr:7.14)\n",
      "3440: accuracy:0.89 loss: 1.57116 (lr:7.14)\n",
      "3460: accuracy:0.89 loss: 1.57059 (lr:7.14)\n",
      "3480: accuracy:0.85 loss: 1.60582 (lr:7.14)\n",
      "3500: accuracy:0.85 loss: 1.61063 (lr:7.14)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.8628 test loss: 1.59763\n",
      "3520: accuracy:0.9 loss: 1.56022 (lr:7.14)\n",
      "3540: accuracy:0.89 loss: 1.57061 (lr:7.14)\n",
      "3560: accuracy:0.93 loss: 1.53104 (lr:7.14)\n",
      "3580: accuracy:0.82 loss: 1.63572 (lr:7.14)\n",
      "3600: accuracy:0.93 loss: 1.53258 (lr:7.14)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.8595 test loss: 1.60172\n",
      "3620: accuracy:0.84 loss: 1.61395 (lr:7.14)\n",
      "3640: accuracy:0.91 loss: 1.55366 (lr:7.14)\n",
      "3660: accuracy:0.85 loss: 1.61194 (lr:7.14)\n",
      "3680: accuracy:0.82 loss: 1.63742 (lr:7.14)\n",
      "3700: accuracy:0.94 loss: 1.52275 (lr:7.14)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.8666 test loss: 1.59411\n",
      "3720: accuracy:0.79 loss: 1.66914 (lr:7.14)\n",
      "3740: accuracy:0.86 loss: 1.60172 (lr:7.14)\n",
      "3760: accuracy:0.87 loss: 1.59086 (lr:7.14)\n",
      "3780: accuracy:0.84 loss: 1.62065 (lr:7.14)\n",
      "3800: accuracy:0.89 loss: 1.57117 (lr:7.14)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.8648 test loss: 1.59608\n",
      "3820: accuracy:0.81 loss: 1.65045 (lr:7.14)\n",
      "3840: accuracy:0.91 loss: 1.55134 (lr:7.14)\n",
      "3860: accuracy:0.86 loss: 1.5948 (lr:7.14)\n",
      "3880: accuracy:0.81 loss: 1.65127 (lr:7.14)\n",
      "3900: accuracy:0.9 loss: 1.55892 (lr:7.14)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.8653 test loss: 1.59502\n",
      "3920: accuracy:0.85 loss: 1.61149 (lr:7.14)\n",
      "3940: accuracy:0.84 loss: 1.62205 (lr:7.14)\n",
      "3960: accuracy:0.91 loss: 1.5489 (lr:7.14)\n",
      "3980: accuracy:0.89 loss: 1.56791 (lr:7.14)\n",
      "4000: accuracy:0.93 loss: 1.5314 (lr:7.14)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.8667 test loss: 1.59424\n",
      "4020: accuracy:0.89 loss: 1.5708 (lr:7.14)\n",
      "4040: accuracy:0.87 loss: 1.59287 (lr:7.14)\n",
      "4060: accuracy:0.89 loss: 1.56613 (lr:7.14)\n",
      "4080: accuracy:0.82 loss: 1.6421 (lr:7.14)\n",
      "4100: accuracy:0.85 loss: 1.6101 (lr:7.14)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.8634 test loss: 1.59756\n",
      "4120: accuracy:0.9 loss: 1.56316 (lr:7.14)\n",
      "4140: accuracy:0.86 loss: 1.59712 (lr:7.14)\n",
      "4160: accuracy:0.87 loss: 1.59167 (lr:7.14)\n",
      "4180: accuracy:0.92 loss: 1.54036 (lr:7.14)\n",
      "4200: accuracy:0.9 loss: 1.56091 (lr:7.14)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.8681 test loss: 1.59281\n",
      "4220: accuracy:0.86 loss: 1.60022 (lr:7.14)\n",
      "4240: accuracy:0.9 loss: 1.56537 (lr:7.14)\n",
      "4260: accuracy:0.91 loss: 1.5511 (lr:7.14)\n",
      "4280: accuracy:0.88 loss: 1.58059 (lr:7.14)\n",
      "4300: accuracy:0.86 loss: 1.60085 (lr:7.14)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.8702 test loss: 1.59044\n",
      "4320: accuracy:0.88 loss: 1.58076 (lr:7.14)\n",
      "4340: accuracy:0.89 loss: 1.57127 (lr:7.14)\n",
      "4360: accuracy:0.81 loss: 1.65134 (lr:7.14)\n",
      "4380: accuracy:0.86 loss: 1.59743 (lr:7.14)\n",
      "4400: accuracy:0.91 loss: 1.55078 (lr:7.14)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.8712 test loss: 1.5896\n",
      "4420: accuracy:0.87 loss: 1.58941 (lr:7.14)\n",
      "4440: accuracy:0.87 loss: 1.59132 (lr:7.14)\n",
      "4460: accuracy:0.86 loss: 1.60504 (lr:7.14)\n",
      "4480: accuracy:0.89 loss: 1.57066 (lr:7.14)\n",
      "4500: accuracy:0.79 loss: 1.66349 (lr:7.14)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.8713 test loss: 1.58935\n",
      "4520: accuracy:0.88 loss: 1.58054 (lr:7.14)\n",
      "4540: accuracy:0.87 loss: 1.59265 (lr:7.14)\n",
      "4560: accuracy:0.94 loss: 1.52094 (lr:7.14)\n",
      "4580: accuracy:0.94 loss: 1.51715 (lr:7.14)\n",
      "4600: accuracy:0.92 loss: 1.5358 (lr:7.14)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9461 test loss: 1.51568\n",
      "4620: accuracy:0.92 loss: 1.5305 (lr:7.14)\n",
      "4640: accuracy:0.97 loss: 1.49996 (lr:7.14)\n",
      "4660: accuracy:0.93 loss: 1.52574 (lr:7.14)\n",
      "4680: accuracy:0.92 loss: 1.54815 (lr:7.14)\n",
      "4700: accuracy:0.89 loss: 1.57456 (lr:7.14)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9406 test loss: 1.51999\n",
      "4720: accuracy:0.93 loss: 1.53325 (lr:7.14)\n",
      "4740: accuracy:0.95 loss: 1.51344 (lr:7.14)\n",
      "4760: accuracy:0.95 loss: 1.51664 (lr:7.14)\n",
      "4780: accuracy:1.0 loss: 1.46124 (lr:7.14)\n",
      "4800: accuracy:0.92 loss: 1.53402 (lr:7.14)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9498 test loss: 1.51107\n",
      "4820: accuracy:1.0 loss: 1.46155 (lr:7.14)\n",
      "4840: accuracy:0.97 loss: 1.49337 (lr:7.14)\n",
      "4860: accuracy:0.98 loss: 1.4816 (lr:7.14)\n",
      "4880: accuracy:0.9 loss: 1.55921 (lr:7.14)\n",
      "4900: accuracy:0.98 loss: 1.48212 (lr:7.14)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9537 test loss: 1.50695\n",
      "4920: accuracy:0.96 loss: 1.50292 (lr:7.14)\n",
      "4940: accuracy:0.97 loss: 1.49357 (lr:7.14)\n",
      "4960: accuracy:0.92 loss: 1.54559 (lr:7.14)\n",
      "4980: accuracy:0.96 loss: 1.50162 (lr:7.14)\n",
      "5000: accuracy:0.94 loss: 1.52072 (lr:7.14)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9418 test loss: 1.51879\n",
      "5020: accuracy:0.96 loss: 1.50269 (lr:7.14)\n",
      "5040: accuracy:0.97 loss: 1.49498 (lr:7.14)\n",
      "5060: accuracy:0.97 loss: 1.50205 (lr:7.14)\n",
      "5080: accuracy:0.91 loss: 1.54709 (lr:7.14)\n",
      "5100: accuracy:0.97 loss: 1.49094 (lr:7.14)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9492 test loss: 1.51174\n",
      "5120: accuracy:0.98 loss: 1.4828 (lr:7.14)\n",
      "5140: accuracy:0.97 loss: 1.49003 (lr:7.14)\n",
      "5160: accuracy:0.94 loss: 1.52145 (lr:7.14)\n",
      "5180: accuracy:0.94 loss: 1.51511 (lr:7.14)\n",
      "5200: accuracy:1.0 loss: 1.46399 (lr:7.14)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9576 test loss: 1.50331\n",
      "5220: accuracy:0.94 loss: 1.51737 (lr:7.14)\n",
      "5240: accuracy:0.96 loss: 1.50069 (lr:7.14)\n",
      "5260: accuracy:0.98 loss: 1.48143 (lr:7.14)\n",
      "5280: accuracy:0.94 loss: 1.52243 (lr:7.14)\n",
      "5300: accuracy:0.96 loss: 1.50105 (lr:7.14)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9564 test loss: 1.50496\n",
      "5320: accuracy:0.98 loss: 1.48496 (lr:7.14)\n",
      "5340: accuracy:0.93 loss: 1.53471 (lr:7.14)\n",
      "5360: accuracy:0.97 loss: 1.49181 (lr:7.14)\n",
      "5380: accuracy:0.94 loss: 1.52468 (lr:7.14)\n",
      "5400: accuracy:0.93 loss: 1.52708 (lr:7.14)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9356 test loss: 1.52555\n",
      "5420: accuracy:0.98 loss: 1.48309 (lr:7.14)\n",
      "5440: accuracy:0.92 loss: 1.53729 (lr:7.14)\n",
      "5460: accuracy:0.93 loss: 1.53186 (lr:7.14)\n",
      "5480: accuracy:0.91 loss: 1.55507 (lr:7.14)\n",
      "5500: accuracy:0.97 loss: 1.48917 (lr:7.14)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9519 test loss: 1.50864\n",
      "5520: accuracy:0.97 loss: 1.48942 (lr:7.14)\n",
      "5540: accuracy:0.92 loss: 1.53726 (lr:7.14)\n",
      "5560: accuracy:0.94 loss: 1.52099 (lr:7.14)\n",
      "5580: accuracy:0.92 loss: 1.53883 (lr:7.14)\n",
      "5600: accuracy:0.98 loss: 1.48373 (lr:7.14)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9545 test loss: 1.50683\n",
      "5620: accuracy:0.92 loss: 1.54122 (lr:7.14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5640: accuracy:0.98 loss: 1.4836 (lr:7.14)\n",
      "5660: accuracy:0.99 loss: 1.47253 (lr:7.14)\n",
      "5680: accuracy:0.93 loss: 1.53467 (lr:7.14)\n",
      "5700: accuracy:0.96 loss: 1.50146 (lr:7.14)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9483 test loss: 1.51244\n",
      "5720: accuracy:0.98 loss: 1.48698 (lr:7.14)\n",
      "5740: accuracy:0.93 loss: 1.52621 (lr:7.14)\n",
      "5760: accuracy:0.97 loss: 1.49102 (lr:7.14)\n",
      "5780: accuracy:1.0 loss: 1.46134 (lr:7.14)\n",
      "5800: accuracy:0.94 loss: 1.52047 (lr:7.14)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.942 test loss: 1.51864\n",
      "5820: accuracy:0.97 loss: 1.49116 (lr:7.14)\n",
      "5840: accuracy:0.93 loss: 1.53142 (lr:7.14)\n",
      "5860: accuracy:0.97 loss: 1.49115 (lr:7.14)\n",
      "5880: accuracy:0.92 loss: 1.54942 (lr:7.14)\n",
      "5900: accuracy:0.97 loss: 1.49529 (lr:7.14)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9564 test loss: 1.50476\n",
      "5920: accuracy:0.98 loss: 1.48116 (lr:7.14)\n",
      "5940: accuracy:0.98 loss: 1.48181 (lr:7.14)\n",
      "5960: accuracy:0.96 loss: 1.50117 (lr:7.14)\n",
      "5980: accuracy:0.98 loss: 1.48115 (lr:7.14)\n",
      "6000: accuracy:0.95 loss: 1.51472 (lr:7.14)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9415 test loss: 1.51909\n",
      "6020: accuracy:0.96 loss: 1.50977 (lr:7.14)\n",
      "6040: accuracy:0.98 loss: 1.48116 (lr:7.14)\n",
      "6060: accuracy:0.94 loss: 1.51986 (lr:7.14)\n",
      "6080: accuracy:0.93 loss: 1.52525 (lr:7.14)\n",
      "6100: accuracy:0.99 loss: 1.4709 (lr:7.14)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9605 test loss: 1.50076\n",
      "6120: accuracy:0.98 loss: 1.47998 (lr:7.14)\n",
      "6140: accuracy:0.98 loss: 1.47902 (lr:7.14)\n",
      "6160: accuracy:0.97 loss: 1.49105 (lr:7.14)\n",
      "6180: accuracy:0.97 loss: 1.49118 (lr:7.14)\n",
      "6200: accuracy:0.92 loss: 1.53743 (lr:7.14)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9526 test loss: 1.50837\n",
      "6220: accuracy:0.93 loss: 1.53094 (lr:7.14)\n",
      "6240: accuracy:0.95 loss: 1.51199 (lr:7.14)\n",
      "6260: accuracy:0.93 loss: 1.53567 (lr:7.14)\n",
      "6280: accuracy:0.98 loss: 1.48124 (lr:7.14)\n",
      "6300: accuracy:0.92 loss: 1.53316 (lr:7.14)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.956 test loss: 1.50511\n",
      "6320: accuracy:0.97 loss: 1.49305 (lr:7.14)\n",
      "6340: accuracy:0.97 loss: 1.49482 (lr:7.14)\n",
      "6360: accuracy:0.95 loss: 1.51073 (lr:7.14)\n",
      "6380: accuracy:0.99 loss: 1.47115 (lr:7.14)\n",
      "6400: accuracy:0.92 loss: 1.5391 (lr:7.14)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9548 test loss: 1.50621\n",
      "6420: accuracy:0.96 loss: 1.50116 (lr:7.14)\n",
      "6440: accuracy:0.92 loss: 1.54092 (lr:7.14)\n",
      "6460: accuracy:0.94 loss: 1.5222 (lr:7.14)\n",
      "6480: accuracy:0.96 loss: 1.49844 (lr:7.14)\n",
      "6500: accuracy:0.99 loss: 1.47327 (lr:7.14)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9422 test loss: 1.51846\n",
      "6520: accuracy:0.94 loss: 1.52389 (lr:7.14)\n",
      "6540: accuracy:0.97 loss: 1.49081 (lr:7.14)\n",
      "6560: accuracy:0.98 loss: 1.48006 (lr:7.14)\n",
      "6580: accuracy:0.95 loss: 1.51082 (lr:7.14)\n",
      "6600: accuracy:0.91 loss: 1.5477 (lr:7.14)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9492 test loss: 1.51184\n",
      "6620: accuracy:0.97 loss: 1.49505 (lr:7.14)\n",
      "6640: accuracy:0.96 loss: 1.49737 (lr:7.14)\n",
      "6660: accuracy:0.96 loss: 1.50523 (lr:7.14)\n",
      "6680: accuracy:0.92 loss: 1.54102 (lr:7.14)\n",
      "6700: accuracy:0.95 loss: 1.50627 (lr:7.14)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9573 test loss: 1.50377\n",
      "6720: accuracy:0.97 loss: 1.49159 (lr:7.14)\n",
      "6740: accuracy:0.93 loss: 1.53062 (lr:7.14)\n",
      "6760: accuracy:0.97 loss: 1.49129 (lr:7.14)\n",
      "6780: accuracy:0.97 loss: 1.49079 (lr:7.14)\n",
      "6800: accuracy:0.96 loss: 1.5065 (lr:7.14)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9561 test loss: 1.50522\n",
      "6820: accuracy:0.98 loss: 1.48338 (lr:7.14)\n",
      "6840: accuracy:0.93 loss: 1.53274 (lr:7.14)\n",
      "6860: accuracy:0.98 loss: 1.48116 (lr:7.14)\n",
      "6880: accuracy:0.93 loss: 1.53142 (lr:7.14)\n",
      "6900: accuracy:0.92 loss: 1.54115 (lr:7.14)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9523 test loss: 1.50852\n",
      "6920: accuracy:0.97 loss: 1.49107 (lr:7.14)\n",
      "6940: accuracy:0.96 loss: 1.50115 (lr:7.14)\n",
      "6960: accuracy:0.98 loss: 1.48137 (lr:7.14)\n",
      "6980: accuracy:0.94 loss: 1.52201 (lr:7.14)\n",
      "7000: accuracy:0.95 loss: 1.5104 (lr:7.14)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9556 test loss: 1.50557\n",
      "7020: accuracy:0.96 loss: 1.4921 (lr:7.14)\n",
      "7040: accuracy:0.96 loss: 1.50113 (lr:7.14)\n",
      "7060: accuracy:0.97 loss: 1.48529 (lr:7.14)\n",
      "7080: accuracy:0.97 loss: 1.49165 (lr:7.14)\n",
      "7100: accuracy:0.98 loss: 1.48442 (lr:7.14)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9572 test loss: 1.50399\n",
      "7120: accuracy:0.94 loss: 1.52096 (lr:7.14)\n",
      "7140: accuracy:0.95 loss: 1.51471 (lr:7.14)\n",
      "7160: accuracy:0.95 loss: 1.51067 (lr:7.14)\n",
      "7180: accuracy:0.96 loss: 1.50115 (lr:7.14)\n",
      "7200: accuracy:0.96 loss: 1.50115 (lr:7.14)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9612 test loss: 1.49993\n",
      "7220: accuracy:0.96 loss: 1.50128 (lr:7.14)\n",
      "7240: accuracy:0.96 loss: 1.50284 (lr:7.14)\n",
      "7260: accuracy:0.96 loss: 1.50068 (lr:7.14)\n",
      "7280: accuracy:0.99 loss: 1.47849 (lr:7.14)\n",
      "7300: accuracy:0.99 loss: 1.4733 (lr:7.14)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9561 test loss: 1.50502\n",
      "7320: accuracy:0.96 loss: 1.5008 (lr:7.14)\n",
      "7340: accuracy:0.95 loss: 1.51183 (lr:7.14)\n",
      "7360: accuracy:0.97 loss: 1.49115 (lr:7.14)\n",
      "7380: accuracy:0.95 loss: 1.51107 (lr:7.14)\n",
      "7400: accuracy:0.97 loss: 1.4921 (lr:7.14)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9562 test loss: 1.50492\n",
      "7420: accuracy:0.94 loss: 1.52144 (lr:7.14)\n",
      "7440: accuracy:0.98 loss: 1.48111 (lr:7.14)\n",
      "7460: accuracy:0.97 loss: 1.49046 (lr:7.14)\n",
      "7480: accuracy:0.93 loss: 1.53121 (lr:7.14)\n",
      "7500: accuracy:0.97 loss: 1.49141 (lr:7.14)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9588 test loss: 1.50212\n",
      "7520: accuracy:0.98 loss: 1.48206 (lr:7.14)\n",
      "7540: accuracy:0.96 loss: 1.50133 (lr:7.14)\n",
      "7560: accuracy:1.0 loss: 1.46779 (lr:7.14)\n",
      "7580: accuracy:0.99 loss: 1.47115 (lr:7.14)\n",
      "7600: accuracy:0.99 loss: 1.47123 (lr:7.14)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9418 test loss: 1.51944\n",
      "7620: accuracy:0.96 loss: 1.49808 (lr:7.14)\n",
      "7640: accuracy:0.98 loss: 1.48115 (lr:7.14)\n",
      "7660: accuracy:0.96 loss: 1.50127 (lr:7.14)\n",
      "7680: accuracy:0.94 loss: 1.51879 (lr:7.14)\n",
      "7700: accuracy:0.98 loss: 1.48112 (lr:7.14)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9579 test loss: 1.50313\n",
      "7720: accuracy:0.95 loss: 1.51119 (lr:7.14)\n",
      "7740: accuracy:0.98 loss: 1.48115 (lr:7.14)\n",
      "7760: accuracy:0.95 loss: 1.51122 (lr:7.14)\n",
      "7780: accuracy:0.95 loss: 1.51142 (lr:7.14)\n",
      "7800: accuracy:0.94 loss: 1.52035 (lr:7.14)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9588 test loss: 1.50242\n",
      "7820: accuracy:0.98 loss: 1.48121 (lr:7.14)\n",
      "7840: accuracy:0.97 loss: 1.48935 (lr:7.14)\n",
      "7860: accuracy:0.97 loss: 1.49132 (lr:7.14)\n",
      "7880: accuracy:0.96 loss: 1.49991 (lr:7.14)\n",
      "7900: accuracy:0.93 loss: 1.52959 (lr:7.14)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9525 test loss: 1.50849\n",
      "7920: accuracy:0.99 loss: 1.47213 (lr:7.14)\n",
      "7940: accuracy:0.97 loss: 1.49158 (lr:7.14)\n",
      "7960: accuracy:0.99 loss: 1.47115 (lr:7.14)\n",
      "7980: accuracy:0.94 loss: 1.51686 (lr:7.14)\n",
      "8000: accuracy:0.94 loss: 1.52409 (lr:7.14)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9589 test loss: 1.50196\n",
      "8020: accuracy:0.96 loss: 1.50122 (lr:7.14)\n",
      "8040: accuracy:0.93 loss: 1.53105 (lr:7.14)\n",
      "8060: accuracy:0.97 loss: 1.49581 (lr:7.14)\n",
      "8080: accuracy:0.95 loss: 1.51115 (lr:7.14)\n",
      "8100: accuracy:0.96 loss: 1.50112 (lr:7.14)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9613 test loss: 1.49952\n",
      "8120: accuracy:0.97 loss: 1.48847 (lr:7.14)\n",
      "8140: accuracy:0.95 loss: 1.51105 (lr:7.14)\n",
      "8160: accuracy:0.97 loss: 1.49138 (lr:7.14)\n",
      "8180: accuracy:0.99 loss: 1.47484 (lr:7.14)\n",
      "8200: accuracy:0.99 loss: 1.47193 (lr:7.14)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9581 test loss: 1.50301\n",
      "8220: accuracy:0.98 loss: 1.48119 (lr:7.14)\n",
      "8240: accuracy:0.97 loss: 1.49123 (lr:7.14)\n",
      "8260: accuracy:0.96 loss: 1.50116 (lr:7.14)\n",
      "8280: accuracy:0.96 loss: 1.50359 (lr:7.14)\n",
      "8300: accuracy:0.96 loss: 1.50115 (lr:7.14)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9615 test loss: 1.49944\n",
      "8320: accuracy:0.98 loss: 1.4823 (lr:7.14)\n",
      "8340: accuracy:0.96 loss: 1.50429 (lr:7.14)\n",
      "8360: accuracy:0.99 loss: 1.47246 (lr:7.14)\n",
      "8380: accuracy:0.96 loss: 1.49923 (lr:7.14)\n",
      "8400: accuracy:0.98 loss: 1.48897 (lr:7.14)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9614 test loss: 1.49965\n",
      "8420: accuracy:0.97 loss: 1.49188 (lr:7.14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8440: accuracy:0.97 loss: 1.49115 (lr:7.14)\n",
      "8460: accuracy:0.97 loss: 1.49156 (lr:7.14)\n",
      "8480: accuracy:0.97 loss: 1.48948 (lr:7.14)\n",
      "8500: accuracy:0.95 loss: 1.51428 (lr:7.14)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9529 test loss: 1.50804\n",
      "8520: accuracy:0.97 loss: 1.48876 (lr:7.14)\n",
      "8540: accuracy:0.96 loss: 1.50115 (lr:7.14)\n",
      "8560: accuracy:0.97 loss: 1.49082 (lr:7.14)\n",
      "8580: accuracy:0.93 loss: 1.53115 (lr:7.14)\n",
      "8600: accuracy:0.94 loss: 1.52535 (lr:7.14)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9561 test loss: 1.50473\n",
      "8620: accuracy:0.97 loss: 1.48879 (lr:7.14)\n",
      "8640: accuracy:0.97 loss: 1.48997 (lr:7.14)\n",
      "8660: accuracy:0.98 loss: 1.48115 (lr:7.14)\n",
      "8680: accuracy:0.92 loss: 1.53703 (lr:7.14)\n",
      "8700: accuracy:0.98 loss: 1.48112 (lr:7.14)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9607 test loss: 1.50024\n",
      "8720: accuracy:0.98 loss: 1.48116 (lr:7.14)\n",
      "8740: accuracy:0.97 loss: 1.49115 (lr:7.14)\n",
      "8760: accuracy:0.99 loss: 1.47116 (lr:7.14)\n",
      "8780: accuracy:0.97 loss: 1.49135 (lr:7.14)\n",
      "8800: accuracy:0.97 loss: 1.49311 (lr:7.14)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9587 test loss: 1.50215\n",
      "8820: accuracy:0.99 loss: 1.47117 (lr:7.14)\n",
      "8840: accuracy:0.93 loss: 1.531 (lr:7.14)\n",
      "8860: accuracy:0.97 loss: 1.49115 (lr:7.14)\n",
      "8880: accuracy:0.99 loss: 1.4732 (lr:7.14)\n",
      "8900: accuracy:0.98 loss: 1.48141 (lr:7.14)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9635 test loss: 1.4975\n",
      "8920: accuracy:0.98 loss: 1.48115 (lr:7.14)\n",
      "8940: accuracy:0.96 loss: 1.50092 (lr:7.14)\n",
      "8960: accuracy:0.96 loss: 1.50115 (lr:7.14)\n",
      "8980: accuracy:0.97 loss: 1.49115 (lr:7.14)\n",
      "9000: accuracy:0.97 loss: 1.48696 (lr:7.14)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9636 test loss: 1.49725\n",
      "9020: accuracy:0.94 loss: 1.51984 (lr:7.14)\n",
      "9040: accuracy:0.95 loss: 1.511 (lr:7.14)\n",
      "9060: accuracy:0.97 loss: 1.49109 (lr:7.14)\n",
      "9080: accuracy:0.97 loss: 1.49115 (lr:7.14)\n",
      "9100: accuracy:0.93 loss: 1.52742 (lr:7.14)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9635 test loss: 1.49734\n",
      "9120: accuracy:0.95 loss: 1.5131 (lr:7.14)\n",
      "9140: accuracy:0.98 loss: 1.48134 (lr:7.14)\n",
      "9160: accuracy:0.96 loss: 1.50042 (lr:7.14)\n",
      "9180: accuracy:0.95 loss: 1.51055 (lr:7.14)\n",
      "9200: accuracy:0.96 loss: 1.50278 (lr:7.14)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9465 test loss: 1.51422\n",
      "9220: accuracy:0.94 loss: 1.52091 (lr:7.14)\n",
      "9240: accuracy:0.96 loss: 1.50275 (lr:7.14)\n",
      "9260: accuracy:0.95 loss: 1.51117 (lr:7.14)\n",
      "9280: accuracy:1.0 loss: 1.4615 (lr:7.14)\n",
      "9300: accuracy:0.99 loss: 1.47115 (lr:7.14)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9639 test loss: 1.49725\n",
      "9320: accuracy:0.93 loss: 1.53124 (lr:7.14)\n",
      "9340: accuracy:0.91 loss: 1.554 (lr:7.14)\n",
      "9360: accuracy:0.99 loss: 1.47135 (lr:7.14)\n",
      "9380: accuracy:0.98 loss: 1.48184 (lr:7.14)\n",
      "9400: accuracy:0.97 loss: 1.49102 (lr:7.14)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9545 test loss: 1.50638\n",
      "9420: accuracy:0.98 loss: 1.48046 (lr:7.14)\n",
      "9440: accuracy:0.98 loss: 1.48215 (lr:7.14)\n",
      "9460: accuracy:0.96 loss: 1.50505 (lr:7.14)\n",
      "9480: accuracy:0.99 loss: 1.47115 (lr:7.14)\n",
      "9500: accuracy:0.97 loss: 1.49413 (lr:7.14)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9615 test loss: 1.49954\n",
      "9520: accuracy:0.93 loss: 1.52621 (lr:7.14)\n",
      "9540: accuracy:0.96 loss: 1.50123 (lr:7.14)\n",
      "9560: accuracy:0.95 loss: 1.5114 (lr:7.14)\n",
      "9580: accuracy:0.96 loss: 1.50114 (lr:7.14)\n",
      "9600: accuracy:0.93 loss: 1.5302 (lr:7.14)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.96 test loss: 1.50118\n",
      "9620: accuracy:0.95 loss: 1.51965 (lr:7.14)\n",
      "9640: accuracy:0.96 loss: 1.50745 (lr:7.14)\n",
      "9660: accuracy:0.96 loss: 1.50122 (lr:7.14)\n",
      "9680: accuracy:0.96 loss: 1.49918 (lr:7.14)\n",
      "9700: accuracy:0.93 loss: 1.53137 (lr:7.14)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9572 test loss: 1.50359\n",
      "9720: accuracy:0.99 loss: 1.47148 (lr:7.14)\n",
      "9740: accuracy:0.97 loss: 1.49115 (lr:7.14)\n",
      "9760: accuracy:0.94 loss: 1.52118 (lr:7.14)\n",
      "9780: accuracy:0.97 loss: 1.48657 (lr:7.14)\n",
      "9800: accuracy:0.97 loss: 1.49118 (lr:7.14)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9639 test loss: 1.49753\n",
      "9820: accuracy:0.95 loss: 1.51088 (lr:7.14)\n",
      "9840: accuracy:0.99 loss: 1.47115 (lr:7.14)\n",
      "9860: accuracy:0.96 loss: 1.49814 (lr:7.14)\n",
      "9880: accuracy:0.98 loss: 1.48259 (lr:7.14)\n",
      "9900: accuracy:0.96 loss: 1.50103 (lr:7.14)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9624 test loss: 1.49875\n",
      "9920: accuracy:0.94 loss: 1.52095 (lr:7.14)\n",
      "9940: accuracy:0.98 loss: 1.48101 (lr:7.14)\n",
      "9960: accuracy:0.97 loss: 1.49114 (lr:7.14)\n",
      "9980: accuracy:0.97 loss: 1.49111 (lr:7.14)\n",
      "10000: accuracy:0.96 loss: 1.50056 (lr:7.14)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9527 test loss: 1.5082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:42<02:49, 42.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.09 loss: 2.30653 (lr:1.08)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1047 test loss: 2.3068\n",
      "20: accuracy:0.61 loss: 1.89505 (lr:1.08)\n",
      "40: accuracy:0.58 loss: 1.90434 (lr:1.08)\n",
      "60: accuracy:0.69 loss: 1.78426 (lr:1.08)\n",
      "80: accuracy:0.81 loss: 1.68594 (lr:1.08)\n",
      "100: accuracy:0.74 loss: 1.7208 (lr:1.08)\n",
      "100: ********* epoch 1 ********* test accuracy:0.743 test loss: 1.72914\n",
      "120: accuracy:0.81 loss: 1.66282 (lr:1.08)\n",
      "140: accuracy:0.79 loss: 1.67748 (lr:1.08)\n",
      "160: accuracy:0.8 loss: 1.6725 (lr:1.08)\n",
      "180: accuracy:0.87 loss: 1.59834 (lr:1.08)\n",
      "200: accuracy:0.85 loss: 1.62668 (lr:1.08)\n",
      "200: ********* epoch 1 ********* test accuracy:0.8335 test loss: 1.64083\n",
      "220: accuracy:0.86 loss: 1.6427 (lr:1.08)\n",
      "240: accuracy:0.74 loss: 1.72282 (lr:1.08)\n",
      "260: accuracy:0.89 loss: 1.58316 (lr:1.08)\n",
      "280: accuracy:0.9 loss: 1.5747 (lr:1.08)\n",
      "300: accuracy:0.92 loss: 1.53829 (lr:1.08)\n",
      "300: ********* epoch 1 ********* test accuracy:0.9029 test loss: 1.57126\n",
      "320: accuracy:0.96 loss: 1.52436 (lr:1.08)\n",
      "340: accuracy:0.93 loss: 1.53573 (lr:1.08)\n",
      "360: accuracy:0.9 loss: 1.58385 (lr:1.08)\n",
      "380: accuracy:0.88 loss: 1.57842 (lr:1.08)\n",
      "400: accuracy:0.84 loss: 1.63119 (lr:1.08)\n",
      "400: ********* epoch 1 ********* test accuracy:0.9097 test loss: 1.55827\n",
      "420: accuracy:0.94 loss: 1.5273 (lr:1.08)\n",
      "440: accuracy:0.89 loss: 1.57972 (lr:1.08)\n",
      "460: accuracy:0.88 loss: 1.58319 (lr:1.08)\n",
      "480: accuracy:0.83 loss: 1.6187 (lr:1.08)\n",
      "500: accuracy:0.93 loss: 1.53641 (lr:1.08)\n",
      "500: ********* epoch 1 ********* test accuracy:0.9103 test loss: 1.55679\n",
      "520: accuracy:0.96 loss: 1.51301 (lr:1.08)\n",
      "540: accuracy:0.91 loss: 1.55725 (lr:1.08)\n",
      "560: accuracy:0.96 loss: 1.50837 (lr:1.08)\n",
      "580: accuracy:0.91 loss: 1.55961 (lr:1.08)\n",
      "600: accuracy:0.91 loss: 1.55873 (lr:1.08)\n",
      "600: ********* epoch 2 ********* test accuracy:0.9194 test loss: 1.54901\n",
      "620: accuracy:0.9 loss: 1.56406 (lr:1.08)\n",
      "640: accuracy:0.94 loss: 1.5432 (lr:1.08)\n",
      "660: accuracy:0.92 loss: 1.55958 (lr:1.08)\n",
      "680: accuracy:0.96 loss: 1.50592 (lr:1.08)\n",
      "700: accuracy:0.94 loss: 1.53082 (lr:1.08)\n",
      "700: ********* epoch 2 ********* test accuracy:0.9313 test loss: 1.53542\n",
      "720: accuracy:0.95 loss: 1.52453 (lr:1.08)\n",
      "740: accuracy:0.9 loss: 1.57196 (lr:1.08)\n",
      "760: accuracy:0.94 loss: 1.53928 (lr:1.08)\n",
      "780: accuracy:0.93 loss: 1.53177 (lr:1.08)\n",
      "800: accuracy:0.99 loss: 1.48775 (lr:1.08)\n",
      "800: ********* epoch 2 ********* test accuracy:0.9331 test loss: 1.53219\n",
      "820: accuracy:0.97 loss: 1.50195 (lr:1.08)\n",
      "840: accuracy:0.94 loss: 1.52996 (lr:1.08)\n",
      "860: accuracy:0.96 loss: 1.51108 (lr:1.08)\n",
      "880: accuracy:0.87 loss: 1.58358 (lr:1.08)\n",
      "900: accuracy:0.93 loss: 1.54727 (lr:1.08)\n",
      "900: ********* epoch 2 ********* test accuracy:0.9356 test loss: 1.53106\n",
      "920: accuracy:0.92 loss: 1.54354 (lr:1.08)\n",
      "940: accuracy:0.94 loss: 1.52167 (lr:1.08)\n",
      "960: accuracy:0.92 loss: 1.55032 (lr:1.08)\n",
      "980: accuracy:0.9 loss: 1.56094 (lr:1.08)\n",
      "1000: accuracy:0.95 loss: 1.51005 (lr:1.08)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9311 test loss: 1.53412\n",
      "1020: accuracy:0.95 loss: 1.5129 (lr:1.08)\n",
      "1040: accuracy:0.94 loss: 1.53335 (lr:1.08)\n",
      "1060: accuracy:0.95 loss: 1.50945 (lr:1.08)\n",
      "1080: accuracy:0.95 loss: 1.5197 (lr:1.08)\n",
      "1100: accuracy:0.9 loss: 1.55036 (lr:1.08)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9387 test loss: 1.52702\n",
      "1120: accuracy:0.93 loss: 1.52442 (lr:1.08)\n",
      "1140: accuracy:0.97 loss: 1.4965 (lr:1.08)\n",
      "1160: accuracy:0.97 loss: 1.51004 (lr:1.08)\n",
      "1180: accuracy:0.9 loss: 1.57475 (lr:1.08)\n",
      "1200: accuracy:0.98 loss: 1.49787 (lr:1.08)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9448 test loss: 1.52186\n",
      "1220: accuracy:0.98 loss: 1.49033 (lr:1.08)\n",
      "1240: accuracy:0.94 loss: 1.5251 (lr:1.08)\n",
      "1260: accuracy:0.94 loss: 1.51728 (lr:1.08)\n",
      "1280: accuracy:0.95 loss: 1.51247 (lr:1.08)\n",
      "1300: accuracy:0.92 loss: 1.53309 (lr:1.08)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9447 test loss: 1.52099\n",
      "1320: accuracy:0.92 loss: 1.53042 (lr:1.08)\n",
      "1340: accuracy:0.92 loss: 1.53556 (lr:1.08)\n",
      "1360: accuracy:0.91 loss: 1.54347 (lr:1.08)\n",
      "1380: accuracy:0.92 loss: 1.55516 (lr:1.08)\n",
      "1400: accuracy:0.94 loss: 1.5408 (lr:1.08)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9441 test loss: 1.52148\n",
      "1420: accuracy:0.95 loss: 1.5176 (lr:1.08)\n",
      "1440: accuracy:0.95 loss: 1.5185 (lr:1.08)\n",
      "1460: accuracy:0.96 loss: 1.49976 (lr:1.08)\n",
      "1480: accuracy:0.97 loss: 1.49508 (lr:1.08)\n",
      "1500: accuracy:0.95 loss: 1.51254 (lr:1.08)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9496 test loss: 1.51605\n",
      "1520: accuracy:0.93 loss: 1.53406 (lr:1.08)\n",
      "1540: accuracy:0.92 loss: 1.54609 (lr:1.08)\n",
      "1560: accuracy:0.98 loss: 1.48619 (lr:1.08)\n",
      "1580: accuracy:0.95 loss: 1.52319 (lr:1.08)\n",
      "1600: accuracy:0.97 loss: 1.49184 (lr:1.08)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9498 test loss: 1.51555\n",
      "1620: accuracy:0.98 loss: 1.49038 (lr:1.08)\n",
      "1640: accuracy:0.96 loss: 1.50736 (lr:1.08)\n",
      "1660: accuracy:0.97 loss: 1.4955 (lr:1.08)\n",
      "1680: accuracy:0.99 loss: 1.48039 (lr:1.08)\n",
      "1700: accuracy:0.94 loss: 1.52215 (lr:1.08)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9515 test loss: 1.51288\n",
      "1720: accuracy:0.95 loss: 1.51341 (lr:1.08)\n",
      "1740: accuracy:0.99 loss: 1.47602 (lr:1.08)\n",
      "1760: accuracy:0.96 loss: 1.49464 (lr:1.08)\n",
      "1780: accuracy:0.97 loss: 1.49674 (lr:1.08)\n",
      "1800: accuracy:0.96 loss: 1.51294 (lr:1.08)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9525 test loss: 1.51335\n",
      "1820: accuracy:0.93 loss: 1.53776 (lr:1.08)\n",
      "1840: accuracy:0.94 loss: 1.5271 (lr:1.08)\n",
      "1860: accuracy:0.96 loss: 1.50988 (lr:1.08)\n",
      "1880: accuracy:0.95 loss: 1.51505 (lr:1.08)\n",
      "1900: accuracy:0.97 loss: 1.50233 (lr:1.08)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9548 test loss: 1.51017\n",
      "1920: accuracy:0.97 loss: 1.4964 (lr:1.08)\n",
      "1940: accuracy:0.95 loss: 1.51494 (lr:1.08)\n",
      "1960: accuracy:0.96 loss: 1.49608 (lr:1.08)\n",
      "1980: accuracy:0.97 loss: 1.4883 (lr:1.08)\n",
      "2000: accuracy:0.96 loss: 1.51113 (lr:1.08)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9496 test loss: 1.51563\n",
      "2020: accuracy:0.98 loss: 1.49095 (lr:1.08)\n",
      "2040: accuracy:0.9 loss: 1.55621 (lr:1.08)\n",
      "2060: accuracy:0.96 loss: 1.50433 (lr:1.08)\n",
      "2080: accuracy:0.98 loss: 1.48348 (lr:1.08)\n",
      "2100: accuracy:0.96 loss: 1.50862 (lr:1.08)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9566 test loss: 1.50794\n",
      "2120: accuracy:0.97 loss: 1.49811 (lr:1.08)\n",
      "2140: accuracy:0.96 loss: 1.50864 (lr:1.08)\n",
      "2160: accuracy:0.95 loss: 1.51105 (lr:1.08)\n",
      "2180: accuracy:0.95 loss: 1.51061 (lr:1.08)\n",
      "2200: accuracy:0.95 loss: 1.51695 (lr:1.08)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9545 test loss: 1.51122\n",
      "2220: accuracy:1.0 loss: 1.47297 (lr:1.08)\n",
      "2240: accuracy:0.96 loss: 1.50929 (lr:1.08)\n",
      "2260: accuracy:0.98 loss: 1.48729 (lr:1.08)\n",
      "2280: accuracy:0.94 loss: 1.52834 (lr:1.08)\n",
      "2300: accuracy:0.96 loss: 1.49875 (lr:1.08)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9594 test loss: 1.50631\n",
      "2320: accuracy:1.0 loss: 1.46673 (lr:1.08)\n",
      "2340: accuracy:0.95 loss: 1.50535 (lr:1.08)\n",
      "2360: accuracy:0.89 loss: 1.56062 (lr:1.08)\n",
      "2380: accuracy:0.96 loss: 1.50405 (lr:1.08)\n",
      "2400: accuracy:0.97 loss: 1.48858 (lr:1.08)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9583 test loss: 1.50572\n",
      "2420: accuracy:1.0 loss: 1.46661 (lr:1.08)\n",
      "2440: accuracy:0.96 loss: 1.50716 (lr:1.08)\n",
      "2460: accuracy:0.96 loss: 1.5026 (lr:1.08)\n",
      "2480: accuracy:0.98 loss: 1.48547 (lr:1.08)\n",
      "2500: accuracy:0.95 loss: 1.50757 (lr:1.08)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9591 test loss: 1.50422\n",
      "2520: accuracy:0.96 loss: 1.50346 (lr:1.08)\n",
      "2540: accuracy:0.95 loss: 1.5195 (lr:1.08)\n",
      "2560: accuracy:0.95 loss: 1.51684 (lr:1.08)\n",
      "2580: accuracy:0.95 loss: 1.51746 (lr:1.08)\n",
      "2600: accuracy:0.96 loss: 1.49893 (lr:1.08)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9614 test loss: 1.50267\n",
      "2620: accuracy:0.97 loss: 1.48853 (lr:1.08)\n",
      "2640: accuracy:0.97 loss: 1.48675 (lr:1.08)\n",
      "2660: accuracy:0.98 loss: 1.48969 (lr:1.08)\n",
      "2680: accuracy:0.98 loss: 1.48969 (lr:1.08)\n",
      "2700: accuracy:0.99 loss: 1.47155 (lr:1.08)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9618 test loss: 1.50278\n",
      "2720: accuracy:0.93 loss: 1.52636 (lr:1.08)\n",
      "2740: accuracy:0.94 loss: 1.51943 (lr:1.08)\n",
      "2760: accuracy:0.98 loss: 1.47946 (lr:1.08)\n",
      "2780: accuracy:0.98 loss: 1.49158 (lr:1.08)\n",
      "2800: accuracy:0.98 loss: 1.49469 (lr:1.08)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9622 test loss: 1.50244\n",
      "2820: accuracy:0.98 loss: 1.48079 (lr:1.08)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2840: accuracy:0.95 loss: 1.5144 (lr:1.08)\n",
      "2860: accuracy:0.98 loss: 1.4834 (lr:1.08)\n",
      "2880: accuracy:0.99 loss: 1.47722 (lr:1.08)\n",
      "2900: accuracy:0.96 loss: 1.50824 (lr:1.08)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9601 test loss: 1.50362\n",
      "2920: accuracy:0.98 loss: 1.48819 (lr:1.08)\n",
      "2940: accuracy:0.96 loss: 1.48968 (lr:1.08)\n",
      "2960: accuracy:0.96 loss: 1.51394 (lr:1.08)\n",
      "2980: accuracy:0.96 loss: 1.5146 (lr:1.08)\n",
      "3000: accuracy:0.96 loss: 1.49378 (lr:1.08)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9633 test loss: 1.50117\n",
      "3020: accuracy:0.99 loss: 1.47188 (lr:1.08)\n",
      "3040: accuracy:0.96 loss: 1.50564 (lr:1.08)\n",
      "3060: accuracy:0.95 loss: 1.51303 (lr:1.08)\n",
      "3080: accuracy:0.97 loss: 1.50077 (lr:1.08)\n",
      "3100: accuracy:0.98 loss: 1.48804 (lr:1.08)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9628 test loss: 1.50115\n",
      "3120: accuracy:0.91 loss: 1.55909 (lr:1.08)\n",
      "3140: accuracy:0.99 loss: 1.47266 (lr:1.08)\n",
      "3160: accuracy:0.98 loss: 1.48357 (lr:1.08)\n",
      "3180: accuracy:0.93 loss: 1.52681 (lr:1.08)\n",
      "3200: accuracy:0.96 loss: 1.49876 (lr:1.08)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9668 test loss: 1.49751\n",
      "3220: accuracy:0.98 loss: 1.48004 (lr:1.08)\n",
      "3240: accuracy:0.98 loss: 1.48152 (lr:1.08)\n",
      "3260: accuracy:0.96 loss: 1.50714 (lr:1.08)\n",
      "3280: accuracy:0.95 loss: 1.51708 (lr:1.08)\n",
      "3300: accuracy:1.0 loss: 1.47523 (lr:1.08)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9644 test loss: 1.4998\n",
      "3320: accuracy:0.92 loss: 1.5282 (lr:1.08)\n",
      "3340: accuracy:0.96 loss: 1.50898 (lr:1.08)\n",
      "3360: accuracy:0.94 loss: 1.51331 (lr:1.08)\n",
      "3380: accuracy:0.98 loss: 1.47996 (lr:1.08)\n",
      "3400: accuracy:0.98 loss: 1.48448 (lr:1.08)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.959 test loss: 1.50361\n",
      "3420: accuracy:0.98 loss: 1.49081 (lr:1.08)\n",
      "3440: accuracy:0.97 loss: 1.49537 (lr:1.08)\n",
      "3460: accuracy:0.96 loss: 1.50058 (lr:1.08)\n",
      "3480: accuracy:0.99 loss: 1.47143 (lr:1.08)\n",
      "3500: accuracy:0.99 loss: 1.47569 (lr:1.08)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9637 test loss: 1.49901\n",
      "3520: accuracy:0.93 loss: 1.5219 (lr:1.08)\n",
      "3540: accuracy:0.97 loss: 1.49646 (lr:1.08)\n",
      "3560: accuracy:0.97 loss: 1.49465 (lr:1.08)\n",
      "3580: accuracy:0.99 loss: 1.47842 (lr:1.08)\n",
      "3600: accuracy:0.97 loss: 1.49893 (lr:1.08)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9634 test loss: 1.49973\n",
      "3620: accuracy:0.99 loss: 1.47683 (lr:1.08)\n",
      "3640: accuracy:0.98 loss: 1.48356 (lr:1.08)\n",
      "3660: accuracy:0.98 loss: 1.48169 (lr:1.08)\n",
      "3680: accuracy:0.95 loss: 1.51449 (lr:1.08)\n",
      "3700: accuracy:0.99 loss: 1.47993 (lr:1.08)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9672 test loss: 1.49714\n",
      "3720: accuracy:0.97 loss: 1.48777 (lr:1.08)\n",
      "3740: accuracy:1.0 loss: 1.46678 (lr:1.08)\n",
      "3760: accuracy:0.97 loss: 1.49542 (lr:1.08)\n",
      "3780: accuracy:0.97 loss: 1.49791 (lr:1.08)\n",
      "3800: accuracy:0.99 loss: 1.47136 (lr:1.08)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9685 test loss: 1.49537\n",
      "3820: accuracy:0.99 loss: 1.47173 (lr:1.08)\n",
      "3840: accuracy:0.96 loss: 1.50008 (lr:1.08)\n",
      "3860: accuracy:0.96 loss: 1.4952 (lr:1.08)\n",
      "3880: accuracy:0.98 loss: 1.48229 (lr:1.08)\n",
      "3900: accuracy:0.99 loss: 1.47356 (lr:1.08)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9658 test loss: 1.49811\n",
      "3920: accuracy:0.99 loss: 1.47239 (lr:1.08)\n",
      "3940: accuracy:0.99 loss: 1.47894 (lr:1.08)\n",
      "3960: accuracy:0.96 loss: 1.49799 (lr:1.08)\n",
      "3980: accuracy:0.96 loss: 1.50285 (lr:1.08)\n",
      "4000: accuracy:0.97 loss: 1.50147 (lr:1.08)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9644 test loss: 1.49886\n",
      "4020: accuracy:0.94 loss: 1.52249 (lr:1.08)\n",
      "4040: accuracy:0.96 loss: 1.50585 (lr:1.08)\n",
      "4060: accuracy:0.98 loss: 1.4801 (lr:1.08)\n",
      "4080: accuracy:0.97 loss: 1.49448 (lr:1.08)\n",
      "4100: accuracy:0.98 loss: 1.48896 (lr:1.08)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9689 test loss: 1.49568\n",
      "4120: accuracy:0.98 loss: 1.48434 (lr:1.08)\n",
      "4140: accuracy:0.94 loss: 1.52615 (lr:1.08)\n",
      "4160: accuracy:0.97 loss: 1.49735 (lr:1.08)\n",
      "4180: accuracy:0.98 loss: 1.4835 (lr:1.08)\n",
      "4200: accuracy:0.98 loss: 1.48769 (lr:1.08)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9691 test loss: 1.49414\n",
      "4220: accuracy:0.98 loss: 1.48391 (lr:1.08)\n",
      "4240: accuracy:0.97 loss: 1.50176 (lr:1.08)\n",
      "4260: accuracy:0.99 loss: 1.47839 (lr:1.08)\n",
      "4280: accuracy:0.98 loss: 1.48461 (lr:1.08)\n",
      "4300: accuracy:0.95 loss: 1.50861 (lr:1.08)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9662 test loss: 1.4972\n",
      "4320: accuracy:0.97 loss: 1.49298 (lr:1.08)\n",
      "4340: accuracy:0.96 loss: 1.49826 (lr:1.08)\n",
      "4360: accuracy:0.97 loss: 1.48965 (lr:1.08)\n",
      "4380: accuracy:0.97 loss: 1.49213 (lr:1.08)\n",
      "4400: accuracy:0.96 loss: 1.50124 (lr:1.08)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9696 test loss: 1.49488\n",
      "4420: accuracy:0.97 loss: 1.49675 (lr:1.08)\n",
      "4440: accuracy:0.99 loss: 1.47359 (lr:1.08)\n",
      "4460: accuracy:0.99 loss: 1.47633 (lr:1.08)\n",
      "4480: accuracy:0.98 loss: 1.48299 (lr:1.08)\n",
      "4500: accuracy:0.96 loss: 1.50712 (lr:1.08)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9693 test loss: 1.49411\n",
      "4520: accuracy:0.97 loss: 1.4934 (lr:1.08)\n",
      "4540: accuracy:0.99 loss: 1.47041 (lr:1.08)\n",
      "4560: accuracy:0.99 loss: 1.474 (lr:1.08)\n",
      "4580: accuracy:0.98 loss: 1.48841 (lr:1.08)\n",
      "4600: accuracy:0.94 loss: 1.53033 (lr:1.08)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9681 test loss: 1.49479\n",
      "4620: accuracy:0.98 loss: 1.48547 (lr:1.08)\n",
      "4640: accuracy:0.96 loss: 1.50698 (lr:1.08)\n",
      "4660: accuracy:0.99 loss: 1.47739 (lr:1.08)\n",
      "4680: accuracy:0.95 loss: 1.52035 (lr:1.08)\n",
      "4700: accuracy:0.98 loss: 1.48929 (lr:1.08)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.97 test loss: 1.4936\n",
      "4720: accuracy:0.98 loss: 1.49264 (lr:1.08)\n",
      "4740: accuracy:0.95 loss: 1.51083 (lr:1.08)\n",
      "4760: accuracy:0.99 loss: 1.47215 (lr:1.08)\n",
      "4780: accuracy:0.96 loss: 1.49923 (lr:1.08)\n",
      "4800: accuracy:0.98 loss: 1.48855 (lr:1.08)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9673 test loss: 1.49623\n",
      "4820: accuracy:0.96 loss: 1.50446 (lr:1.08)\n",
      "4840: accuracy:0.97 loss: 1.48825 (lr:1.08)\n",
      "4860: accuracy:0.98 loss: 1.48125 (lr:1.08)\n",
      "4880: accuracy:0.95 loss: 1.50707 (lr:1.08)\n",
      "4900: accuracy:0.98 loss: 1.48141 (lr:1.08)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9705 test loss: 1.49297\n",
      "4920: accuracy:0.97 loss: 1.49835 (lr:1.08)\n",
      "4940: accuracy:0.98 loss: 1.48596 (lr:1.08)\n",
      "4960: accuracy:0.99 loss: 1.476 (lr:1.08)\n",
      "4980: accuracy:0.99 loss: 1.47543 (lr:1.08)\n",
      "5000: accuracy:0.98 loss: 1.48376 (lr:1.08)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9689 test loss: 1.49453\n",
      "5020: accuracy:0.99 loss: 1.47438 (lr:1.08)\n",
      "5040: accuracy:0.97 loss: 1.48871 (lr:1.08)\n",
      "5060: accuracy:0.92 loss: 1.54012 (lr:1.08)\n",
      "5080: accuracy:0.95 loss: 1.51383 (lr:1.08)\n",
      "5100: accuracy:0.94 loss: 1.51823 (lr:1.08)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9711 test loss: 1.49258\n",
      "5120: accuracy:0.96 loss: 1.50167 (lr:1.08)\n",
      "5140: accuracy:0.99 loss: 1.47609 (lr:1.08)\n",
      "5160: accuracy:0.99 loss: 1.47403 (lr:1.08)\n",
      "5180: accuracy:0.96 loss: 1.49905 (lr:1.08)\n",
      "5200: accuracy:0.97 loss: 1.48614 (lr:1.08)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9675 test loss: 1.49554\n",
      "5220: accuracy:0.98 loss: 1.4768 (lr:1.08)\n",
      "5240: accuracy:0.98 loss: 1.48234 (lr:1.08)\n",
      "5260: accuracy:0.94 loss: 1.51741 (lr:1.08)\n",
      "5280: accuracy:0.97 loss: 1.48681 (lr:1.08)\n",
      "5300: accuracy:0.99 loss: 1.47689 (lr:1.08)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9703 test loss: 1.49306\n",
      "5320: accuracy:0.99 loss: 1.47244 (lr:1.08)\n",
      "5340: accuracy:0.99 loss: 1.4819 (lr:1.08)\n",
      "5360: accuracy:0.98 loss: 1.48828 (lr:1.08)\n",
      "5380: accuracy:0.97 loss: 1.49117 (lr:1.08)\n",
      "5400: accuracy:0.98 loss: 1.48394 (lr:1.08)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9702 test loss: 1.49403\n",
      "5420: accuracy:0.96 loss: 1.49942 (lr:1.08)\n",
      "5440: accuracy:1.0 loss: 1.4658 (lr:1.08)\n",
      "5460: accuracy:0.98 loss: 1.48567 (lr:1.08)\n",
      "5480: accuracy:0.97 loss: 1.49722 (lr:1.08)\n",
      "5500: accuracy:0.99 loss: 1.47095 (lr:1.08)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9716 test loss: 1.49203\n",
      "5520: accuracy:0.96 loss: 1.50145 (lr:1.08)\n",
      "5540: accuracy:1.0 loss: 1.47341 (lr:1.08)\n",
      "5560: accuracy:0.95 loss: 1.50527 (lr:1.08)\n",
      "5580: accuracy:0.97 loss: 1.48657 (lr:1.08)\n",
      "5600: accuracy:0.99 loss: 1.47067 (lr:1.08)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9716 test loss: 1.49204\n",
      "5620: accuracy:0.96 loss: 1.49865 (lr:1.08)\n",
      "5640: accuracy:1.0 loss: 1.46368 (lr:1.08)\n",
      "5660: accuracy:1.0 loss: 1.46799 (lr:1.08)\n",
      "5680: accuracy:1.0 loss: 1.46387 (lr:1.08)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5700: accuracy:1.0 loss: 1.46652 (lr:1.08)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9718 test loss: 1.49228\n",
      "5720: accuracy:0.98 loss: 1.47744 (lr:1.08)\n",
      "5740: accuracy:0.99 loss: 1.47441 (lr:1.08)\n",
      "5760: accuracy:0.94 loss: 1.51416 (lr:1.08)\n",
      "5780: accuracy:1.0 loss: 1.47033 (lr:1.08)\n",
      "5800: accuracy:0.99 loss: 1.471 (lr:1.08)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9724 test loss: 1.49095\n",
      "5820: accuracy:0.97 loss: 1.4882 (lr:1.08)\n",
      "5840: accuracy:0.97 loss: 1.49982 (lr:1.08)\n",
      "5860: accuracy:0.99 loss: 1.47969 (lr:1.08)\n",
      "5880: accuracy:0.99 loss: 1.47196 (lr:1.08)\n",
      "5900: accuracy:0.95 loss: 1.51385 (lr:1.08)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9725 test loss: 1.4905\n",
      "5920: accuracy:0.98 loss: 1.48201 (lr:1.08)\n",
      "5940: accuracy:0.96 loss: 1.5104 (lr:1.08)\n",
      "5960: accuracy:0.98 loss: 1.48469 (lr:1.08)\n",
      "5980: accuracy:0.99 loss: 1.47442 (lr:1.08)\n",
      "6000: accuracy:0.96 loss: 1.51102 (lr:1.08)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9732 test loss: 1.49034\n",
      "6020: accuracy:1.0 loss: 1.46199 (lr:1.08)\n",
      "6040: accuracy:0.99 loss: 1.47249 (lr:1.08)\n",
      "6060: accuracy:0.98 loss: 1.48074 (lr:1.08)\n",
      "6080: accuracy:0.99 loss: 1.47757 (lr:1.08)\n",
      "6100: accuracy:0.95 loss: 1.51783 (lr:1.08)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9726 test loss: 1.49054\n",
      "6120: accuracy:0.97 loss: 1.48177 (lr:1.08)\n",
      "6140: accuracy:0.99 loss: 1.47825 (lr:1.08)\n",
      "6160: accuracy:0.98 loss: 1.48044 (lr:1.08)\n",
      "6180: accuracy:0.98 loss: 1.48402 (lr:1.08)\n",
      "6200: accuracy:0.97 loss: 1.49585 (lr:1.08)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9716 test loss: 1.49151\n",
      "6220: accuracy:0.97 loss: 1.49227 (lr:1.08)\n",
      "6240: accuracy:0.98 loss: 1.48206 (lr:1.08)\n",
      "6260: accuracy:0.96 loss: 1.5054 (lr:1.08)\n",
      "6280: accuracy:1.0 loss: 1.46722 (lr:1.08)\n",
      "6300: accuracy:1.0 loss: 1.46703 (lr:1.08)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9741 test loss: 1.48921\n",
      "6320: accuracy:0.96 loss: 1.5076 (lr:1.08)\n",
      "6340: accuracy:0.99 loss: 1.47593 (lr:1.08)\n",
      "6360: accuracy:0.99 loss: 1.47808 (lr:1.08)\n",
      "6380: accuracy:0.97 loss: 1.49266 (lr:1.08)\n",
      "6400: accuracy:0.99 loss: 1.47199 (lr:1.08)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9735 test loss: 1.48957\n",
      "6420: accuracy:0.97 loss: 1.49232 (lr:1.08)\n",
      "6440: accuracy:0.99 loss: 1.47344 (lr:1.08)\n",
      "6460: accuracy:0.97 loss: 1.49231 (lr:1.08)\n",
      "6480: accuracy:0.96 loss: 1.50257 (lr:1.08)\n",
      "6500: accuracy:0.99 loss: 1.47636 (lr:1.08)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.974 test loss: 1.48952\n",
      "6520: accuracy:0.97 loss: 1.49133 (lr:1.08)\n",
      "6540: accuracy:0.96 loss: 1.50715 (lr:1.08)\n",
      "6560: accuracy:1.0 loss: 1.46727 (lr:1.08)\n",
      "6580: accuracy:1.0 loss: 1.46162 (lr:1.08)\n",
      "6600: accuracy:0.98 loss: 1.48733 (lr:1.08)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9735 test loss: 1.49013\n",
      "6620: accuracy:1.0 loss: 1.46207 (lr:1.08)\n",
      "6640: accuracy:0.97 loss: 1.48598 (lr:1.08)\n",
      "6660: accuracy:0.97 loss: 1.48298 (lr:1.08)\n",
      "6680: accuracy:1.0 loss: 1.46796 (lr:1.08)\n",
      "6700: accuracy:0.97 loss: 1.48698 (lr:1.08)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9741 test loss: 1.48959\n",
      "6720: accuracy:0.98 loss: 1.48675 (lr:1.08)\n",
      "6740: accuracy:0.98 loss: 1.48252 (lr:1.08)\n",
      "6760: accuracy:0.97 loss: 1.48788 (lr:1.08)\n",
      "6780: accuracy:0.98 loss: 1.48831 (lr:1.08)\n",
      "6800: accuracy:0.99 loss: 1.47168 (lr:1.08)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9735 test loss: 1.48964\n",
      "6820: accuracy:0.98 loss: 1.48341 (lr:1.08)\n",
      "6840: accuracy:0.98 loss: 1.47911 (lr:1.08)\n",
      "6860: accuracy:0.99 loss: 1.47426 (lr:1.08)\n",
      "6880: accuracy:0.96 loss: 1.50217 (lr:1.08)\n",
      "6900: accuracy:0.99 loss: 1.47362 (lr:1.08)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9753 test loss: 1.48843\n",
      "6920: accuracy:0.99 loss: 1.47403 (lr:1.08)\n",
      "6940: accuracy:0.99 loss: 1.47878 (lr:1.08)\n",
      "6960: accuracy:0.98 loss: 1.48232 (lr:1.08)\n",
      "6980: accuracy:0.99 loss: 1.47147 (lr:1.08)\n",
      "7000: accuracy:1.0 loss: 1.46738 (lr:1.08)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.972 test loss: 1.49037\n",
      "7020: accuracy:1.0 loss: 1.46375 (lr:1.08)\n",
      "7040: accuracy:0.99 loss: 1.4729 (lr:1.08)\n",
      "7060: accuracy:0.99 loss: 1.47663 (lr:1.08)\n",
      "7080: accuracy:0.99 loss: 1.47632 (lr:1.08)\n",
      "7100: accuracy:0.98 loss: 1.48362 (lr:1.08)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.973 test loss: 1.48954\n",
      "7120: accuracy:1.0 loss: 1.46172 (lr:1.08)\n",
      "7140: accuracy:0.97 loss: 1.49329 (lr:1.08)\n",
      "7160: accuracy:0.98 loss: 1.47993 (lr:1.08)\n",
      "7180: accuracy:0.97 loss: 1.48957 (lr:1.08)\n",
      "7200: accuracy:0.97 loss: 1.48934 (lr:1.08)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9729 test loss: 1.48983\n",
      "7220: accuracy:0.97 loss: 1.49039 (lr:1.08)\n",
      "7240: accuracy:1.0 loss: 1.46519 (lr:1.08)\n",
      "7260: accuracy:0.98 loss: 1.48376 (lr:1.08)\n",
      "7280: accuracy:0.98 loss: 1.48284 (lr:1.08)\n",
      "7300: accuracy:0.97 loss: 1.49241 (lr:1.08)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9738 test loss: 1.48918\n",
      "7320: accuracy:0.97 loss: 1.49289 (lr:1.08)\n",
      "7340: accuracy:1.0 loss: 1.46476 (lr:1.08)\n",
      "7360: accuracy:0.99 loss: 1.47407 (lr:1.08)\n",
      "7380: accuracy:0.95 loss: 1.50197 (lr:1.08)\n",
      "7400: accuracy:0.98 loss: 1.48717 (lr:1.08)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.971 test loss: 1.49185\n",
      "7420: accuracy:0.97 loss: 1.49198 (lr:1.08)\n",
      "7440: accuracy:0.99 loss: 1.47155 (lr:1.08)\n",
      "7460: accuracy:1.0 loss: 1.46832 (lr:1.08)\n",
      "7480: accuracy:0.98 loss: 1.48067 (lr:1.08)\n",
      "7500: accuracy:0.98 loss: 1.47992 (lr:1.08)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9732 test loss: 1.48859\n",
      "7520: accuracy:0.99 loss: 1.47315 (lr:1.08)\n",
      "7540: accuracy:0.99 loss: 1.47257 (lr:1.08)\n",
      "7560: accuracy:0.99 loss: 1.4724 (lr:1.08)\n",
      "7580: accuracy:0.99 loss: 1.47035 (lr:1.08)\n",
      "7600: accuracy:0.98 loss: 1.48376 (lr:1.08)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9724 test loss: 1.49047\n",
      "7620: accuracy:1.0 loss: 1.46578 (lr:1.08)\n",
      "7640: accuracy:0.99 loss: 1.47269 (lr:1.08)\n",
      "7660: accuracy:0.96 loss: 1.50178 (lr:1.08)\n",
      "7680: accuracy:0.97 loss: 1.49843 (lr:1.08)\n",
      "7700: accuracy:0.99 loss: 1.47239 (lr:1.08)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9743 test loss: 1.48835\n",
      "7720: accuracy:0.97 loss: 1.4914 (lr:1.08)\n",
      "7740: accuracy:0.98 loss: 1.48252 (lr:1.08)\n",
      "7760: accuracy:0.99 loss: 1.46767 (lr:1.08)\n",
      "7780: accuracy:0.99 loss: 1.47816 (lr:1.08)\n",
      "7800: accuracy:0.99 loss: 1.47118 (lr:1.08)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9748 test loss: 1.48801\n",
      "7820: accuracy:0.99 loss: 1.47501 (lr:1.08)\n",
      "7840: accuracy:0.99 loss: 1.47325 (lr:1.08)\n",
      "7860: accuracy:1.0 loss: 1.46218 (lr:1.08)\n",
      "7880: accuracy:0.97 loss: 1.49449 (lr:1.08)\n",
      "7900: accuracy:0.97 loss: 1.49158 (lr:1.08)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9748 test loss: 1.48765\n",
      "7920: accuracy:1.0 loss: 1.46979 (lr:1.08)\n",
      "7940: accuracy:0.96 loss: 1.49796 (lr:1.08)\n",
      "7960: accuracy:0.99 loss: 1.47159 (lr:1.08)\n",
      "7980: accuracy:1.0 loss: 1.46318 (lr:1.08)\n",
      "8000: accuracy:0.98 loss: 1.48304 (lr:1.08)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9749 test loss: 1.48818\n",
      "8020: accuracy:0.98 loss: 1.47924 (lr:1.08)\n",
      "8040: accuracy:0.97 loss: 1.4837 (lr:1.08)\n",
      "8060: accuracy:1.0 loss: 1.46168 (lr:1.08)\n",
      "8080: accuracy:0.99 loss: 1.47179 (lr:1.08)\n",
      "8100: accuracy:0.99 loss: 1.47055 (lr:1.08)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9754 test loss: 1.48688\n",
      "8120: accuracy:1.0 loss: 1.46588 (lr:1.08)\n",
      "8140: accuracy:0.98 loss: 1.48443 (lr:1.08)\n",
      "8160: accuracy:0.97 loss: 1.49268 (lr:1.08)\n",
      "8180: accuracy:0.98 loss: 1.48292 (lr:1.08)\n",
      "8200: accuracy:1.0 loss: 1.46318 (lr:1.08)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9747 test loss: 1.48843\n",
      "8220: accuracy:1.0 loss: 1.46125 (lr:1.08)\n",
      "8240: accuracy:0.98 loss: 1.48273 (lr:1.08)\n",
      "8260: accuracy:1.0 loss: 1.46355 (lr:1.08)\n",
      "8280: accuracy:0.98 loss: 1.48436 (lr:1.08)\n",
      "8300: accuracy:0.97 loss: 1.49409 (lr:1.08)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9747 test loss: 1.48814\n",
      "8320: accuracy:0.99 loss: 1.47486 (lr:1.08)\n",
      "8340: accuracy:0.98 loss: 1.48237 (lr:1.08)\n",
      "8360: accuracy:1.0 loss: 1.46535 (lr:1.08)\n",
      "8380: accuracy:0.99 loss: 1.47286 (lr:1.08)\n",
      "8400: accuracy:0.99 loss: 1.47483 (lr:1.08)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9721 test loss: 1.49035\n",
      "8420: accuracy:0.97 loss: 1.49126 (lr:1.08)\n",
      "8440: accuracy:0.97 loss: 1.49311 (lr:1.08)\n",
      "8460: accuracy:0.98 loss: 1.486 (lr:1.08)\n",
      "8480: accuracy:0.98 loss: 1.4772 (lr:1.08)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500: accuracy:0.98 loss: 1.48567 (lr:1.08)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9771 test loss: 1.48606\n",
      "8520: accuracy:0.99 loss: 1.47381 (lr:1.08)\n",
      "8540: accuracy:1.0 loss: 1.46565 (lr:1.08)\n",
      "8560: accuracy:0.97 loss: 1.49336 (lr:1.08)\n",
      "8580: accuracy:0.96 loss: 1.49789 (lr:1.08)\n",
      "8600: accuracy:0.99 loss: 1.4735 (lr:1.08)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9761 test loss: 1.48692\n",
      "8620: accuracy:0.99 loss: 1.47262 (lr:1.08)\n",
      "8640: accuracy:0.99 loss: 1.46942 (lr:1.08)\n",
      "8660: accuracy:0.99 loss: 1.47328 (lr:1.08)\n",
      "8680: accuracy:1.0 loss: 1.46271 (lr:1.08)\n",
      "8700: accuracy:0.99 loss: 1.47527 (lr:1.08)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.975 test loss: 1.48729\n",
      "8720: accuracy:0.95 loss: 1.51481 (lr:1.08)\n",
      "8740: accuracy:0.99 loss: 1.47279 (lr:1.08)\n",
      "8760: accuracy:1.0 loss: 1.46229 (lr:1.08)\n",
      "8780: accuracy:0.99 loss: 1.47431 (lr:1.08)\n",
      "8800: accuracy:0.99 loss: 1.47079 (lr:1.08)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9742 test loss: 1.488\n",
      "8820: accuracy:0.98 loss: 1.47783 (lr:1.08)\n",
      "8840: accuracy:1.0 loss: 1.4655 (lr:1.08)\n",
      "8860: accuracy:0.97 loss: 1.49169 (lr:1.08)\n",
      "8880: accuracy:1.0 loss: 1.46231 (lr:1.08)\n",
      "8900: accuracy:1.0 loss: 1.46894 (lr:1.08)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9752 test loss: 1.48728\n",
      "8920: accuracy:0.98 loss: 1.48247 (lr:1.08)\n",
      "8940: accuracy:0.99 loss: 1.47449 (lr:1.08)\n",
      "8960: accuracy:0.99 loss: 1.46839 (lr:1.08)\n",
      "8980: accuracy:0.99 loss: 1.48533 (lr:1.08)\n",
      "9000: accuracy:1.0 loss: 1.47181 (lr:1.08)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9743 test loss: 1.48812\n",
      "9020: accuracy:0.98 loss: 1.4887 (lr:1.08)\n",
      "9040: accuracy:0.99 loss: 1.47161 (lr:1.08)\n",
      "9060: accuracy:0.99 loss: 1.47305 (lr:1.08)\n",
      "9080: accuracy:0.99 loss: 1.47279 (lr:1.08)\n",
      "9100: accuracy:0.97 loss: 1.49626 (lr:1.08)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9738 test loss: 1.48843\n",
      "9120: accuracy:0.99 loss: 1.47311 (lr:1.08)\n",
      "9140: accuracy:0.98 loss: 1.48325 (lr:1.08)\n",
      "9160: accuracy:0.98 loss: 1.48223 (lr:1.08)\n",
      "9180: accuracy:0.97 loss: 1.49247 (lr:1.08)\n",
      "9200: accuracy:0.97 loss: 1.48709 (lr:1.08)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.976 test loss: 1.48705\n",
      "9220: accuracy:1.0 loss: 1.46552 (lr:1.08)\n",
      "9240: accuracy:0.97 loss: 1.49003 (lr:1.08)\n",
      "9260: accuracy:1.0 loss: 1.46493 (lr:1.08)\n",
      "9280: accuracy:0.98 loss: 1.47811 (lr:1.08)\n",
      "9300: accuracy:0.98 loss: 1.48268 (lr:1.08)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9751 test loss: 1.48725\n",
      "9320: accuracy:0.96 loss: 1.50161 (lr:1.08)\n",
      "9340: accuracy:0.98 loss: 1.48444 (lr:1.08)\n",
      "9360: accuracy:0.99 loss: 1.47169 (lr:1.08)\n",
      "9380: accuracy:0.99 loss: 1.47439 (lr:1.08)\n",
      "9400: accuracy:1.0 loss: 1.46341 (lr:1.08)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.976 test loss: 1.48699\n",
      "9420: accuracy:0.94 loss: 1.52112 (lr:1.08)\n",
      "9440: accuracy:0.96 loss: 1.49675 (lr:1.08)\n",
      "9460: accuracy:1.0 loss: 1.46608 (lr:1.08)\n",
      "9480: accuracy:0.99 loss: 1.47317 (lr:1.08)\n",
      "9500: accuracy:0.99 loss: 1.47351 (lr:1.08)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9753 test loss: 1.48696\n",
      "9520: accuracy:0.97 loss: 1.4869 (lr:1.08)\n",
      "9540: accuracy:0.96 loss: 1.49766 (lr:1.08)\n",
      "9560: accuracy:0.97 loss: 1.49867 (lr:1.08)\n",
      "9580: accuracy:0.98 loss: 1.48599 (lr:1.08)\n",
      "9600: accuracy:0.99 loss: 1.47242 (lr:1.08)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9741 test loss: 1.48824\n",
      "9620: accuracy:0.99 loss: 1.47022 (lr:1.08)\n",
      "9640: accuracy:1.0 loss: 1.46419 (lr:1.08)\n",
      "9660: accuracy:0.97 loss: 1.4932 (lr:1.08)\n",
      "9680: accuracy:0.99 loss: 1.47436 (lr:1.08)\n",
      "9700: accuracy:0.99 loss: 1.47283 (lr:1.08)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9748 test loss: 1.48836\n",
      "9720: accuracy:1.0 loss: 1.46571 (lr:1.08)\n",
      "9740: accuracy:0.98 loss: 1.4796 (lr:1.08)\n",
      "9760: accuracy:0.98 loss: 1.48267 (lr:1.08)\n",
      "9780: accuracy:0.98 loss: 1.4888 (lr:1.08)\n",
      "9800: accuracy:0.99 loss: 1.47294 (lr:1.08)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.976 test loss: 1.48681\n",
      "9820: accuracy:1.0 loss: 1.47011 (lr:1.08)\n",
      "9840: accuracy:1.0 loss: 1.46169 (lr:1.08)\n",
      "9860: accuracy:0.96 loss: 1.50048 (lr:1.08)\n",
      "9880: accuracy:0.98 loss: 1.48121 (lr:1.08)\n",
      "9900: accuracy:0.98 loss: 1.48308 (lr:1.08)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9745 test loss: 1.48761\n",
      "9920: accuracy:0.98 loss: 1.48144 (lr:1.08)\n",
      "9940: accuracy:1.0 loss: 1.46362 (lr:1.08)\n",
      "9960: accuracy:1.0 loss: 1.46176 (lr:1.08)\n",
      "9980: accuracy:0.98 loss: 1.4827 (lr:1.08)\n",
      "10000: accuracy:1.0 loss: 1.46382 (lr:1.08)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9728 test loss: 1.48847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [01:14<01:57, 39.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.11 loss: 2.3093 (lr:9.98)\n",
      "0: ********* epoch 1 ********* test accuracy:0.0996 test loss: 2.30702\n",
      "20: accuracy:0.15 loss: 2.296 (lr:9.98)\n",
      "40: accuracy:0.4 loss: 2.061 (lr:9.98)\n",
      "60: accuracy:0.56 loss: 1.88525 (lr:9.98)\n",
      "80: accuracy:0.58 loss: 1.86193 (lr:9.98)\n",
      "100: accuracy:0.75 loss: 1.71753 (lr:9.98)\n",
      "100: ********* epoch 1 ********* test accuracy:0.6944 test loss: 1.76775\n",
      "120: accuracy:0.74 loss: 1.72046 (lr:9.98)\n",
      "140: accuracy:0.8 loss: 1.66472 (lr:9.98)\n",
      "160: accuracy:0.8 loss: 1.66468 (lr:9.98)\n",
      "180: accuracy:0.82 loss: 1.6435 (lr:9.98)\n",
      "200: accuracy:0.81 loss: 1.63755 (lr:9.98)\n",
      "200: ********* epoch 1 ********* test accuracy:0.8119 test loss: 1.64892\n",
      "220: accuracy:0.84 loss: 1.61594 (lr:9.98)\n",
      "240: accuracy:0.75 loss: 1.70479 (lr:9.98)\n",
      "260: accuracy:0.7 loss: 1.76216 (lr:9.98)\n",
      "280: accuracy:0.81 loss: 1.64997 (lr:9.98)\n",
      "300: accuracy:0.83 loss: 1.64385 (lr:9.98)\n",
      "300: ********* epoch 1 ********* test accuracy:0.8232 test loss: 1.63788\n",
      "320: accuracy:0.88 loss: 1.58074 (lr:9.98)\n",
      "340: accuracy:0.83 loss: 1.62652 (lr:9.98)\n",
      "360: accuracy:0.86 loss: 1.60782 (lr:9.98)\n",
      "380: accuracy:0.76 loss: 1.70161 (lr:9.98)\n",
      "400: accuracy:0.9 loss: 1.56132 (lr:9.98)\n",
      "400: ********* epoch 1 ********* test accuracy:0.8233 test loss: 1.63813\n",
      "420: accuracy:0.82 loss: 1.63663 (lr:9.98)\n",
      "440: accuracy:0.82 loss: 1.64628 (lr:9.98)\n",
      "460: accuracy:0.88 loss: 1.5815 (lr:9.98)\n",
      "480: accuracy:0.87 loss: 1.58894 (lr:9.98)\n",
      "500: accuracy:0.79 loss: 1.66923 (lr:9.98)\n",
      "500: ********* epoch 1 ********* test accuracy:0.8298 test loss: 1.63041\n",
      "520: accuracy:0.85 loss: 1.60959 (lr:9.98)\n",
      "540: accuracy:0.84 loss: 1.62816 (lr:9.98)\n",
      "560: accuracy:0.81 loss: 1.64803 (lr:9.98)\n",
      "580: accuracy:0.83 loss: 1.62573 (lr:9.98)\n",
      "600: accuracy:0.88 loss: 1.58322 (lr:9.98)\n",
      "600: ********* epoch 2 ********* test accuracy:0.8403 test loss: 1.62061\n",
      "620: accuracy:0.87 loss: 1.58231 (lr:9.98)\n",
      "640: accuracy:0.91 loss: 1.55305 (lr:9.98)\n",
      "660: accuracy:0.87 loss: 1.57656 (lr:9.98)\n",
      "680: accuracy:0.91 loss: 1.55097 (lr:9.98)\n",
      "700: accuracy:0.84 loss: 1.62342 (lr:9.98)\n",
      "700: ********* epoch 2 ********* test accuracy:0.8337 test loss: 1.62673\n",
      "720: accuracy:0.84 loss: 1.62111 (lr:9.98)\n",
      "740: accuracy:0.79 loss: 1.66988 (lr:9.98)\n",
      "760: accuracy:0.86 loss: 1.59847 (lr:9.98)\n",
      "780: accuracy:0.76 loss: 1.69 (lr:9.98)\n",
      "800: accuracy:0.86 loss: 1.60138 (lr:9.98)\n",
      "800: ********* epoch 2 ********* test accuracy:0.8454 test loss: 1.61553\n",
      "820: accuracy:0.87 loss: 1.58293 (lr:9.98)\n",
      "840: accuracy:0.88 loss: 1.59003 (lr:9.98)\n",
      "860: accuracy:0.92 loss: 1.54337 (lr:9.98)\n",
      "880: accuracy:0.85 loss: 1.61442 (lr:9.98)\n",
      "900: accuracy:0.8 loss: 1.65989 (lr:9.98)\n",
      "900: ********* epoch 2 ********* test accuracy:0.8405 test loss: 1.61973\n",
      "920: accuracy:0.89 loss: 1.57125 (lr:9.98)\n",
      "940: accuracy:0.81 loss: 1.65118 (lr:9.98)\n",
      "960: accuracy:0.83 loss: 1.62922 (lr:9.98)\n",
      "980: accuracy:0.89 loss: 1.56662 (lr:9.98)\n",
      "1000: accuracy:0.79 loss: 1.67471 (lr:9.98)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.8437 test loss: 1.61717\n",
      "1020: accuracy:0.82 loss: 1.63101 (lr:9.98)\n",
      "1040: accuracy:0.86 loss: 1.6016 (lr:9.98)\n",
      "1060: accuracy:0.84 loss: 1.62097 (lr:9.98)\n",
      "1080: accuracy:0.82 loss: 1.63552 (lr:9.98)\n",
      "1100: accuracy:0.88 loss: 1.57845 (lr:9.98)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.8439 test loss: 1.61643\n",
      "1120: accuracy:0.83 loss: 1.63014 (lr:9.98)\n",
      "1140: accuracy:0.87 loss: 1.59299 (lr:9.98)\n",
      "1160: accuracy:0.76 loss: 1.70213 (lr:9.98)\n",
      "1180: accuracy:0.9 loss: 1.56399 (lr:9.98)\n",
      "1200: accuracy:0.88 loss: 1.58427 (lr:9.98)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.8449 test loss: 1.61571\n",
      "1220: accuracy:0.9 loss: 1.56313 (lr:9.98)\n",
      "1240: accuracy:0.93 loss: 1.53144 (lr:9.98)\n",
      "1260: accuracy:0.88 loss: 1.58033 (lr:9.98)\n",
      "1280: accuracy:0.85 loss: 1.61251 (lr:9.98)\n",
      "1300: accuracy:0.8 loss: 1.65945 (lr:9.98)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.8445 test loss: 1.61574\n",
      "1320: accuracy:0.84 loss: 1.62481 (lr:9.98)\n",
      "1340: accuracy:0.72 loss: 1.7363 (lr:9.98)\n",
      "1360: accuracy:0.9 loss: 1.5613 (lr:9.98)\n",
      "1380: accuracy:0.83 loss: 1.63003 (lr:9.98)\n",
      "1400: accuracy:0.94 loss: 1.5176 (lr:9.98)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.8506 test loss: 1.60986\n",
      "1420: accuracy:0.85 loss: 1.61103 (lr:9.98)\n",
      "1440: accuracy:0.9 loss: 1.56106 (lr:9.98)\n",
      "1460: accuracy:0.82 loss: 1.64102 (lr:9.98)\n",
      "1480: accuracy:0.84 loss: 1.62189 (lr:9.98)\n",
      "1500: accuracy:0.85 loss: 1.60696 (lr:9.98)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.8581 test loss: 1.60281\n",
      "1520: accuracy:0.85 loss: 1.61432 (lr:9.98)\n",
      "1540: accuracy:0.84 loss: 1.62808 (lr:9.98)\n",
      "1560: accuracy:0.86 loss: 1.6018 (lr:9.98)\n",
      "1580: accuracy:0.87 loss: 1.59042 (lr:9.98)\n",
      "1600: accuracy:0.79 loss: 1.66952 (lr:9.98)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.8377 test loss: 1.62297\n",
      "1620: accuracy:0.78 loss: 1.68112 (lr:9.98)\n",
      "1640: accuracy:0.88 loss: 1.58266 (lr:9.98)\n",
      "1660: accuracy:0.8 loss: 1.6596 (lr:9.98)\n",
      "1680: accuracy:0.88 loss: 1.58039 (lr:9.98)\n",
      "1700: accuracy:0.85 loss: 1.61961 (lr:9.98)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.8519 test loss: 1.60849\n",
      "1720: accuracy:0.8 loss: 1.66059 (lr:9.98)\n",
      "1740: accuracy:0.89 loss: 1.56918 (lr:9.98)\n",
      "1760: accuracy:0.9 loss: 1.56329 (lr:9.98)\n",
      "1780: accuracy:0.84 loss: 1.62178 (lr:9.98)\n",
      "1800: accuracy:0.84 loss: 1.62116 (lr:9.98)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.8503 test loss: 1.61057\n",
      "1820: accuracy:0.87 loss: 1.59503 (lr:9.98)\n",
      "1840: accuracy:0.86 loss: 1.60039 (lr:9.98)\n",
      "1860: accuracy:0.8 loss: 1.66124 (lr:9.98)\n",
      "1880: accuracy:0.91 loss: 1.55063 (lr:9.98)\n",
      "1900: accuracy:0.85 loss: 1.60748 (lr:9.98)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.846 test loss: 1.61465\n",
      "1920: accuracy:0.82 loss: 1.64104 (lr:9.98)\n",
      "1940: accuracy:0.82 loss: 1.63832 (lr:9.98)\n",
      "1960: accuracy:0.85 loss: 1.61381 (lr:9.98)\n",
      "1980: accuracy:0.88 loss: 1.57923 (lr:9.98)\n",
      "2000: accuracy:0.91 loss: 1.54646 (lr:9.98)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.8516 test loss: 1.60906\n",
      "2020: accuracy:0.83 loss: 1.63269 (lr:9.98)\n",
      "2040: accuracy:0.84 loss: 1.62181 (lr:9.98)\n",
      "2060: accuracy:0.9 loss: 1.56449 (lr:9.98)\n",
      "2080: accuracy:0.86 loss: 1.60055 (lr:9.98)\n",
      "2100: accuracy:0.8 loss: 1.66049 (lr:9.98)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.8558 test loss: 1.60491\n",
      "2120: accuracy:0.89 loss: 1.56907 (lr:9.98)\n",
      "2140: accuracy:0.85 loss: 1.611 (lr:9.98)\n",
      "2160: accuracy:0.87 loss: 1.59032 (lr:9.98)\n",
      "2180: accuracy:0.87 loss: 1.59114 (lr:9.98)\n",
      "2200: accuracy:0.87 loss: 1.59141 (lr:9.98)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.8542 test loss: 1.60615\n",
      "2220: accuracy:0.8 loss: 1.65871 (lr:9.98)\n",
      "2240: accuracy:0.87 loss: 1.58932 (lr:9.98)\n",
      "2260: accuracy:0.85 loss: 1.61142 (lr:9.98)\n",
      "2280: accuracy:0.85 loss: 1.61106 (lr:9.98)\n",
      "2300: accuracy:0.84 loss: 1.61806 (lr:9.98)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.8564 test loss: 1.60457\n",
      "2320: accuracy:0.83 loss: 1.63108 (lr:9.98)\n",
      "2340: accuracy:0.88 loss: 1.58091 (lr:9.98)\n",
      "2360: accuracy:0.82 loss: 1.6441 (lr:9.98)\n",
      "2380: accuracy:0.82 loss: 1.64022 (lr:9.98)\n",
      "2400: accuracy:0.88 loss: 1.5808 (lr:9.98)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.8545 test loss: 1.60609\n",
      "2420: accuracy:0.86 loss: 1.60112 (lr:9.98)\n",
      "2440: accuracy:0.88 loss: 1.58123 (lr:9.98)\n",
      "2460: accuracy:0.85 loss: 1.60544 (lr:9.98)\n",
      "2480: accuracy:0.86 loss: 1.60098 (lr:9.98)\n",
      "2500: accuracy:0.9 loss: 1.56338 (lr:9.98)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.8466 test loss: 1.61437\n",
      "2520: accuracy:0.86 loss: 1.60427 (lr:9.98)\n",
      "2540: accuracy:0.87 loss: 1.58993 (lr:9.98)\n",
      "2560: accuracy:0.79 loss: 1.66676 (lr:9.98)\n",
      "2580: accuracy:0.85 loss: 1.60875 (lr:9.98)\n",
      "2600: accuracy:0.85 loss: 1.61092 (lr:9.98)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.8535 test loss: 1.60734\n",
      "2620: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "2640: accuracy:0.86 loss: 1.60111 (lr:9.98)\n",
      "2660: accuracy:0.88 loss: 1.58132 (lr:9.98)\n",
      "2680: accuracy:0.86 loss: 1.6009 (lr:9.98)\n",
      "2700: accuracy:0.86 loss: 1.5992 (lr:9.98)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.8405 test loss: 1.62033\n",
      "2720: accuracy:0.92 loss: 1.54259 (lr:9.98)\n",
      "2740: accuracy:0.78 loss: 1.67902 (lr:9.98)\n",
      "2760: accuracy:0.82 loss: 1.63948 (lr:9.98)\n",
      "2780: accuracy:0.85 loss: 1.61355 (lr:9.98)\n",
      "2800: accuracy:0.88 loss: 1.58106 (lr:9.98)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.8518 test loss: 1.60925\n",
      "2820: accuracy:0.86 loss: 1.60144 (lr:9.98)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2840: accuracy:0.71 loss: 1.75066 (lr:9.98)\n",
      "2860: accuracy:0.83 loss: 1.63097 (lr:9.98)\n",
      "2880: accuracy:0.89 loss: 1.57095 (lr:9.98)\n",
      "2900: accuracy:0.82 loss: 1.64079 (lr:9.98)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.8503 test loss: 1.61044\n",
      "2920: accuracy:0.84 loss: 1.62086 (lr:9.98)\n",
      "2940: accuracy:0.85 loss: 1.6111 (lr:9.98)\n",
      "2960: accuracy:0.85 loss: 1.61026 (lr:9.98)\n",
      "2980: accuracy:0.88 loss: 1.58065 (lr:9.98)\n",
      "3000: accuracy:0.87 loss: 1.59106 (lr:9.98)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.8443 test loss: 1.61662\n",
      "3020: accuracy:0.84 loss: 1.61928 (lr:9.98)\n",
      "3040: accuracy:0.88 loss: 1.58114 (lr:9.98)\n",
      "3060: accuracy:0.82 loss: 1.64029 (lr:9.98)\n",
      "3080: accuracy:0.83 loss: 1.63448 (lr:9.98)\n",
      "3100: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.855 test loss: 1.60598\n",
      "3120: accuracy:0.9 loss: 1.56059 (lr:9.98)\n",
      "3140: accuracy:0.88 loss: 1.58077 (lr:9.98)\n",
      "3160: accuracy:0.86 loss: 1.6 (lr:9.98)\n",
      "3180: accuracy:0.9 loss: 1.56114 (lr:9.98)\n",
      "3200: accuracy:0.86 loss: 1.59898 (lr:9.98)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.8451 test loss: 1.61555\n",
      "3220: accuracy:0.83 loss: 1.63112 (lr:9.98)\n",
      "3240: accuracy:0.9 loss: 1.56116 (lr:9.98)\n",
      "3260: accuracy:0.86 loss: 1.60005 (lr:9.98)\n",
      "3280: accuracy:0.81 loss: 1.65115 (lr:9.98)\n",
      "3300: accuracy:0.91 loss: 1.55125 (lr:9.98)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.8592 test loss: 1.60161\n",
      "3320: accuracy:0.88 loss: 1.58103 (lr:9.98)\n",
      "3340: accuracy:0.85 loss: 1.61199 (lr:9.98)\n",
      "3360: accuracy:0.84 loss: 1.62102 (lr:9.98)\n",
      "3380: accuracy:0.89 loss: 1.5712 (lr:9.98)\n",
      "3400: accuracy:0.89 loss: 1.57111 (lr:9.98)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.8549 test loss: 1.60584\n",
      "3420: accuracy:0.89 loss: 1.57795 (lr:9.98)\n",
      "3440: accuracy:0.87 loss: 1.59446 (lr:9.98)\n",
      "3460: accuracy:0.85 loss: 1.61269 (lr:9.98)\n",
      "3480: accuracy:0.88 loss: 1.58113 (lr:9.98)\n",
      "3500: accuracy:0.82 loss: 1.64074 (lr:9.98)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.8594 test loss: 1.6017\n",
      "3520: accuracy:0.84 loss: 1.6157 (lr:9.98)\n",
      "3540: accuracy:0.82 loss: 1.6412 (lr:9.98)\n",
      "3560: accuracy:0.9 loss: 1.56072 (lr:9.98)\n",
      "3580: accuracy:0.87 loss: 1.59095 (lr:9.98)\n",
      "3600: accuracy:0.83 loss: 1.63093 (lr:9.98)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.8555 test loss: 1.60534\n",
      "3620: accuracy:0.91 loss: 1.55159 (lr:9.98)\n",
      "3640: accuracy:0.86 loss: 1.6035 (lr:9.98)\n",
      "3660: accuracy:0.81 loss: 1.65107 (lr:9.98)\n",
      "3680: accuracy:0.79 loss: 1.67105 (lr:9.98)\n",
      "3700: accuracy:0.88 loss: 1.57377 (lr:9.98)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.852 test loss: 1.60894\n",
      "3720: accuracy:0.88 loss: 1.58113 (lr:9.98)\n",
      "3740: accuracy:0.82 loss: 1.64097 (lr:9.98)\n",
      "3760: accuracy:0.86 loss: 1.60079 (lr:9.98)\n",
      "3780: accuracy:0.85 loss: 1.6108 (lr:9.98)\n",
      "3800: accuracy:0.88 loss: 1.58086 (lr:9.98)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.8532 test loss: 1.60793\n",
      "3820: accuracy:0.77 loss: 1.69074 (lr:9.98)\n",
      "3840: accuracy:0.81 loss: 1.65108 (lr:9.98)\n",
      "3860: accuracy:0.9 loss: 1.56473 (lr:9.98)\n",
      "3880: accuracy:0.87 loss: 1.5911 (lr:9.98)\n",
      "3900: accuracy:0.83 loss: 1.63091 (lr:9.98)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.8531 test loss: 1.60813\n",
      "3920: accuracy:0.86 loss: 1.60047 (lr:9.98)\n",
      "3940: accuracy:0.92 loss: 1.53999 (lr:9.98)\n",
      "3960: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "3980: accuracy:0.89 loss: 1.56977 (lr:9.98)\n",
      "4000: accuracy:0.82 loss: 1.64124 (lr:9.98)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.8464 test loss: 1.61433\n",
      "4020: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "4040: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "4060: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "4080: accuracy:0.84 loss: 1.62301 (lr:9.98)\n",
      "4100: accuracy:0.87 loss: 1.5887 (lr:9.98)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.8589 test loss: 1.60191\n",
      "4120: accuracy:0.88 loss: 1.57897 (lr:9.98)\n",
      "4140: accuracy:0.87 loss: 1.59115 (lr:9.98)\n",
      "4160: accuracy:0.86 loss: 1.60084 (lr:9.98)\n",
      "4180: accuracy:0.93 loss: 1.53115 (lr:9.98)\n",
      "4200: accuracy:0.83 loss: 1.63077 (lr:9.98)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.86 test loss: 1.60082\n",
      "4220: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "4240: accuracy:0.81 loss: 1.64903 (lr:9.98)\n",
      "4260: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "4280: accuracy:0.92 loss: 1.54367 (lr:9.98)\n",
      "4300: accuracy:0.86 loss: 1.6007 (lr:9.98)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.8517 test loss: 1.60941\n",
      "4320: accuracy:0.86 loss: 1.59965 (lr:9.98)\n",
      "4340: accuracy:0.9 loss: 1.56124 (lr:9.98)\n",
      "4360: accuracy:0.89 loss: 1.57119 (lr:9.98)\n",
      "4380: accuracy:0.84 loss: 1.62115 (lr:9.98)\n",
      "4400: accuracy:0.84 loss: 1.62086 (lr:9.98)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.8584 test loss: 1.60298\n",
      "4420: accuracy:0.82 loss: 1.6409 (lr:9.98)\n",
      "4440: accuracy:0.82 loss: 1.64171 (lr:9.98)\n",
      "4460: accuracy:0.85 loss: 1.6106 (lr:9.98)\n",
      "4480: accuracy:0.89 loss: 1.57078 (lr:9.98)\n",
      "4500: accuracy:0.83 loss: 1.63181 (lr:9.98)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.852 test loss: 1.60863\n",
      "4520: accuracy:0.86 loss: 1.60078 (lr:9.98)\n",
      "4540: accuracy:0.91 loss: 1.55115 (lr:9.98)\n",
      "4560: accuracy:0.85 loss: 1.61232 (lr:9.98)\n",
      "4580: accuracy:0.73 loss: 1.73057 (lr:9.98)\n",
      "4600: accuracy:0.85 loss: 1.60679 (lr:9.98)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.8558 test loss: 1.60518\n",
      "4620: accuracy:0.86 loss: 1.60107 (lr:9.98)\n",
      "4640: accuracy:0.81 loss: 1.65114 (lr:9.98)\n",
      "4660: accuracy:0.89 loss: 1.57291 (lr:9.98)\n",
      "4680: accuracy:0.9 loss: 1.56112 (lr:9.98)\n",
      "4700: accuracy:0.75 loss: 1.71216 (lr:9.98)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.8459 test loss: 1.61487\n",
      "4720: accuracy:0.91 loss: 1.5538 (lr:9.98)\n",
      "4740: accuracy:0.88 loss: 1.58132 (lr:9.98)\n",
      "4760: accuracy:0.83 loss: 1.63178 (lr:9.98)\n",
      "4780: accuracy:0.84 loss: 1.62111 (lr:9.98)\n",
      "4800: accuracy:0.87 loss: 1.59115 (lr:9.98)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.834 test loss: 1.62727\n",
      "4820: accuracy:0.88 loss: 1.5794 (lr:9.98)\n",
      "4840: accuracy:0.81 loss: 1.65126 (lr:9.98)\n",
      "4860: accuracy:0.9 loss: 1.56115 (lr:9.98)\n",
      "4880: accuracy:0.9 loss: 1.56113 (lr:9.98)\n",
      "4900: accuracy:0.87 loss: 1.5911 (lr:9.98)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.8582 test loss: 1.60277\n",
      "4920: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "4940: accuracy:0.83 loss: 1.63174 (lr:9.98)\n",
      "4960: accuracy:0.87 loss: 1.59488 (lr:9.98)\n",
      "4980: accuracy:0.89 loss: 1.57108 (lr:9.98)\n",
      "5000: accuracy:0.9 loss: 1.55592 (lr:9.98)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.8515 test loss: 1.60944\n",
      "5020: accuracy:0.9 loss: 1.5624 (lr:9.98)\n",
      "5040: accuracy:0.87 loss: 1.59115 (lr:9.98)\n",
      "5060: accuracy:0.83 loss: 1.63196 (lr:9.98)\n",
      "5080: accuracy:0.87 loss: 1.5911 (lr:9.98)\n",
      "5100: accuracy:0.94 loss: 1.52085 (lr:9.98)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.8526 test loss: 1.60855\n",
      "5120: accuracy:0.85 loss: 1.61151 (lr:9.98)\n",
      "5140: accuracy:0.85 loss: 1.61336 (lr:9.98)\n",
      "5160: accuracy:0.88 loss: 1.58114 (lr:9.98)\n",
      "5180: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "5200: accuracy:0.8 loss: 1.66101 (lr:9.98)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.8552 test loss: 1.60583\n",
      "5220: accuracy:0.82 loss: 1.64062 (lr:9.98)\n",
      "5240: accuracy:0.87 loss: 1.59065 (lr:9.98)\n",
      "5260: accuracy:0.9 loss: 1.56037 (lr:9.98)\n",
      "5280: accuracy:0.87 loss: 1.59098 (lr:9.98)\n",
      "5300: accuracy:0.8 loss: 1.66078 (lr:9.98)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.8322 test loss: 1.62881\n",
      "5320: accuracy:0.93 loss: 1.53115 (lr:9.98)\n",
      "5340: accuracy:0.76 loss: 1.69825 (lr:9.98)\n",
      "5360: accuracy:0.9 loss: 1.5635 (lr:9.98)\n",
      "5380: accuracy:0.85 loss: 1.61092 (lr:9.98)\n",
      "5400: accuracy:0.89 loss: 1.57115 (lr:9.98)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.8515 test loss: 1.60957\n",
      "5420: accuracy:0.86 loss: 1.60101 (lr:9.98)\n",
      "5440: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "5460: accuracy:0.93 loss: 1.53115 (lr:9.98)\n",
      "5480: accuracy:0.86 loss: 1.59831 (lr:9.98)\n",
      "5500: accuracy:0.85 loss: 1.61113 (lr:9.98)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.8497 test loss: 1.6113\n",
      "5520: accuracy:0.91 loss: 1.5513 (lr:9.98)\n",
      "5540: accuracy:0.87 loss: 1.59145 (lr:9.98)\n",
      "5560: accuracy:0.82 loss: 1.64115 (lr:9.98)\n",
      "5580: accuracy:0.85 loss: 1.61057 (lr:9.98)\n",
      "5600: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.8487 test loss: 1.6123\n",
      "5620: accuracy:0.79 loss: 1.67078 (lr:9.98)\n",
      "5640: accuracy:0.88 loss: 1.58089 (lr:9.98)\n",
      "5660: accuracy:0.92 loss: 1.54141 (lr:9.98)\n",
      "5680: accuracy:0.83 loss: 1.632 (lr:9.98)\n",
      "5700: accuracy:0.86 loss: 1.60298 (lr:9.98)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5700: ********* epoch 10 ********* test accuracy:0.8562 test loss: 1.60501\n",
      "5720: accuracy:0.85 loss: 1.61254 (lr:9.98)\n",
      "5740: accuracy:0.88 loss: 1.58116 (lr:9.98)\n",
      "5760: accuracy:0.79 loss: 1.67256 (lr:9.98)\n",
      "5780: accuracy:0.81 loss: 1.65111 (lr:9.98)\n",
      "5800: accuracy:0.84 loss: 1.62115 (lr:9.98)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.8556 test loss: 1.60548\n",
      "5820: accuracy:0.9 loss: 1.56114 (lr:9.98)\n",
      "5840: accuracy:0.83 loss: 1.63083 (lr:9.98)\n",
      "5860: accuracy:0.93 loss: 1.53153 (lr:9.98)\n",
      "5880: accuracy:0.86 loss: 1.60512 (lr:9.98)\n",
      "5900: accuracy:0.78 loss: 1.68112 (lr:9.98)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.8536 test loss: 1.60745\n",
      "5920: accuracy:0.84 loss: 1.62115 (lr:9.98)\n",
      "5940: accuracy:0.92 loss: 1.53421 (lr:9.98)\n",
      "5960: accuracy:0.83 loss: 1.63191 (lr:9.98)\n",
      "5980: accuracy:0.94 loss: 1.52113 (lr:9.98)\n",
      "6000: accuracy:0.85 loss: 1.611 (lr:9.98)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.8586 test loss: 1.60251\n",
      "6020: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "6040: accuracy:0.85 loss: 1.61076 (lr:9.98)\n",
      "6060: accuracy:0.85 loss: 1.60746 (lr:9.98)\n",
      "6080: accuracy:0.88 loss: 1.58125 (lr:9.98)\n",
      "6100: accuracy:0.87 loss: 1.59097 (lr:9.98)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.8613 test loss: 1.5995\n",
      "6120: accuracy:0.91 loss: 1.55115 (lr:9.98)\n",
      "6140: accuracy:0.84 loss: 1.62115 (lr:9.98)\n",
      "6160: accuracy:0.84 loss: 1.62086 (lr:9.98)\n",
      "6180: accuracy:0.85 loss: 1.61125 (lr:9.98)\n",
      "6200: accuracy:0.85 loss: 1.60741 (lr:9.98)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.8629 test loss: 1.598\n",
      "6220: accuracy:0.85 loss: 1.61078 (lr:9.98)\n",
      "6240: accuracy:0.84 loss: 1.61678 (lr:9.98)\n",
      "6260: accuracy:0.88 loss: 1.57988 (lr:9.98)\n",
      "6280: accuracy:0.85 loss: 1.61012 (lr:9.98)\n",
      "6300: accuracy:0.84 loss: 1.62038 (lr:9.98)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.8621 test loss: 1.59892\n",
      "6320: accuracy:0.86 loss: 1.6007 (lr:9.98)\n",
      "6340: accuracy:0.89 loss: 1.57111 (lr:9.98)\n",
      "6360: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "6380: accuracy:0.87 loss: 1.59109 (lr:9.98)\n",
      "6400: accuracy:0.83 loss: 1.62983 (lr:9.98)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.8466 test loss: 1.61442\n",
      "6420: accuracy:0.85 loss: 1.61051 (lr:9.98)\n",
      "6440: accuracy:0.82 loss: 1.64082 (lr:9.98)\n",
      "6460: accuracy:0.76 loss: 1.70083 (lr:9.98)\n",
      "6480: accuracy:0.87 loss: 1.58952 (lr:9.98)\n",
      "6500: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.8628 test loss: 1.59835\n",
      "6520: accuracy:0.87 loss: 1.59115 (lr:9.98)\n",
      "6540: accuracy:0.9 loss: 1.56115 (lr:9.98)\n",
      "6560: accuracy:0.81 loss: 1.65132 (lr:9.98)\n",
      "6580: accuracy:0.87 loss: 1.59112 (lr:9.98)\n",
      "6600: accuracy:0.84 loss: 1.61982 (lr:9.98)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.8599 test loss: 1.60103\n",
      "6620: accuracy:0.83 loss: 1.62896 (lr:9.98)\n",
      "6640: accuracy:0.93 loss: 1.53115 (lr:9.98)\n",
      "6660: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "6680: accuracy:0.91 loss: 1.55154 (lr:9.98)\n",
      "6700: accuracy:0.83 loss: 1.63116 (lr:9.98)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.851 test loss: 1.60999\n",
      "6720: accuracy:0.84 loss: 1.62115 (lr:9.98)\n",
      "6740: accuracy:0.84 loss: 1.62116 (lr:9.98)\n",
      "6760: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "6780: accuracy:0.86 loss: 1.601 (lr:9.98)\n",
      "6800: accuracy:0.88 loss: 1.58119 (lr:9.98)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.8524 test loss: 1.60866\n",
      "6820: accuracy:0.81 loss: 1.65105 (lr:9.98)\n",
      "6840: accuracy:0.83 loss: 1.63182 (lr:9.98)\n",
      "6860: accuracy:0.83 loss: 1.63085 (lr:9.98)\n",
      "6880: accuracy:0.9 loss: 1.5605 (lr:9.98)\n",
      "6900: accuracy:0.73 loss: 1.73115 (lr:9.98)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.8529 test loss: 1.60828\n",
      "6920: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "6940: accuracy:0.89 loss: 1.57115 (lr:9.98)\n",
      "6960: accuracy:0.9 loss: 1.56206 (lr:9.98)\n",
      "6980: accuracy:0.92 loss: 1.5396 (lr:9.98)\n",
      "7000: accuracy:0.87 loss: 1.59095 (lr:9.98)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.8669 test loss: 1.59413\n",
      "7020: accuracy:0.84 loss: 1.62115 (lr:9.98)\n",
      "7040: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "7060: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "7080: accuracy:0.93 loss: 1.53094 (lr:9.98)\n",
      "7100: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.8532 test loss: 1.60786\n",
      "7120: accuracy:0.88 loss: 1.58018 (lr:9.98)\n",
      "7140: accuracy:0.88 loss: 1.58109 (lr:9.98)\n",
      "7160: accuracy:0.86 loss: 1.60384 (lr:9.98)\n",
      "7180: accuracy:0.85 loss: 1.61087 (lr:9.98)\n",
      "7200: accuracy:0.8 loss: 1.66089 (lr:9.98)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.8631 test loss: 1.59792\n",
      "7220: accuracy:0.84 loss: 1.62114 (lr:9.98)\n",
      "7240: accuracy:0.9 loss: 1.56116 (lr:9.98)\n",
      "7260: accuracy:0.88 loss: 1.58193 (lr:9.98)\n",
      "7280: accuracy:0.81 loss: 1.65293 (lr:9.98)\n",
      "7300: accuracy:0.88 loss: 1.58191 (lr:9.98)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.846 test loss: 1.61484\n",
      "7320: accuracy:0.94 loss: 1.52115 (lr:9.98)\n",
      "7340: accuracy:0.89 loss: 1.57074 (lr:9.98)\n",
      "7360: accuracy:0.79 loss: 1.67205 (lr:9.98)\n",
      "7380: accuracy:0.86 loss: 1.60117 (lr:9.98)\n",
      "7400: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.8505 test loss: 1.61045\n",
      "7420: accuracy:0.81 loss: 1.65201 (lr:9.98)\n",
      "7440: accuracy:0.85 loss: 1.61085 (lr:9.98)\n",
      "7460: accuracy:0.91 loss: 1.55114 (lr:9.98)\n",
      "7480: accuracy:0.9 loss: 1.56115 (lr:9.98)\n",
      "7500: accuracy:0.83 loss: 1.62897 (lr:9.98)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.8545 test loss: 1.60636\n",
      "7520: accuracy:0.8 loss: 1.64956 (lr:9.98)\n",
      "7540: accuracy:0.9 loss: 1.5607 (lr:9.98)\n",
      "7560: accuracy:0.86 loss: 1.60421 (lr:9.98)\n",
      "7580: accuracy:0.85 loss: 1.61066 (lr:9.98)\n",
      "7600: accuracy:0.88 loss: 1.58078 (lr:9.98)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.8602 test loss: 1.60085\n",
      "7620: accuracy:0.85 loss: 1.61081 (lr:9.98)\n",
      "7640: accuracy:0.86 loss: 1.60109 (lr:9.98)\n",
      "7660: accuracy:0.83 loss: 1.62921 (lr:9.98)\n",
      "7680: accuracy:0.86 loss: 1.60114 (lr:9.98)\n",
      "7700: accuracy:0.84 loss: 1.62065 (lr:9.98)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.8554 test loss: 1.60536\n",
      "7720: accuracy:0.88 loss: 1.58081 (lr:9.98)\n",
      "7740: accuracy:0.83 loss: 1.63103 (lr:9.98)\n",
      "7760: accuracy:0.86 loss: 1.60093 (lr:9.98)\n",
      "7780: accuracy:0.9 loss: 1.56115 (lr:9.98)\n",
      "7800: accuracy:0.9 loss: 1.56115 (lr:9.98)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.851 test loss: 1.61004\n",
      "7820: accuracy:0.87 loss: 1.59097 (lr:9.98)\n",
      "7840: accuracy:0.86 loss: 1.60111 (lr:9.98)\n",
      "7860: accuracy:0.83 loss: 1.63079 (lr:9.98)\n",
      "7880: accuracy:0.84 loss: 1.61711 (lr:9.98)\n",
      "7900: accuracy:0.87 loss: 1.59102 (lr:9.98)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.8452 test loss: 1.61578\n",
      "7920: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "7940: accuracy:0.84 loss: 1.62115 (lr:9.98)\n",
      "7960: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "7980: accuracy:0.87 loss: 1.59058 (lr:9.98)\n",
      "8000: accuracy:0.81 loss: 1.65098 (lr:9.98)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.8596 test loss: 1.60158\n",
      "8020: accuracy:0.86 loss: 1.60239 (lr:9.98)\n",
      "8040: accuracy:0.89 loss: 1.57092 (lr:9.98)\n",
      "8060: accuracy:0.88 loss: 1.58516 (lr:9.98)\n",
      "8080: accuracy:0.84 loss: 1.62101 (lr:9.98)\n",
      "8100: accuracy:0.84 loss: 1.62087 (lr:9.98)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.8602 test loss: 1.60091\n",
      "8120: accuracy:0.82 loss: 1.64115 (lr:9.98)\n",
      "8140: accuracy:0.88 loss: 1.58147 (lr:9.98)\n",
      "8160: accuracy:0.89 loss: 1.57115 (lr:9.98)\n",
      "8180: accuracy:0.89 loss: 1.57112 (lr:9.98)\n",
      "8200: accuracy:0.88 loss: 1.58091 (lr:9.98)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.8626 test loss: 1.59834\n",
      "8220: accuracy:0.84 loss: 1.62087 (lr:9.98)\n",
      "8240: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "8260: accuracy:0.87 loss: 1.59115 (lr:9.98)\n",
      "8280: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "8300: accuracy:0.89 loss: 1.57473 (lr:9.98)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.8638 test loss: 1.59718\n",
      "8320: accuracy:0.83 loss: 1.63115 (lr:9.98)\n",
      "8340: accuracy:0.86 loss: 1.6009 (lr:9.98)\n",
      "8360: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "8380: accuracy:0.88 loss: 1.58 (lr:9.98)\n",
      "8400: accuracy:0.8 loss: 1.66115 (lr:9.98)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.8668 test loss: 1.59442\n",
      "8420: accuracy:0.84 loss: 1.62077 (lr:9.98)\n",
      "8440: accuracy:0.85 loss: 1.60908 (lr:9.98)\n",
      "8460: accuracy:0.84 loss: 1.62097 (lr:9.98)\n",
      "8480: accuracy:0.9 loss: 1.56115 (lr:9.98)\n",
      "8500: accuracy:0.91 loss: 1.55132 (lr:9.98)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.8614 test loss: 1.59951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8520: accuracy:0.87 loss: 1.59114 (lr:9.98)\n",
      "8540: accuracy:0.79 loss: 1.67099 (lr:9.98)\n",
      "8560: accuracy:0.82 loss: 1.63639 (lr:9.98)\n",
      "8580: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "8600: accuracy:0.8 loss: 1.65811 (lr:9.98)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.8628 test loss: 1.59827\n",
      "8620: accuracy:0.8 loss: 1.6637 (lr:9.98)\n",
      "8640: accuracy:0.84 loss: 1.62114 (lr:9.98)\n",
      "8660: accuracy:0.86 loss: 1.60112 (lr:9.98)\n",
      "8680: accuracy:0.9 loss: 1.56115 (lr:9.98)\n",
      "8700: accuracy:0.84 loss: 1.62104 (lr:9.98)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.8631 test loss: 1.59789\n",
      "8720: accuracy:0.82 loss: 1.64072 (lr:9.98)\n",
      "8740: accuracy:0.87 loss: 1.59115 (lr:9.98)\n",
      "8760: accuracy:0.82 loss: 1.64054 (lr:9.98)\n",
      "8780: accuracy:0.87 loss: 1.59087 (lr:9.98)\n",
      "8800: accuracy:0.83 loss: 1.63105 (lr:9.98)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.8496 test loss: 1.61145\n",
      "8820: accuracy:0.85 loss: 1.61117 (lr:9.98)\n",
      "8840: accuracy:0.91 loss: 1.55115 (lr:9.98)\n",
      "8860: accuracy:0.85 loss: 1.61114 (lr:9.98)\n",
      "8880: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "8900: accuracy:0.87 loss: 1.5907 (lr:9.98)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.8573 test loss: 1.60384\n",
      "8920: accuracy:0.88 loss: 1.58112 (lr:9.98)\n",
      "8940: accuracy:0.87 loss: 1.59115 (lr:9.98)\n",
      "8960: accuracy:0.86 loss: 1.60144 (lr:9.98)\n",
      "8980: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "9000: accuracy:0.89 loss: 1.57216 (lr:9.98)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.8547 test loss: 1.60621\n",
      "9020: accuracy:0.85 loss: 1.61122 (lr:9.98)\n",
      "9040: accuracy:0.84 loss: 1.62115 (lr:9.98)\n",
      "9060: accuracy:0.83 loss: 1.62741 (lr:9.98)\n",
      "9080: accuracy:0.84 loss: 1.62101 (lr:9.98)\n",
      "9100: accuracy:0.81 loss: 1.65106 (lr:9.98)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.8653 test loss: 1.59565\n",
      "9120: accuracy:0.82 loss: 1.64093 (lr:9.98)\n",
      "9140: accuracy:0.86 loss: 1.60088 (lr:9.98)\n",
      "9160: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "9180: accuracy:0.84 loss: 1.62093 (lr:9.98)\n",
      "9200: accuracy:0.89 loss: 1.57115 (lr:9.98)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.8524 test loss: 1.60853\n",
      "9220: accuracy:0.87 loss: 1.59115 (lr:9.98)\n",
      "9240: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "9260: accuracy:0.93 loss: 1.53115 (lr:9.98)\n",
      "9280: accuracy:0.95 loss: 1.5111 (lr:9.98)\n",
      "9300: accuracy:0.87 loss: 1.59117 (lr:9.98)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.8577 test loss: 1.60351\n",
      "9320: accuracy:0.86 loss: 1.59922 (lr:9.98)\n",
      "9340: accuracy:0.88 loss: 1.58115 (lr:9.98)\n",
      "9360: accuracy:0.87 loss: 1.59119 (lr:9.98)\n",
      "9380: accuracy:0.89 loss: 1.57115 (lr:9.98)\n",
      "9400: accuracy:0.85 loss: 1.61029 (lr:9.98)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.8526 test loss: 1.60823\n",
      "9420: accuracy:0.9 loss: 1.56115 (lr:9.98)\n",
      "9440: accuracy:0.86 loss: 1.59964 (lr:9.98)\n",
      "9460: accuracy:0.85 loss: 1.61089 (lr:9.98)\n",
      "9480: accuracy:0.82 loss: 1.63636 (lr:9.98)\n",
      "9500: accuracy:0.86 loss: 1.60072 (lr:9.98)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.8603 test loss: 1.60089\n",
      "9520: accuracy:0.85 loss: 1.60809 (lr:9.98)\n",
      "9540: accuracy:0.86 loss: 1.60098 (lr:9.98)\n",
      "9560: accuracy:0.88 loss: 1.58551 (lr:9.98)\n",
      "9580: accuracy:0.86 loss: 1.59766 (lr:9.98)\n",
      "9600: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.8637 test loss: 1.59755\n",
      "9620: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "9640: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "9660: accuracy:0.83 loss: 1.63102 (lr:9.98)\n",
      "9680: accuracy:0.9 loss: 1.5609 (lr:9.98)\n",
      "9700: accuracy:0.85 loss: 1.61114 (lr:9.98)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.8572 test loss: 1.60386\n",
      "9720: accuracy:0.81 loss: 1.65113 (lr:9.98)\n",
      "9740: accuracy:0.86 loss: 1.60115 (lr:9.98)\n",
      "9760: accuracy:0.93 loss: 1.53097 (lr:9.98)\n",
      "9780: accuracy:0.82 loss: 1.64124 (lr:9.98)\n",
      "9800: accuracy:0.89 loss: 1.57094 (lr:9.98)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.8615 test loss: 1.59967\n",
      "9820: accuracy:0.92 loss: 1.54098 (lr:9.98)\n",
      "9840: accuracy:0.89 loss: 1.57267 (lr:9.98)\n",
      "9860: accuracy:0.91 loss: 1.55115 (lr:9.98)\n",
      "9880: accuracy:0.85 loss: 1.61123 (lr:9.98)\n",
      "9900: accuracy:0.88 loss: 1.58114 (lr:9.98)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.8618 test loss: 1.59906\n",
      "9920: accuracy:0.88 loss: 1.58099 (lr:9.98)\n",
      "9940: accuracy:0.9 loss: 1.56106 (lr:9.98)\n",
      "9960: accuracy:0.85 loss: 1.61115 (lr:9.98)\n",
      "9980: accuracy:0.91 loss: 1.55114 (lr:9.98)\n",
      "10000: accuracy:0.84 loss: 1.62255 (lr:9.98)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.8613 test loss: 1.59975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [01:49<01:16, 38.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.13 loss: 2.30451 (lr:3.64)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1213 test loss: 2.30381\n",
      "20: accuracy:0.59 loss: 1.90192 (lr:3.64)\n",
      "40: accuracy:0.77 loss: 1.69442 (lr:3.64)\n",
      "60: accuracy:0.79 loss: 1.677 (lr:3.64)\n",
      "80: accuracy:0.78 loss: 1.68605 (lr:3.64)\n",
      "100: accuracy:0.81 loss: 1.65083 (lr:3.64)\n",
      "100: ********* epoch 1 ********* test accuracy:0.8439 test loss: 1.62182\n",
      "120: accuracy:0.79 loss: 1.67402 (lr:3.64)\n",
      "140: accuracy:0.8 loss: 1.65311 (lr:3.64)\n",
      "160: accuracy:0.88 loss: 1.58989 (lr:3.64)\n",
      "180: accuracy:0.91 loss: 1.55106 (lr:3.64)\n",
      "200: accuracy:0.86 loss: 1.59797 (lr:3.64)\n",
      "200: ********* epoch 1 ********* test accuracy:0.913 test loss: 1.55123\n",
      "220: accuracy:0.92 loss: 1.54938 (lr:3.64)\n",
      "240: accuracy:0.89 loss: 1.58324 (lr:3.64)\n",
      "260: accuracy:0.93 loss: 1.53058 (lr:3.64)\n",
      "280: accuracy:0.94 loss: 1.53125 (lr:3.64)\n",
      "300: accuracy:0.9 loss: 1.55957 (lr:3.64)\n",
      "300: ********* epoch 1 ********* test accuracy:0.9269 test loss: 1.53633\n",
      "320: accuracy:0.92 loss: 1.5441 (lr:3.64)\n",
      "340: accuracy:0.93 loss: 1.53467 (lr:3.64)\n",
      "360: accuracy:0.96 loss: 1.50952 (lr:3.64)\n",
      "380: accuracy:0.92 loss: 1.53782 (lr:3.64)\n",
      "400: accuracy:0.94 loss: 1.52187 (lr:3.64)\n",
      "400: ********* epoch 1 ********* test accuracy:0.9371 test loss: 1.52585\n",
      "420: accuracy:0.92 loss: 1.54272 (lr:3.64)\n",
      "440: accuracy:0.97 loss: 1.50563 (lr:3.64)\n",
      "460: accuracy:0.96 loss: 1.51132 (lr:3.64)\n",
      "480: accuracy:0.94 loss: 1.51525 (lr:3.64)\n",
      "500: accuracy:0.93 loss: 1.5368 (lr:3.64)\n",
      "500: ********* epoch 1 ********* test accuracy:0.9423 test loss: 1.52245\n",
      "520: accuracy:0.93 loss: 1.54055 (lr:3.64)\n",
      "540: accuracy:0.93 loss: 1.53333 (lr:3.64)\n",
      "560: accuracy:0.91 loss: 1.55608 (lr:3.64)\n",
      "580: accuracy:0.95 loss: 1.51062 (lr:3.64)\n",
      "600: accuracy:0.88 loss: 1.57302 (lr:3.64)\n",
      "600: ********* epoch 2 ********* test accuracy:0.9443 test loss: 1.51883\n",
      "620: accuracy:0.94 loss: 1.52169 (lr:3.64)\n",
      "640: accuracy:0.94 loss: 1.526 (lr:3.64)\n",
      "660: accuracy:1.0 loss: 1.47067 (lr:3.64)\n",
      "680: accuracy:0.97 loss: 1.4955 (lr:3.64)\n",
      "700: accuracy:0.96 loss: 1.50706 (lr:3.64)\n",
      "700: ********* epoch 2 ********* test accuracy:0.9511 test loss: 1.51153\n",
      "720: accuracy:0.96 loss: 1.49869 (lr:3.64)\n",
      "740: accuracy:0.98 loss: 1.48325 (lr:3.64)\n",
      "760: accuracy:0.96 loss: 1.49688 (lr:3.64)\n",
      "780: accuracy:0.96 loss: 1.49838 (lr:3.64)\n",
      "800: accuracy:0.96 loss: 1.50497 (lr:3.64)\n",
      "800: ********* epoch 2 ********* test accuracy:0.9442 test loss: 1.5188\n",
      "820: accuracy:0.93 loss: 1.52789 (lr:3.64)\n",
      "840: accuracy:0.95 loss: 1.50857 (lr:3.64)\n",
      "860: accuracy:0.96 loss: 1.50175 (lr:3.64)\n",
      "880: accuracy:0.94 loss: 1.52982 (lr:3.64)\n",
      "900: accuracy:0.98 loss: 1.48148 (lr:3.64)\n",
      "900: ********* epoch 2 ********* test accuracy:0.9542 test loss: 1.50875\n",
      "920: accuracy:0.95 loss: 1.50927 (lr:3.64)\n",
      "940: accuracy:0.98 loss: 1.49265 (lr:3.64)\n",
      "960: accuracy:0.99 loss: 1.47939 (lr:3.64)\n",
      "980: accuracy:0.96 loss: 1.51167 (lr:3.64)\n",
      "1000: accuracy:0.98 loss: 1.4821 (lr:3.64)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.952 test loss: 1.5096\n",
      "1020: accuracy:0.96 loss: 1.50639 (lr:3.64)\n",
      "1040: accuracy:0.96 loss: 1.50194 (lr:3.64)\n",
      "1060: accuracy:0.95 loss: 1.51424 (lr:3.64)\n",
      "1080: accuracy:0.95 loss: 1.51732 (lr:3.64)\n",
      "1100: accuracy:0.95 loss: 1.51291 (lr:3.64)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.954 test loss: 1.50808\n",
      "1120: accuracy:0.92 loss: 1.54113 (lr:3.64)\n",
      "1140: accuracy:0.97 loss: 1.49464 (lr:3.64)\n",
      "1160: accuracy:0.97 loss: 1.49622 (lr:3.64)\n",
      "1180: accuracy:0.88 loss: 1.57258 (lr:3.64)\n",
      "1200: accuracy:0.94 loss: 1.51285 (lr:3.64)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9558 test loss: 1.50644\n",
      "1220: accuracy:0.97 loss: 1.49198 (lr:3.64)\n",
      "1240: accuracy:0.96 loss: 1.51637 (lr:3.64)\n",
      "1260: accuracy:0.93 loss: 1.52613 (lr:3.64)\n",
      "1280: accuracy:0.97 loss: 1.49297 (lr:3.64)\n",
      "1300: accuracy:0.95 loss: 1.51706 (lr:3.64)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9609 test loss: 1.50226\n",
      "1320: accuracy:0.95 loss: 1.51682 (lr:3.64)\n",
      "1340: accuracy:0.97 loss: 1.49292 (lr:3.64)\n",
      "1360: accuracy:0.97 loss: 1.48064 (lr:3.64)\n",
      "1380: accuracy:0.92 loss: 1.54482 (lr:3.64)\n",
      "1400: accuracy:0.99 loss: 1.47461 (lr:3.64)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.96 test loss: 1.50222\n",
      "1420: accuracy:0.93 loss: 1.52713 (lr:3.64)\n",
      "1440: accuracy:0.96 loss: 1.49738 (lr:3.64)\n",
      "1460: accuracy:0.95 loss: 1.50771 (lr:3.64)\n",
      "1480: accuracy:0.92 loss: 1.54344 (lr:3.64)\n",
      "1500: accuracy:0.94 loss: 1.51587 (lr:3.64)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9549 test loss: 1.50741\n",
      "1520: accuracy:0.96 loss: 1.49895 (lr:3.64)\n",
      "1540: accuracy:0.98 loss: 1.48449 (lr:3.64)\n",
      "1560: accuracy:0.95 loss: 1.51372 (lr:3.64)\n",
      "1580: accuracy:0.95 loss: 1.5074 (lr:3.64)\n",
      "1600: accuracy:0.97 loss: 1.48872 (lr:3.64)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9585 test loss: 1.50345\n",
      "1620: accuracy:1.0 loss: 1.47097 (lr:3.64)\n",
      "1640: accuracy:0.97 loss: 1.49894 (lr:3.64)\n",
      "1660: accuracy:0.96 loss: 1.50048 (lr:3.64)\n",
      "1680: accuracy:0.99 loss: 1.47368 (lr:3.64)\n",
      "1700: accuracy:0.92 loss: 1.54314 (lr:3.64)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9616 test loss: 1.50045\n",
      "1720: accuracy:0.94 loss: 1.52666 (lr:3.64)\n",
      "1740: accuracy:0.94 loss: 1.52372 (lr:3.64)\n",
      "1760: accuracy:0.97 loss: 1.48908 (lr:3.64)\n",
      "1780: accuracy:0.95 loss: 1.51019 (lr:3.64)\n",
      "1800: accuracy:0.95 loss: 1.50468 (lr:3.64)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9554 test loss: 1.50589\n",
      "1820: accuracy:0.97 loss: 1.48879 (lr:3.64)\n",
      "1840: accuracy:0.96 loss: 1.50009 (lr:3.64)\n",
      "1860: accuracy:0.98 loss: 1.48159 (lr:3.64)\n",
      "1880: accuracy:0.96 loss: 1.49985 (lr:3.64)\n",
      "1900: accuracy:0.97 loss: 1.49515 (lr:3.64)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9588 test loss: 1.50315\n",
      "1920: accuracy:0.96 loss: 1.50226 (lr:3.64)\n",
      "1940: accuracy:0.96 loss: 1.50893 (lr:3.64)\n",
      "1960: accuracy:0.98 loss: 1.4825 (lr:3.64)\n",
      "1980: accuracy:0.96 loss: 1.49715 (lr:3.64)\n",
      "2000: accuracy:0.98 loss: 1.48584 (lr:3.64)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.965 test loss: 1.49714\n",
      "2020: accuracy:0.96 loss: 1.49992 (lr:3.64)\n",
      "2040: accuracy:0.99 loss: 1.47193 (lr:3.64)\n",
      "2060: accuracy:0.95 loss: 1.51758 (lr:3.64)\n",
      "2080: accuracy:0.97 loss: 1.49126 (lr:3.64)\n",
      "2100: accuracy:0.98 loss: 1.47993 (lr:3.64)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.966 test loss: 1.49534\n",
      "2120: accuracy:0.96 loss: 1.50063 (lr:3.64)\n",
      "2140: accuracy:0.96 loss: 1.50616 (lr:3.64)\n",
      "2160: accuracy:0.99 loss: 1.47454 (lr:3.64)\n",
      "2180: accuracy:0.93 loss: 1.52237 (lr:3.64)\n",
      "2200: accuracy:0.97 loss: 1.49436 (lr:3.64)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9584 test loss: 1.50287\n",
      "2220: accuracy:0.98 loss: 1.48105 (lr:3.64)\n",
      "2240: accuracy:0.96 loss: 1.49585 (lr:3.64)\n",
      "2260: accuracy:0.94 loss: 1.51669 (lr:3.64)\n",
      "2280: accuracy:0.98 loss: 1.48722 (lr:3.64)\n",
      "2300: accuracy:0.99 loss: 1.48061 (lr:3.64)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9674 test loss: 1.49429\n",
      "2320: accuracy:0.95 loss: 1.51124 (lr:3.64)\n",
      "2340: accuracy:0.94 loss: 1.51806 (lr:3.64)\n",
      "2360: accuracy:0.97 loss: 1.49116 (lr:3.64)\n",
      "2380: accuracy:0.94 loss: 1.52331 (lr:3.64)\n",
      "2400: accuracy:0.98 loss: 1.48545 (lr:3.64)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9593 test loss: 1.5035\n",
      "2420: accuracy:0.98 loss: 1.48372 (lr:3.64)\n",
      "2440: accuracy:0.95 loss: 1.51428 (lr:3.64)\n",
      "2460: accuracy:0.96 loss: 1.50707 (lr:3.64)\n",
      "2480: accuracy:0.98 loss: 1.48404 (lr:3.64)\n",
      "2500: accuracy:0.95 loss: 1.50778 (lr:3.64)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9538 test loss: 1.50782\n",
      "2520: accuracy:0.96 loss: 1.5026 (lr:3.64)\n",
      "2540: accuracy:0.96 loss: 1.50245 (lr:3.64)\n",
      "2560: accuracy:0.94 loss: 1.52501 (lr:3.64)\n",
      "2580: accuracy:0.96 loss: 1.50485 (lr:3.64)\n",
      "2600: accuracy:0.96 loss: 1.50119 (lr:3.64)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9653 test loss: 1.49619\n",
      "2620: accuracy:0.99 loss: 1.47114 (lr:3.64)\n",
      "2640: accuracy:0.97 loss: 1.48726 (lr:3.64)\n",
      "2660: accuracy:0.94 loss: 1.52105 (lr:3.64)\n",
      "2680: accuracy:0.97 loss: 1.49369 (lr:3.64)\n",
      "2700: accuracy:0.98 loss: 1.48243 (lr:3.64)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9617 test loss: 1.4992\n",
      "2720: accuracy:0.94 loss: 1.5198 (lr:3.64)\n",
      "2740: accuracy:0.98 loss: 1.48106 (lr:3.64)\n",
      "2760: accuracy:0.98 loss: 1.48741 (lr:3.64)\n",
      "2780: accuracy:0.96 loss: 1.50433 (lr:3.64)\n",
      "2800: accuracy:0.97 loss: 1.49742 (lr:3.64)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9647 test loss: 1.49685\n",
      "2820: accuracy:0.95 loss: 1.50772 (lr:3.64)\n",
      "2840: accuracy:0.98 loss: 1.4822 (lr:3.64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2860: accuracy:0.98 loss: 1.47783 (lr:3.64)\n",
      "2880: accuracy:0.97 loss: 1.49121 (lr:3.64)\n",
      "2900: accuracy:0.99 loss: 1.46973 (lr:3.64)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9614 test loss: 1.50048\n",
      "2920: accuracy:1.0 loss: 1.46455 (lr:3.64)\n",
      "2940: accuracy:0.99 loss: 1.47174 (lr:3.64)\n",
      "2960: accuracy:0.97 loss: 1.49073 (lr:3.64)\n",
      "2980: accuracy:0.98 loss: 1.48154 (lr:3.64)\n",
      "3000: accuracy:1.0 loss: 1.46396 (lr:3.64)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9659 test loss: 1.49584\n",
      "3020: accuracy:0.97 loss: 1.49234 (lr:3.64)\n",
      "3040: accuracy:0.96 loss: 1.50694 (lr:3.64)\n",
      "3060: accuracy:0.94 loss: 1.51526 (lr:3.64)\n",
      "3080: accuracy:0.96 loss: 1.49335 (lr:3.64)\n",
      "3100: accuracy:0.99 loss: 1.473 (lr:3.64)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9674 test loss: 1.49443\n",
      "3120: accuracy:0.96 loss: 1.50433 (lr:3.64)\n",
      "3140: accuracy:0.97 loss: 1.48856 (lr:3.64)\n",
      "3160: accuracy:0.94 loss: 1.51508 (lr:3.64)\n",
      "3180: accuracy:0.97 loss: 1.48972 (lr:3.64)\n",
      "3200: accuracy:1.0 loss: 1.46196 (lr:3.64)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9711 test loss: 1.4902\n",
      "3220: accuracy:0.96 loss: 1.49791 (lr:3.64)\n",
      "3240: accuracy:0.96 loss: 1.50107 (lr:3.64)\n",
      "3260: accuracy:0.95 loss: 1.50945 (lr:3.64)\n",
      "3280: accuracy:0.97 loss: 1.49698 (lr:3.64)\n",
      "3300: accuracy:0.98 loss: 1.48549 (lr:3.64)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9655 test loss: 1.4958\n",
      "3320: accuracy:0.96 loss: 1.50548 (lr:3.64)\n",
      "3340: accuracy:0.98 loss: 1.48403 (lr:3.64)\n",
      "3360: accuracy:0.93 loss: 1.52312 (lr:3.64)\n",
      "3380: accuracy:0.99 loss: 1.47116 (lr:3.64)\n",
      "3400: accuracy:0.98 loss: 1.48187 (lr:3.64)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.969 test loss: 1.49277\n",
      "3420: accuracy:0.98 loss: 1.48246 (lr:3.64)\n",
      "3440: accuracy:0.97 loss: 1.48702 (lr:3.64)\n",
      "3460: accuracy:0.97 loss: 1.4926 (lr:3.64)\n",
      "3480: accuracy:0.99 loss: 1.47199 (lr:3.64)\n",
      "3500: accuracy:1.0 loss: 1.4653 (lr:3.64)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9667 test loss: 1.49403\n",
      "3520: accuracy:0.97 loss: 1.49267 (lr:3.64)\n",
      "3540: accuracy:1.0 loss: 1.46624 (lr:3.64)\n",
      "3560: accuracy:0.96 loss: 1.50415 (lr:3.64)\n",
      "3580: accuracy:0.97 loss: 1.48856 (lr:3.64)\n",
      "3600: accuracy:0.99 loss: 1.47578 (lr:3.64)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.971 test loss: 1.49042\n",
      "3620: accuracy:0.99 loss: 1.47169 (lr:3.64)\n",
      "3640: accuracy:0.98 loss: 1.48328 (lr:3.64)\n",
      "3660: accuracy:0.98 loss: 1.47821 (lr:3.64)\n",
      "3680: accuracy:0.98 loss: 1.49266 (lr:3.64)\n",
      "3700: accuracy:0.99 loss: 1.47165 (lr:3.64)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9703 test loss: 1.49055\n",
      "3720: accuracy:0.98 loss: 1.48154 (lr:3.64)\n",
      "3740: accuracy:0.97 loss: 1.49128 (lr:3.64)\n",
      "3760: accuracy:0.96 loss: 1.49929 (lr:3.64)\n",
      "3780: accuracy:1.0 loss: 1.46289 (lr:3.64)\n",
      "3800: accuracy:0.98 loss: 1.47995 (lr:3.64)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9724 test loss: 1.48923\n",
      "3820: accuracy:1.0 loss: 1.46337 (lr:3.64)\n",
      "3840: accuracy:0.97 loss: 1.49224 (lr:3.64)\n",
      "3860: accuracy:0.98 loss: 1.47752 (lr:3.64)\n",
      "3880: accuracy:0.97 loss: 1.48868 (lr:3.64)\n",
      "3900: accuracy:0.96 loss: 1.50044 (lr:3.64)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.97 test loss: 1.49138\n",
      "3920: accuracy:0.98 loss: 1.48241 (lr:3.64)\n",
      "3940: accuracy:0.98 loss: 1.48146 (lr:3.64)\n",
      "3960: accuracy:0.97 loss: 1.48932 (lr:3.64)\n",
      "3980: accuracy:0.98 loss: 1.47975 (lr:3.64)\n",
      "4000: accuracy:0.98 loss: 1.4867 (lr:3.64)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9603 test loss: 1.50064\n",
      "4020: accuracy:0.98 loss: 1.48486 (lr:3.64)\n",
      "4040: accuracy:0.98 loss: 1.48389 (lr:3.64)\n",
      "4060: accuracy:0.99 loss: 1.47343 (lr:3.64)\n",
      "4080: accuracy:0.97 loss: 1.49221 (lr:3.64)\n",
      "4100: accuracy:0.98 loss: 1.48466 (lr:3.64)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9659 test loss: 1.49531\n",
      "4120: accuracy:0.96 loss: 1.49876 (lr:3.64)\n",
      "4140: accuracy:1.0 loss: 1.46129 (lr:3.64)\n",
      "4160: accuracy:0.97 loss: 1.4927 (lr:3.64)\n",
      "4180: accuracy:0.97 loss: 1.49079 (lr:3.64)\n",
      "4200: accuracy:0.96 loss: 1.49962 (lr:3.64)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9693 test loss: 1.49223\n",
      "4220: accuracy:0.99 loss: 1.47333 (lr:3.64)\n",
      "4240: accuracy:0.98 loss: 1.47628 (lr:3.64)\n",
      "4260: accuracy:0.97 loss: 1.48614 (lr:3.64)\n",
      "4280: accuracy:0.99 loss: 1.47114 (lr:3.64)\n",
      "4300: accuracy:0.93 loss: 1.52826 (lr:3.64)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9678 test loss: 1.49356\n",
      "4320: accuracy:0.98 loss: 1.48072 (lr:3.64)\n",
      "4340: accuracy:0.98 loss: 1.481 (lr:3.64)\n",
      "4360: accuracy:0.97 loss: 1.49393 (lr:3.64)\n",
      "4380: accuracy:0.99 loss: 1.47112 (lr:3.64)\n",
      "4400: accuracy:0.98 loss: 1.48114 (lr:3.64)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9684 test loss: 1.49296\n",
      "4420: accuracy:0.99 loss: 1.47314 (lr:3.64)\n",
      "4440: accuracy:0.97 loss: 1.49115 (lr:3.64)\n",
      "4460: accuracy:0.97 loss: 1.48733 (lr:3.64)\n",
      "4480: accuracy:1.0 loss: 1.46138 (lr:3.64)\n",
      "4500: accuracy:0.99 loss: 1.47146 (lr:3.64)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9698 test loss: 1.49186\n",
      "4520: accuracy:1.0 loss: 1.46238 (lr:3.64)\n",
      "4540: accuracy:0.99 loss: 1.47471 (lr:3.64)\n",
      "4560: accuracy:0.99 loss: 1.47306 (lr:3.64)\n",
      "4580: accuracy:0.99 loss: 1.47342 (lr:3.64)\n",
      "4600: accuracy:0.98 loss: 1.48313 (lr:3.64)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9711 test loss: 1.4898\n",
      "4620: accuracy:0.99 loss: 1.47045 (lr:3.64)\n",
      "4640: accuracy:0.96 loss: 1.49871 (lr:3.64)\n",
      "4660: accuracy:0.95 loss: 1.51228 (lr:3.64)\n",
      "4680: accuracy:0.99 loss: 1.47437 (lr:3.64)\n",
      "4700: accuracy:1.0 loss: 1.46257 (lr:3.64)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9732 test loss: 1.48833\n",
      "4720: accuracy:0.98 loss: 1.47764 (lr:3.64)\n",
      "4740: accuracy:0.98 loss: 1.48594 (lr:3.64)\n",
      "4760: accuracy:0.99 loss: 1.47238 (lr:3.64)\n",
      "4780: accuracy:0.94 loss: 1.52177 (lr:3.64)\n",
      "4800: accuracy:0.99 loss: 1.47355 (lr:3.64)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9651 test loss: 1.49611\n",
      "4820: accuracy:0.98 loss: 1.48204 (lr:3.64)\n",
      "4840: accuracy:0.95 loss: 1.51066 (lr:3.64)\n",
      "4860: accuracy:0.97 loss: 1.48648 (lr:3.64)\n",
      "4880: accuracy:0.97 loss: 1.49079 (lr:3.64)\n",
      "4900: accuracy:0.97 loss: 1.48699 (lr:3.64)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9638 test loss: 1.49719\n",
      "4920: accuracy:0.97 loss: 1.4907 (lr:3.64)\n",
      "4940: accuracy:0.97 loss: 1.48628 (lr:3.64)\n",
      "4960: accuracy:0.98 loss: 1.48256 (lr:3.64)\n",
      "4980: accuracy:0.99 loss: 1.47762 (lr:3.64)\n",
      "5000: accuracy:1.0 loss: 1.46164 (lr:3.64)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9733 test loss: 1.48822\n",
      "5020: accuracy:1.0 loss: 1.46476 (lr:3.64)\n",
      "5040: accuracy:0.97 loss: 1.48706 (lr:3.64)\n",
      "5060: accuracy:0.98 loss: 1.4835 (lr:3.64)\n",
      "5080: accuracy:0.96 loss: 1.50773 (lr:3.64)\n",
      "5100: accuracy:0.95 loss: 1.50673 (lr:3.64)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9701 test loss: 1.49109\n",
      "5120: accuracy:1.0 loss: 1.46474 (lr:3.64)\n",
      "5140: accuracy:0.98 loss: 1.47776 (lr:3.64)\n",
      "5160: accuracy:0.98 loss: 1.48517 (lr:3.64)\n",
      "5180: accuracy:0.99 loss: 1.47344 (lr:3.64)\n",
      "5200: accuracy:0.96 loss: 1.5015 (lr:3.64)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9732 test loss: 1.48833\n",
      "5220: accuracy:0.99 loss: 1.47069 (lr:3.64)\n",
      "5240: accuracy:0.96 loss: 1.4986 (lr:3.64)\n",
      "5260: accuracy:0.98 loss: 1.48358 (lr:3.64)\n",
      "5280: accuracy:0.99 loss: 1.47476 (lr:3.64)\n",
      "5300: accuracy:0.98 loss: 1.48551 (lr:3.64)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9726 test loss: 1.48894\n",
      "5320: accuracy:0.98 loss: 1.48208 (lr:3.64)\n",
      "5340: accuracy:0.97 loss: 1.49002 (lr:3.64)\n",
      "5360: accuracy:0.99 loss: 1.46791 (lr:3.64)\n",
      "5380: accuracy:0.98 loss: 1.4808 (lr:3.64)\n",
      "5400: accuracy:0.99 loss: 1.47281 (lr:3.64)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9706 test loss: 1.4908\n",
      "5420: accuracy:0.95 loss: 1.51262 (lr:3.64)\n",
      "5440: accuracy:0.98 loss: 1.48125 (lr:3.64)\n",
      "5460: accuracy:0.98 loss: 1.48117 (lr:3.64)\n",
      "5480: accuracy:0.98 loss: 1.48079 (lr:3.64)\n",
      "5500: accuracy:1.0 loss: 1.46593 (lr:3.64)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9713 test loss: 1.49047\n",
      "5520: accuracy:0.98 loss: 1.48298 (lr:3.64)\n",
      "5540: accuracy:0.99 loss: 1.47114 (lr:3.64)\n",
      "5560: accuracy:0.99 loss: 1.47332 (lr:3.64)\n",
      "5580: accuracy:0.99 loss: 1.47401 (lr:3.64)\n",
      "5600: accuracy:0.97 loss: 1.49144 (lr:3.64)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9734 test loss: 1.48766\n",
      "5620: accuracy:0.98 loss: 1.48218 (lr:3.64)\n",
      "5640: accuracy:0.95 loss: 1.50588 (lr:3.64)\n",
      "5660: accuracy:0.98 loss: 1.48035 (lr:3.64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5680: accuracy:0.98 loss: 1.48191 (lr:3.64)\n",
      "5700: accuracy:0.99 loss: 1.47238 (lr:3.64)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.9728 test loss: 1.48847\n",
      "5720: accuracy:0.98 loss: 1.4717 (lr:3.64)\n",
      "5740: accuracy:1.0 loss: 1.4613 (lr:3.64)\n",
      "5760: accuracy:1.0 loss: 1.46512 (lr:3.64)\n",
      "5780: accuracy:0.99 loss: 1.47333 (lr:3.64)\n",
      "5800: accuracy:0.97 loss: 1.49445 (lr:3.64)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9705 test loss: 1.49061\n",
      "5820: accuracy:0.97 loss: 1.49004 (lr:3.64)\n",
      "5840: accuracy:0.99 loss: 1.47588 (lr:3.64)\n",
      "5860: accuracy:1.0 loss: 1.46337 (lr:3.64)\n",
      "5880: accuracy:1.0 loss: 1.46139 (lr:3.64)\n",
      "5900: accuracy:0.96 loss: 1.49872 (lr:3.64)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9717 test loss: 1.48963\n",
      "5920: accuracy:0.97 loss: 1.49056 (lr:3.64)\n",
      "5940: accuracy:0.99 loss: 1.47131 (lr:3.64)\n",
      "5960: accuracy:0.97 loss: 1.49061 (lr:3.64)\n",
      "5980: accuracy:0.97 loss: 1.48961 (lr:3.64)\n",
      "6000: accuracy:0.99 loss: 1.47081 (lr:3.64)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.972 test loss: 1.4889\n",
      "6020: accuracy:0.99 loss: 1.47296 (lr:3.64)\n",
      "6040: accuracy:0.96 loss: 1.50508 (lr:3.64)\n",
      "6060: accuracy:1.0 loss: 1.46124 (lr:3.64)\n",
      "6080: accuracy:0.99 loss: 1.47123 (lr:3.64)\n",
      "6100: accuracy:0.99 loss: 1.47246 (lr:3.64)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9732 test loss: 1.4879\n",
      "6120: accuracy:0.98 loss: 1.47841 (lr:3.64)\n",
      "6140: accuracy:0.96 loss: 1.49857 (lr:3.64)\n",
      "6160: accuracy:0.98 loss: 1.48145 (lr:3.64)\n",
      "6180: accuracy:0.99 loss: 1.47013 (lr:3.64)\n",
      "6200: accuracy:0.98 loss: 1.48513 (lr:3.64)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9704 test loss: 1.49086\n",
      "6220: accuracy:0.97 loss: 1.48771 (lr:3.64)\n",
      "6240: accuracy:0.96 loss: 1.49744 (lr:3.64)\n",
      "6260: accuracy:0.94 loss: 1.51842 (lr:3.64)\n",
      "6280: accuracy:0.99 loss: 1.471 (lr:3.64)\n",
      "6300: accuracy:1.0 loss: 1.46124 (lr:3.64)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9748 test loss: 1.48643\n",
      "6320: accuracy:0.99 loss: 1.4732 (lr:3.64)\n",
      "6340: accuracy:0.99 loss: 1.4712 (lr:3.64)\n",
      "6360: accuracy:1.0 loss: 1.46148 (lr:3.64)\n",
      "6380: accuracy:0.99 loss: 1.47305 (lr:3.64)\n",
      "6400: accuracy:0.97 loss: 1.49156 (lr:3.64)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.974 test loss: 1.48756\n",
      "6420: accuracy:0.98 loss: 1.48169 (lr:3.64)\n",
      "6440: accuracy:0.95 loss: 1.51055 (lr:3.64)\n",
      "6460: accuracy:1.0 loss: 1.46609 (lr:3.64)\n",
      "6480: accuracy:0.99 loss: 1.47268 (lr:3.64)\n",
      "6500: accuracy:0.97 loss: 1.49642 (lr:3.64)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9731 test loss: 1.48846\n",
      "6520: accuracy:0.98 loss: 1.4812 (lr:3.64)\n",
      "6540: accuracy:0.99 loss: 1.47203 (lr:3.64)\n",
      "6560: accuracy:0.97 loss: 1.49145 (lr:3.64)\n",
      "6580: accuracy:0.99 loss: 1.47403 (lr:3.64)\n",
      "6600: accuracy:0.97 loss: 1.49225 (lr:3.64)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9713 test loss: 1.48933\n",
      "6620: accuracy:0.98 loss: 1.48125 (lr:3.64)\n",
      "6640: accuracy:0.98 loss: 1.48219 (lr:3.64)\n",
      "6660: accuracy:0.99 loss: 1.47127 (lr:3.64)\n",
      "6680: accuracy:0.98 loss: 1.48037 (lr:3.64)\n",
      "6700: accuracy:0.99 loss: 1.4671 (lr:3.64)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9728 test loss: 1.48798\n",
      "6720: accuracy:0.99 loss: 1.47713 (lr:3.64)\n",
      "6740: accuracy:0.99 loss: 1.47216 (lr:3.64)\n",
      "6760: accuracy:0.98 loss: 1.48609 (lr:3.64)\n",
      "6780: accuracy:0.97 loss: 1.49089 (lr:3.64)\n",
      "6800: accuracy:0.98 loss: 1.48128 (lr:3.64)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9765 test loss: 1.48533\n",
      "6820: accuracy:1.0 loss: 1.46118 (lr:3.64)\n",
      "6840: accuracy:0.98 loss: 1.48153 (lr:3.64)\n",
      "6860: accuracy:0.97 loss: 1.49099 (lr:3.64)\n",
      "6880: accuracy:0.98 loss: 1.47952 (lr:3.64)\n",
      "6900: accuracy:0.99 loss: 1.47257 (lr:3.64)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9744 test loss: 1.48681\n",
      "6920: accuracy:0.96 loss: 1.49691 (lr:3.64)\n",
      "6940: accuracy:0.99 loss: 1.4742 (lr:3.64)\n",
      "6960: accuracy:0.99 loss: 1.46655 (lr:3.64)\n",
      "6980: accuracy:0.97 loss: 1.49121 (lr:3.64)\n",
      "7000: accuracy:0.98 loss: 1.48417 (lr:3.64)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9645 test loss: 1.49674\n",
      "7020: accuracy:1.0 loss: 1.46375 (lr:3.64)\n",
      "7040: accuracy:0.97 loss: 1.49367 (lr:3.64)\n",
      "7060: accuracy:0.99 loss: 1.47538 (lr:3.64)\n",
      "7080: accuracy:0.98 loss: 1.48174 (lr:3.64)\n",
      "7100: accuracy:0.97 loss: 1.4928 (lr:3.64)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9738 test loss: 1.48798\n",
      "7120: accuracy:0.99 loss: 1.47108 (lr:3.64)\n",
      "7140: accuracy:0.99 loss: 1.46948 (lr:3.64)\n",
      "7160: accuracy:0.99 loss: 1.47099 (lr:3.64)\n",
      "7180: accuracy:0.98 loss: 1.48165 (lr:3.64)\n",
      "7200: accuracy:0.99 loss: 1.46782 (lr:3.64)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9748 test loss: 1.48691\n",
      "7220: accuracy:1.0 loss: 1.46173 (lr:3.64)\n",
      "7240: accuracy:0.99 loss: 1.47406 (lr:3.64)\n",
      "7260: accuracy:1.0 loss: 1.46586 (lr:3.64)\n",
      "7280: accuracy:0.98 loss: 1.4831 (lr:3.64)\n",
      "7300: accuracy:0.98 loss: 1.47805 (lr:3.64)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9714 test loss: 1.49007\n",
      "7320: accuracy:0.99 loss: 1.4674 (lr:3.64)\n",
      "7340: accuracy:0.97 loss: 1.49158 (lr:3.64)\n",
      "7360: accuracy:0.99 loss: 1.47211 (lr:3.64)\n",
      "7380: accuracy:0.98 loss: 1.48184 (lr:3.64)\n",
      "7400: accuracy:0.96 loss: 1.50122 (lr:3.64)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9737 test loss: 1.4879\n",
      "7420: accuracy:0.95 loss: 1.51075 (lr:3.64)\n",
      "7440: accuracy:0.96 loss: 1.50434 (lr:3.64)\n",
      "7460: accuracy:0.99 loss: 1.47301 (lr:3.64)\n",
      "7480: accuracy:0.99 loss: 1.47123 (lr:3.64)\n",
      "7500: accuracy:0.99 loss: 1.46816 (lr:3.64)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9742 test loss: 1.48685\n",
      "7520: accuracy:0.97 loss: 1.4902 (lr:3.64)\n",
      "7540: accuracy:0.98 loss: 1.48131 (lr:3.64)\n",
      "7560: accuracy:1.0 loss: 1.46501 (lr:3.64)\n",
      "7580: accuracy:1.0 loss: 1.46316 (lr:3.64)\n",
      "7600: accuracy:0.98 loss: 1.48118 (lr:3.64)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9752 test loss: 1.48589\n",
      "7620: accuracy:1.0 loss: 1.46192 (lr:3.64)\n",
      "7640: accuracy:0.96 loss: 1.49868 (lr:3.64)\n",
      "7660: accuracy:0.98 loss: 1.48202 (lr:3.64)\n",
      "7680: accuracy:0.99 loss: 1.46871 (lr:3.64)\n",
      "7700: accuracy:0.98 loss: 1.4812 (lr:3.64)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.972 test loss: 1.48939\n",
      "7720: accuracy:0.99 loss: 1.4712 (lr:3.64)\n",
      "7740: accuracy:1.0 loss: 1.46227 (lr:3.64)\n",
      "7760: accuracy:0.99 loss: 1.4742 (lr:3.64)\n",
      "7780: accuracy:0.99 loss: 1.47397 (lr:3.64)\n",
      "7800: accuracy:0.99 loss: 1.47118 (lr:3.64)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9757 test loss: 1.48583\n",
      "7820: accuracy:0.99 loss: 1.47122 (lr:3.64)\n",
      "7840: accuracy:1.0 loss: 1.46125 (lr:3.64)\n",
      "7860: accuracy:0.98 loss: 1.47744 (lr:3.64)\n",
      "7880: accuracy:0.99 loss: 1.4663 (lr:3.64)\n",
      "7900: accuracy:1.0 loss: 1.46219 (lr:3.64)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9754 test loss: 1.48594\n",
      "7920: accuracy:0.99 loss: 1.47114 (lr:3.64)\n",
      "7940: accuracy:0.96 loss: 1.49742 (lr:3.64)\n",
      "7960: accuracy:0.98 loss: 1.47595 (lr:3.64)\n",
      "7980: accuracy:0.97 loss: 1.4895 (lr:3.64)\n",
      "8000: accuracy:0.98 loss: 1.4812 (lr:3.64)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.972 test loss: 1.48911\n",
      "8020: accuracy:0.99 loss: 1.47115 (lr:3.64)\n",
      "8040: accuracy:0.99 loss: 1.47465 (lr:3.64)\n",
      "8060: accuracy:0.97 loss: 1.48807 (lr:3.64)\n",
      "8080: accuracy:0.98 loss: 1.47937 (lr:3.64)\n",
      "8100: accuracy:0.98 loss: 1.4852 (lr:3.64)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.9729 test loss: 1.48802\n",
      "8120: accuracy:0.98 loss: 1.4809 (lr:3.64)\n",
      "8140: accuracy:1.0 loss: 1.46115 (lr:3.64)\n",
      "8160: accuracy:0.97 loss: 1.49253 (lr:3.64)\n",
      "8180: accuracy:1.0 loss: 1.46128 (lr:3.64)\n",
      "8200: accuracy:0.99 loss: 1.47325 (lr:3.64)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9748 test loss: 1.48641\n",
      "8220: accuracy:0.99 loss: 1.47288 (lr:3.64)\n",
      "8240: accuracy:1.0 loss: 1.46188 (lr:3.64)\n",
      "8260: accuracy:0.97 loss: 1.48426 (lr:3.64)\n",
      "8280: accuracy:0.99 loss: 1.47123 (lr:3.64)\n",
      "8300: accuracy:0.96 loss: 1.5013 (lr:3.64)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9736 test loss: 1.4875\n",
      "8320: accuracy:1.0 loss: 1.46826 (lr:3.64)\n",
      "8340: accuracy:0.97 loss: 1.49637 (lr:3.64)\n",
      "8360: accuracy:0.97 loss: 1.49483 (lr:3.64)\n",
      "8380: accuracy:0.99 loss: 1.47265 (lr:3.64)\n",
      "8400: accuracy:0.98 loss: 1.47927 (lr:3.64)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9729 test loss: 1.48823\n",
      "8420: accuracy:0.99 loss: 1.47053 (lr:3.64)\n",
      "8440: accuracy:0.99 loss: 1.47117 (lr:3.64)\n",
      "8460: accuracy:0.96 loss: 1.49986 (lr:3.64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8480: accuracy:0.99 loss: 1.476 (lr:3.64)\n",
      "8500: accuracy:0.99 loss: 1.47254 (lr:3.64)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9739 test loss: 1.48688\n",
      "8520: accuracy:0.99 loss: 1.47117 (lr:3.64)\n",
      "8540: accuracy:1.0 loss: 1.46163 (lr:3.64)\n",
      "8560: accuracy:0.99 loss: 1.46649 (lr:3.64)\n",
      "8580: accuracy:1.0 loss: 1.46159 (lr:3.64)\n",
      "8600: accuracy:0.98 loss: 1.47877 (lr:3.64)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9769 test loss: 1.48423\n",
      "8620: accuracy:0.99 loss: 1.4713 (lr:3.64)\n",
      "8640: accuracy:0.99 loss: 1.47537 (lr:3.64)\n",
      "8660: accuracy:0.98 loss: 1.47545 (lr:3.64)\n",
      "8680: accuracy:0.98 loss: 1.48248 (lr:3.64)\n",
      "8700: accuracy:0.97 loss: 1.49602 (lr:3.64)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9754 test loss: 1.48632\n",
      "8720: accuracy:0.98 loss: 1.47558 (lr:3.64)\n",
      "8740: accuracy:1.0 loss: 1.46363 (lr:3.64)\n",
      "8760: accuracy:0.98 loss: 1.48235 (lr:3.64)\n",
      "8780: accuracy:0.95 loss: 1.50537 (lr:3.64)\n",
      "8800: accuracy:1.0 loss: 1.46249 (lr:3.64)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9683 test loss: 1.49214\n",
      "8820: accuracy:0.98 loss: 1.48528 (lr:3.64)\n",
      "8840: accuracy:0.99 loss: 1.47044 (lr:3.64)\n",
      "8860: accuracy:0.99 loss: 1.46981 (lr:3.64)\n",
      "8880: accuracy:0.99 loss: 1.47124 (lr:3.64)\n",
      "8900: accuracy:0.99 loss: 1.46816 (lr:3.64)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.9711 test loss: 1.48985\n",
      "8920: accuracy:1.0 loss: 1.46117 (lr:3.64)\n",
      "8940: accuracy:0.99 loss: 1.47079 (lr:3.64)\n",
      "8960: accuracy:1.0 loss: 1.46125 (lr:3.64)\n",
      "8980: accuracy:0.97 loss: 1.49026 (lr:3.64)\n",
      "9000: accuracy:0.99 loss: 1.46677 (lr:3.64)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9708 test loss: 1.49037\n",
      "9020: accuracy:0.99 loss: 1.47168 (lr:3.64)\n",
      "9040: accuracy:0.99 loss: 1.46595 (lr:3.64)\n",
      "9060: accuracy:0.98 loss: 1.48462 (lr:3.64)\n",
      "9080: accuracy:0.99 loss: 1.47178 (lr:3.64)\n",
      "9100: accuracy:0.95 loss: 1.50894 (lr:3.64)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.9743 test loss: 1.4875\n",
      "9120: accuracy:0.96 loss: 1.49961 (lr:3.64)\n",
      "9140: accuracy:0.97 loss: 1.49123 (lr:3.64)\n",
      "9160: accuracy:0.99 loss: 1.47128 (lr:3.64)\n",
      "9180: accuracy:0.97 loss: 1.48672 (lr:3.64)\n",
      "9200: accuracy:0.96 loss: 1.50043 (lr:3.64)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9755 test loss: 1.48539\n",
      "9220: accuracy:0.99 loss: 1.4726 (lr:3.64)\n",
      "9240: accuracy:0.99 loss: 1.47119 (lr:3.64)\n",
      "9260: accuracy:0.99 loss: 1.47125 (lr:3.64)\n",
      "9280: accuracy:0.99 loss: 1.46597 (lr:3.64)\n",
      "9300: accuracy:0.99 loss: 1.47158 (lr:3.64)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9743 test loss: 1.48725\n",
      "9320: accuracy:0.98 loss: 1.47892 (lr:3.64)\n",
      "9340: accuracy:0.98 loss: 1.48354 (lr:3.64)\n",
      "9360: accuracy:0.96 loss: 1.50016 (lr:3.64)\n",
      "9380: accuracy:0.99 loss: 1.46872 (lr:3.64)\n",
      "9400: accuracy:0.98 loss: 1.47811 (lr:3.64)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9743 test loss: 1.48749\n",
      "9420: accuracy:1.0 loss: 1.46568 (lr:3.64)\n",
      "9440: accuracy:0.99 loss: 1.46819 (lr:3.64)\n",
      "9460: accuracy:0.99 loss: 1.47258 (lr:3.64)\n",
      "9480: accuracy:1.0 loss: 1.46454 (lr:3.64)\n",
      "9500: accuracy:0.99 loss: 1.46599 (lr:3.64)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9751 test loss: 1.4865\n",
      "9520: accuracy:0.96 loss: 1.50422 (lr:3.64)\n",
      "9540: accuracy:0.97 loss: 1.48639 (lr:3.64)\n",
      "9560: accuracy:1.0 loss: 1.46115 (lr:3.64)\n",
      "9580: accuracy:1.0 loss: 1.46174 (lr:3.64)\n",
      "9600: accuracy:0.94 loss: 1.52353 (lr:3.64)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9733 test loss: 1.48818\n",
      "9620: accuracy:0.99 loss: 1.47098 (lr:3.64)\n",
      "9640: accuracy:0.97 loss: 1.49132 (lr:3.64)\n",
      "9660: accuracy:0.97 loss: 1.49283 (lr:3.64)\n",
      "9680: accuracy:0.97 loss: 1.49116 (lr:3.64)\n",
      "9700: accuracy:0.99 loss: 1.47387 (lr:3.64)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9746 test loss: 1.48698\n",
      "9720: accuracy:0.98 loss: 1.47866 (lr:3.64)\n",
      "9740: accuracy:0.97 loss: 1.4884 (lr:3.64)\n",
      "9760: accuracy:1.0 loss: 1.46249 (lr:3.64)\n",
      "9780: accuracy:0.98 loss: 1.47904 (lr:3.64)\n",
      "9800: accuracy:0.99 loss: 1.47053 (lr:3.64)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9741 test loss: 1.48724\n",
      "9820: accuracy:0.99 loss: 1.4723 (lr:3.64)\n",
      "9840: accuracy:1.0 loss: 1.46146 (lr:3.64)\n",
      "9860: accuracy:1.0 loss: 1.46122 (lr:3.64)\n",
      "9880: accuracy:1.0 loss: 1.46146 (lr:3.64)\n",
      "9900: accuracy:1.0 loss: 1.46576 (lr:3.64)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9773 test loss: 1.48417\n",
      "9920: accuracy:0.99 loss: 1.47233 (lr:3.64)\n",
      "9940: accuracy:0.98 loss: 1.47988 (lr:3.64)\n",
      "9960: accuracy:0.97 loss: 1.49104 (lr:3.64)\n",
      "9980: accuracy:0.99 loss: 1.47119 (lr:3.64)\n",
      "10000: accuracy:1.0 loss: 1.46395 (lr:3.64)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9785 test loss: 1.48304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [02:29<00:38, 38.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.11 loss: 2.3 (lr:2.58)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1022 test loss: 2.29952\n",
      "20: accuracy:0.58 loss: 1.88175 (lr:2.58)\n",
      "40: accuracy:0.71 loss: 1.74759 (lr:2.58)\n",
      "60: accuracy:0.84 loss: 1.63895 (lr:2.58)\n",
      "80: accuracy:0.79 loss: 1.66328 (lr:2.58)\n",
      "100: accuracy:0.81 loss: 1.66642 (lr:2.58)\n",
      "100: ********* epoch 1 ********* test accuracy:0.8313 test loss: 1.63535\n",
      "120: accuracy:0.78 loss: 1.67404 (lr:2.58)\n",
      "140: accuracy:0.86 loss: 1.606 (lr:2.58)\n",
      "160: accuracy:0.9 loss: 1.56981 (lr:2.58)\n",
      "180: accuracy:0.83 loss: 1.638 (lr:2.58)\n",
      "200: accuracy:0.89 loss: 1.57386 (lr:2.58)\n",
      "200: ********* epoch 1 ********* test accuracy:0.8672 test loss: 1.60333\n",
      "220: accuracy:0.85 loss: 1.60492 (lr:2.58)\n",
      "240: accuracy:0.84 loss: 1.61974 (lr:2.58)\n",
      "260: accuracy:0.84 loss: 1.64408 (lr:2.58)\n",
      "280: accuracy:0.88 loss: 1.58356 (lr:2.58)\n",
      "300: accuracy:0.85 loss: 1.61788 (lr:2.58)\n",
      "300: ********* epoch 1 ********* test accuracy:0.8712 test loss: 1.59357\n",
      "320: accuracy:0.94 loss: 1.53032 (lr:2.58)\n",
      "340: accuracy:0.93 loss: 1.54489 (lr:2.58)\n",
      "360: accuracy:0.92 loss: 1.56156 (lr:2.58)\n",
      "380: accuracy:0.94 loss: 1.53156 (lr:2.58)\n",
      "400: accuracy:0.92 loss: 1.53274 (lr:2.58)\n",
      "400: ********* epoch 1 ********* test accuracy:0.9216 test loss: 1.54282\n",
      "420: accuracy:0.9 loss: 1.57726 (lr:2.58)\n",
      "440: accuracy:0.95 loss: 1.50915 (lr:2.58)\n",
      "460: accuracy:0.97 loss: 1.49615 (lr:2.58)\n",
      "480: accuracy:0.91 loss: 1.55602 (lr:2.58)\n",
      "500: accuracy:0.97 loss: 1.50354 (lr:2.58)\n",
      "500: ********* epoch 1 ********* test accuracy:0.9372 test loss: 1.52758\n",
      "520: accuracy:0.95 loss: 1.51464 (lr:2.58)\n",
      "540: accuracy:0.9 loss: 1.5616 (lr:2.58)\n",
      "560: accuracy:0.94 loss: 1.52217 (lr:2.58)\n",
      "580: accuracy:0.91 loss: 1.55578 (lr:2.58)\n",
      "600: accuracy:0.92 loss: 1.53758 (lr:2.58)\n",
      "600: ********* epoch 2 ********* test accuracy:0.9394 test loss: 1.5258\n",
      "620: accuracy:0.93 loss: 1.54851 (lr:2.58)\n",
      "640: accuracy:0.94 loss: 1.5288 (lr:2.58)\n",
      "660: accuracy:0.95 loss: 1.50882 (lr:2.58)\n",
      "680: accuracy:0.95 loss: 1.51303 (lr:2.58)\n",
      "700: accuracy:0.92 loss: 1.55402 (lr:2.58)\n",
      "700: ********* epoch 2 ********* test accuracy:0.9341 test loss: 1.53106\n",
      "720: accuracy:0.93 loss: 1.53276 (lr:2.58)\n",
      "740: accuracy:0.96 loss: 1.5035 (lr:2.58)\n",
      "760: accuracy:0.93 loss: 1.54136 (lr:2.58)\n",
      "780: accuracy:0.96 loss: 1.50282 (lr:2.58)\n",
      "800: accuracy:0.95 loss: 1.51363 (lr:2.58)\n",
      "800: ********* epoch 2 ********* test accuracy:0.9493 test loss: 1.51449\n",
      "820: accuracy:0.98 loss: 1.49107 (lr:2.58)\n",
      "840: accuracy:0.99 loss: 1.47472 (lr:2.58)\n",
      "860: accuracy:0.97 loss: 1.49403 (lr:2.58)\n",
      "880: accuracy:0.89 loss: 1.57477 (lr:2.58)\n",
      "900: accuracy:0.94 loss: 1.52432 (lr:2.58)\n",
      "900: ********* epoch 2 ********* test accuracy:0.9467 test loss: 1.5169\n",
      "920: accuracy:0.94 loss: 1.51931 (lr:2.58)\n",
      "940: accuracy:0.96 loss: 1.51174 (lr:2.58)\n",
      "960: accuracy:0.95 loss: 1.51209 (lr:2.58)\n",
      "980: accuracy:0.97 loss: 1.4958 (lr:2.58)\n",
      "1000: accuracy:0.96 loss: 1.50853 (lr:2.58)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9538 test loss: 1.5093\n",
      "1020: accuracy:0.93 loss: 1.53523 (lr:2.58)\n",
      "1040: accuracy:0.95 loss: 1.52867 (lr:2.58)\n",
      "1060: accuracy:0.96 loss: 1.5059 (lr:2.58)\n",
      "1080: accuracy:0.93 loss: 1.52853 (lr:2.58)\n",
      "1100: accuracy:0.95 loss: 1.51747 (lr:2.58)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9555 test loss: 1.50763\n",
      "1120: accuracy:0.97 loss: 1.49254 (lr:2.58)\n",
      "1140: accuracy:0.97 loss: 1.49725 (lr:2.58)\n",
      "1160: accuracy:0.97 loss: 1.50404 (lr:2.58)\n",
      "1180: accuracy:0.97 loss: 1.48721 (lr:2.58)\n",
      "1200: accuracy:0.94 loss: 1.52483 (lr:2.58)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9486 test loss: 1.51431\n",
      "1220: accuracy:0.98 loss: 1.49568 (lr:2.58)\n",
      "1240: accuracy:0.96 loss: 1.51221 (lr:2.58)\n",
      "1260: accuracy:0.96 loss: 1.50665 (lr:2.58)\n",
      "1280: accuracy:0.96 loss: 1.50085 (lr:2.58)\n",
      "1300: accuracy:0.96 loss: 1.49653 (lr:2.58)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9468 test loss: 1.51473\n",
      "1320: accuracy:0.95 loss: 1.50316 (lr:2.58)\n",
      "1340: accuracy:0.93 loss: 1.53495 (lr:2.58)\n",
      "1360: accuracy:0.97 loss: 1.4891 (lr:2.58)\n",
      "1380: accuracy:0.96 loss: 1.51012 (lr:2.58)\n",
      "1400: accuracy:0.97 loss: 1.49325 (lr:2.58)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9571 test loss: 1.50499\n",
      "1420: accuracy:0.98 loss: 1.48593 (lr:2.58)\n",
      "1440: accuracy:0.98 loss: 1.48219 (lr:2.58)\n",
      "1460: accuracy:0.94 loss: 1.52137 (lr:2.58)\n",
      "1480: accuracy:0.98 loss: 1.4866 (lr:2.58)\n",
      "1500: accuracy:0.99 loss: 1.48379 (lr:2.58)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9615 test loss: 1.50074\n",
      "1520: accuracy:0.95 loss: 1.51048 (lr:2.58)\n",
      "1540: accuracy:0.95 loss: 1.51348 (lr:2.58)\n",
      "1560: accuracy:0.97 loss: 1.48234 (lr:2.58)\n",
      "1580: accuracy:0.99 loss: 1.47514 (lr:2.58)\n",
      "1600: accuracy:0.97 loss: 1.49383 (lr:2.58)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9605 test loss: 1.50262\n",
      "1620: accuracy:0.97 loss: 1.49237 (lr:2.58)\n",
      "1640: accuracy:0.99 loss: 1.47369 (lr:2.58)\n",
      "1660: accuracy:0.94 loss: 1.51767 (lr:2.58)\n",
      "1680: accuracy:0.98 loss: 1.48196 (lr:2.58)\n",
      "1700: accuracy:0.95 loss: 1.51396 (lr:2.58)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9625 test loss: 1.4998\n",
      "1720: accuracy:0.92 loss: 1.54595 (lr:2.58)\n",
      "1740: accuracy:0.98 loss: 1.4912 (lr:2.58)\n",
      "1760: accuracy:0.95 loss: 1.50877 (lr:2.58)\n",
      "1780: accuracy:0.98 loss: 1.47721 (lr:2.58)\n",
      "1800: accuracy:0.94 loss: 1.51479 (lr:2.58)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9634 test loss: 1.5\n",
      "1820: accuracy:0.99 loss: 1.4775 (lr:2.58)\n",
      "1840: accuracy:0.95 loss: 1.51435 (lr:2.58)\n",
      "1860: accuracy:0.95 loss: 1.51463 (lr:2.58)\n",
      "1880: accuracy:0.93 loss: 1.52721 (lr:2.58)\n",
      "1900: accuracy:0.93 loss: 1.5305 (lr:2.58)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9634 test loss: 1.4996\n",
      "1920: accuracy:0.93 loss: 1.52055 (lr:2.58)\n",
      "1940: accuracy:0.93 loss: 1.53278 (lr:2.58)\n",
      "1960: accuracy:0.98 loss: 1.4864 (lr:2.58)\n",
      "1980: accuracy:0.94 loss: 1.52189 (lr:2.58)\n",
      "2000: accuracy:0.94 loss: 1.52167 (lr:2.58)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9671 test loss: 1.49586\n",
      "2020: accuracy:0.99 loss: 1.47967 (lr:2.58)\n",
      "2040: accuracy:0.99 loss: 1.47311 (lr:2.58)\n",
      "2060: accuracy:0.99 loss: 1.47859 (lr:2.58)\n",
      "2080: accuracy:0.97 loss: 1.4943 (lr:2.58)\n",
      "2100: accuracy:0.96 loss: 1.49681 (lr:2.58)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9675 test loss: 1.49481\n",
      "2120: accuracy:0.91 loss: 1.54984 (lr:2.58)\n",
      "2140: accuracy:0.99 loss: 1.47311 (lr:2.58)\n",
      "2160: accuracy:0.99 loss: 1.47287 (lr:2.58)\n",
      "2180: accuracy:0.99 loss: 1.47497 (lr:2.58)\n",
      "2200: accuracy:0.96 loss: 1.50872 (lr:2.58)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9686 test loss: 1.49469\n",
      "2220: accuracy:0.95 loss: 1.5163 (lr:2.58)\n",
      "2240: accuracy:0.96 loss: 1.49851 (lr:2.58)\n",
      "2260: accuracy:0.97 loss: 1.49392 (lr:2.58)\n",
      "2280: accuracy:0.98 loss: 1.48623 (lr:2.58)\n",
      "2300: accuracy:1.0 loss: 1.46399 (lr:2.58)\n",
      "2300: ********* epoch 4 ********* test accuracy:0.9664 test loss: 1.49631\n",
      "2320: accuracy:0.98 loss: 1.48383 (lr:2.58)\n",
      "2340: accuracy:0.99 loss: 1.47582 (lr:2.58)\n",
      "2360: accuracy:0.95 loss: 1.52072 (lr:2.58)\n",
      "2380: accuracy:0.99 loss: 1.47476 (lr:2.58)\n",
      "2400: accuracy:0.97 loss: 1.49254 (lr:2.58)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9679 test loss: 1.49508\n",
      "2420: accuracy:0.99 loss: 1.47421 (lr:2.58)\n",
      "2440: accuracy:0.99 loss: 1.47138 (lr:2.58)\n",
      "2460: accuracy:0.96 loss: 1.5065 (lr:2.58)\n",
      "2480: accuracy:0.96 loss: 1.49497 (lr:2.58)\n",
      "2500: accuracy:0.98 loss: 1.48714 (lr:2.58)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9669 test loss: 1.49524\n",
      "2520: accuracy:0.99 loss: 1.47091 (lr:2.58)\n",
      "2540: accuracy:0.98 loss: 1.48285 (lr:2.58)\n",
      "2560: accuracy:0.99 loss: 1.47456 (lr:2.58)\n",
      "2580: accuracy:0.95 loss: 1.51386 (lr:2.58)\n",
      "2600: accuracy:0.96 loss: 1.49756 (lr:2.58)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9699 test loss: 1.49249\n",
      "2620: accuracy:0.99 loss: 1.47242 (lr:2.58)\n",
      "2640: accuracy:0.95 loss: 1.51577 (lr:2.58)\n",
      "2660: accuracy:0.95 loss: 1.51728 (lr:2.58)\n",
      "2680: accuracy:1.0 loss: 1.46393 (lr:2.58)\n",
      "2700: accuracy:0.97 loss: 1.49401 (lr:2.58)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9712 test loss: 1.49141\n",
      "2720: accuracy:0.99 loss: 1.47052 (lr:2.58)\n",
      "2740: accuracy:0.98 loss: 1.48182 (lr:2.58)\n",
      "2760: accuracy:0.97 loss: 1.49454 (lr:2.58)\n",
      "2780: accuracy:0.97 loss: 1.49251 (lr:2.58)\n",
      "2800: accuracy:0.95 loss: 1.51274 (lr:2.58)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9664 test loss: 1.49496\n",
      "2820: accuracy:0.98 loss: 1.48129 (lr:2.58)\n",
      "2840: accuracy:1.0 loss: 1.46349 (lr:2.58)\n",
      "2860: accuracy:0.99 loss: 1.47593 (lr:2.58)\n",
      "2880: accuracy:0.97 loss: 1.49151 (lr:2.58)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900: accuracy:0.97 loss: 1.49272 (lr:2.58)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9683 test loss: 1.49413\n",
      "2920: accuracy:0.96 loss: 1.5003 (lr:2.58)\n",
      "2940: accuracy:0.99 loss: 1.47135 (lr:2.58)\n",
      "2960: accuracy:0.95 loss: 1.51582 (lr:2.58)\n",
      "2980: accuracy:0.97 loss: 1.49692 (lr:2.58)\n",
      "3000: accuracy:0.97 loss: 1.49227 (lr:2.58)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.9576 test loss: 1.50513\n",
      "3020: accuracy:0.99 loss: 1.47296 (lr:2.58)\n",
      "3040: accuracy:0.98 loss: 1.48104 (lr:2.58)\n",
      "3060: accuracy:0.97 loss: 1.49416 (lr:2.58)\n",
      "3080: accuracy:0.97 loss: 1.49645 (lr:2.58)\n",
      "3100: accuracy:0.93 loss: 1.53044 (lr:2.58)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9667 test loss: 1.49564\n",
      "3120: accuracy:0.99 loss: 1.4761 (lr:2.58)\n",
      "3140: accuracy:0.96 loss: 1.49888 (lr:2.58)\n",
      "3160: accuracy:0.98 loss: 1.48329 (lr:2.58)\n",
      "3180: accuracy:0.95 loss: 1.50582 (lr:2.58)\n",
      "3200: accuracy:0.97 loss: 1.49229 (lr:2.58)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9701 test loss: 1.49158\n",
      "3220: accuracy:0.97 loss: 1.48747 (lr:2.58)\n",
      "3240: accuracy:1.0 loss: 1.46574 (lr:2.58)\n",
      "3260: accuracy:0.99 loss: 1.48472 (lr:2.58)\n",
      "3280: accuracy:0.96 loss: 1.50521 (lr:2.58)\n",
      "3300: accuracy:0.97 loss: 1.48788 (lr:2.58)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9726 test loss: 1.48934\n",
      "3320: accuracy:0.97 loss: 1.48769 (lr:2.58)\n",
      "3340: accuracy:0.97 loss: 1.49093 (lr:2.58)\n",
      "3360: accuracy:0.96 loss: 1.49973 (lr:2.58)\n",
      "3380: accuracy:0.98 loss: 1.47999 (lr:2.58)\n",
      "3400: accuracy:0.97 loss: 1.49829 (lr:2.58)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9696 test loss: 1.49251\n",
      "3420: accuracy:0.94 loss: 1.51258 (lr:2.58)\n",
      "3440: accuracy:0.98 loss: 1.48348 (lr:2.58)\n",
      "3460: accuracy:0.97 loss: 1.49817 (lr:2.58)\n",
      "3480: accuracy:0.97 loss: 1.48986 (lr:2.58)\n",
      "3500: accuracy:0.99 loss: 1.47227 (lr:2.58)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9721 test loss: 1.48976\n",
      "3520: accuracy:0.96 loss: 1.50176 (lr:2.58)\n",
      "3540: accuracy:1.0 loss: 1.46238 (lr:2.58)\n",
      "3560: accuracy:0.96 loss: 1.49768 (lr:2.58)\n",
      "3580: accuracy:0.99 loss: 1.47282 (lr:2.58)\n",
      "3600: accuracy:0.97 loss: 1.48892 (lr:2.58)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9679 test loss: 1.4941\n",
      "3620: accuracy:0.96 loss: 1.50639 (lr:2.58)\n",
      "3640: accuracy:0.98 loss: 1.47823 (lr:2.58)\n",
      "3660: accuracy:0.99 loss: 1.47545 (lr:2.58)\n",
      "3680: accuracy:0.99 loss: 1.47316 (lr:2.58)\n",
      "3700: accuracy:0.99 loss: 1.46892 (lr:2.58)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9726 test loss: 1.4892\n",
      "3720: accuracy:0.97 loss: 1.4926 (lr:2.58)\n",
      "3740: accuracy:0.98 loss: 1.4898 (lr:2.58)\n",
      "3760: accuracy:0.96 loss: 1.49887 (lr:2.58)\n",
      "3780: accuracy:0.98 loss: 1.47721 (lr:2.58)\n",
      "3800: accuracy:0.98 loss: 1.4817 (lr:2.58)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9723 test loss: 1.48933\n",
      "3820: accuracy:1.0 loss: 1.46293 (lr:2.58)\n",
      "3840: accuracy:0.98 loss: 1.48851 (lr:2.58)\n",
      "3860: accuracy:0.97 loss: 1.49198 (lr:2.58)\n",
      "3880: accuracy:0.98 loss: 1.48666 (lr:2.58)\n",
      "3900: accuracy:0.98 loss: 1.48161 (lr:2.58)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9722 test loss: 1.48928\n",
      "3920: accuracy:0.98 loss: 1.48544 (lr:2.58)\n",
      "3940: accuracy:0.95 loss: 1.50929 (lr:2.58)\n",
      "3960: accuracy:0.99 loss: 1.47079 (lr:2.58)\n",
      "3980: accuracy:0.97 loss: 1.48744 (lr:2.58)\n",
      "4000: accuracy:0.99 loss: 1.47253 (lr:2.58)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.9695 test loss: 1.49224\n",
      "4020: accuracy:0.99 loss: 1.47149 (lr:2.58)\n",
      "4040: accuracy:0.98 loss: 1.48491 (lr:2.58)\n",
      "4060: accuracy:0.98 loss: 1.48142 (lr:2.58)\n",
      "4080: accuracy:0.98 loss: 1.48152 (lr:2.58)\n",
      "4100: accuracy:0.99 loss: 1.47497 (lr:2.58)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9751 test loss: 1.48725\n",
      "4120: accuracy:0.96 loss: 1.50328 (lr:2.58)\n",
      "4140: accuracy:0.99 loss: 1.47085 (lr:2.58)\n",
      "4160: accuracy:0.99 loss: 1.46696 (lr:2.58)\n",
      "4180: accuracy:0.99 loss: 1.47256 (lr:2.58)\n",
      "4200: accuracy:0.99 loss: 1.46699 (lr:2.58)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9721 test loss: 1.49018\n",
      "4220: accuracy:0.97 loss: 1.49041 (lr:2.58)\n",
      "4240: accuracy:0.98 loss: 1.48101 (lr:2.58)\n",
      "4260: accuracy:0.96 loss: 1.49835 (lr:2.58)\n",
      "4280: accuracy:0.99 loss: 1.47601 (lr:2.58)\n",
      "4300: accuracy:0.98 loss: 1.48125 (lr:2.58)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9721 test loss: 1.48976\n",
      "4320: accuracy:0.98 loss: 1.48919 (lr:2.58)\n",
      "4340: accuracy:0.99 loss: 1.47617 (lr:2.58)\n",
      "4360: accuracy:0.95 loss: 1.5122 (lr:2.58)\n",
      "4380: accuracy:0.98 loss: 1.48259 (lr:2.58)\n",
      "4400: accuracy:0.99 loss: 1.47144 (lr:2.58)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9743 test loss: 1.48768\n",
      "4420: accuracy:0.95 loss: 1.51104 (lr:2.58)\n",
      "4440: accuracy:0.98 loss: 1.48173 (lr:2.58)\n",
      "4460: accuracy:1.0 loss: 1.47144 (lr:2.58)\n",
      "4480: accuracy:0.98 loss: 1.48125 (lr:2.58)\n",
      "4500: accuracy:0.97 loss: 1.48786 (lr:2.58)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.9726 test loss: 1.48916\n",
      "4520: accuracy:0.95 loss: 1.50598 (lr:2.58)\n",
      "4540: accuracy:1.0 loss: 1.46345 (lr:2.58)\n",
      "4560: accuracy:1.0 loss: 1.46309 (lr:2.58)\n",
      "4580: accuracy:0.98 loss: 1.48554 (lr:2.58)\n",
      "4600: accuracy:0.99 loss: 1.47129 (lr:2.58)\n",
      "4600: ********* epoch 8 ********* test accuracy:0.9745 test loss: 1.48719\n",
      "4620: accuracy:0.97 loss: 1.49353 (lr:2.58)\n",
      "4640: accuracy:0.98 loss: 1.48872 (lr:2.58)\n",
      "4660: accuracy:0.99 loss: 1.47639 (lr:2.58)\n",
      "4680: accuracy:0.95 loss: 1.51212 (lr:2.58)\n",
      "4700: accuracy:0.98 loss: 1.48212 (lr:2.58)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.973 test loss: 1.4888\n",
      "4720: accuracy:0.97 loss: 1.4902 (lr:2.58)\n",
      "4740: accuracy:0.97 loss: 1.48982 (lr:2.58)\n",
      "4760: accuracy:0.97 loss: 1.4892 (lr:2.58)\n",
      "4780: accuracy:0.98 loss: 1.483 (lr:2.58)\n",
      "4800: accuracy:0.97 loss: 1.49486 (lr:2.58)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9705 test loss: 1.49184\n",
      "4820: accuracy:0.99 loss: 1.47227 (lr:2.58)\n",
      "4840: accuracy:0.97 loss: 1.49131 (lr:2.58)\n",
      "4860: accuracy:0.98 loss: 1.48087 (lr:2.58)\n",
      "4880: accuracy:0.99 loss: 1.48202 (lr:2.58)\n",
      "4900: accuracy:0.94 loss: 1.51503 (lr:2.58)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9735 test loss: 1.48824\n",
      "4920: accuracy:0.99 loss: 1.47589 (lr:2.58)\n",
      "4940: accuracy:0.97 loss: 1.4924 (lr:2.58)\n",
      "4960: accuracy:0.96 loss: 1.50578 (lr:2.58)\n",
      "4980: accuracy:0.99 loss: 1.47665 (lr:2.58)\n",
      "5000: accuracy:0.97 loss: 1.49072 (lr:2.58)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9737 test loss: 1.48869\n",
      "5020: accuracy:0.98 loss: 1.48179 (lr:2.58)\n",
      "5040: accuracy:0.96 loss: 1.50762 (lr:2.58)\n",
      "5060: accuracy:0.96 loss: 1.49195 (lr:2.58)\n",
      "5080: accuracy:0.98 loss: 1.48419 (lr:2.58)\n",
      "5100: accuracy:1.0 loss: 1.46153 (lr:2.58)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9752 test loss: 1.48719\n",
      "5120: accuracy:0.99 loss: 1.47314 (lr:2.58)\n",
      "5140: accuracy:0.99 loss: 1.47121 (lr:2.58)\n",
      "5160: accuracy:0.99 loss: 1.47898 (lr:2.58)\n",
      "5180: accuracy:0.98 loss: 1.47672 (lr:2.58)\n",
      "5200: accuracy:0.98 loss: 1.4737 (lr:2.58)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.9735 test loss: 1.48839\n",
      "5220: accuracy:0.99 loss: 1.47325 (lr:2.58)\n",
      "5240: accuracy:0.97 loss: 1.48433 (lr:2.58)\n",
      "5260: accuracy:0.99 loss: 1.4731 (lr:2.58)\n",
      "5280: accuracy:0.98 loss: 1.48325 (lr:2.58)\n",
      "5300: accuracy:0.98 loss: 1.48538 (lr:2.58)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9714 test loss: 1.49012\n",
      "5320: accuracy:0.97 loss: 1.49273 (lr:2.58)\n",
      "5340: accuracy:0.99 loss: 1.47275 (lr:2.58)\n",
      "5360: accuracy:1.0 loss: 1.46936 (lr:2.58)\n",
      "5380: accuracy:0.99 loss: 1.47134 (lr:2.58)\n",
      "5400: accuracy:0.99 loss: 1.47181 (lr:2.58)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9742 test loss: 1.48717\n",
      "5420: accuracy:1.0 loss: 1.46275 (lr:2.58)\n",
      "5440: accuracy:0.98 loss: 1.48245 (lr:2.58)\n",
      "5460: accuracy:0.99 loss: 1.47318 (lr:2.58)\n",
      "5480: accuracy:0.96 loss: 1.49649 (lr:2.58)\n",
      "5500: accuracy:0.99 loss: 1.47833 (lr:2.58)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9738 test loss: 1.48842\n",
      "5520: accuracy:0.97 loss: 1.48528 (lr:2.58)\n",
      "5540: accuracy:0.99 loss: 1.47205 (lr:2.58)\n",
      "5560: accuracy:1.0 loss: 1.46237 (lr:2.58)\n",
      "5580: accuracy:0.98 loss: 1.48555 (lr:2.58)\n",
      "5600: accuracy:0.98 loss: 1.48957 (lr:2.58)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9741 test loss: 1.48733\n",
      "5620: accuracy:0.99 loss: 1.47317 (lr:2.58)\n",
      "5640: accuracy:0.99 loss: 1.47332 (lr:2.58)\n",
      "5660: accuracy:1.0 loss: 1.46218 (lr:2.58)\n",
      "5680: accuracy:1.0 loss: 1.46907 (lr:2.58)\n",
      "5700: accuracy:0.95 loss: 1.50733 (lr:2.58)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.974 test loss: 1.48783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5720: accuracy:0.97 loss: 1.48847 (lr:2.58)\n",
      "5740: accuracy:1.0 loss: 1.46143 (lr:2.58)\n",
      "5760: accuracy:0.98 loss: 1.48122 (lr:2.58)\n",
      "5780: accuracy:0.99 loss: 1.4709 (lr:2.58)\n",
      "5800: accuracy:0.97 loss: 1.49154 (lr:2.58)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9728 test loss: 1.48874\n",
      "5820: accuracy:0.98 loss: 1.48073 (lr:2.58)\n",
      "5840: accuracy:0.98 loss: 1.48501 (lr:2.58)\n",
      "5860: accuracy:0.96 loss: 1.49939 (lr:2.58)\n",
      "5880: accuracy:0.98 loss: 1.48021 (lr:2.58)\n",
      "5900: accuracy:0.96 loss: 1.49378 (lr:2.58)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9746 test loss: 1.48711\n",
      "5920: accuracy:0.98 loss: 1.48515 (lr:2.58)\n",
      "5940: accuracy:0.98 loss: 1.48203 (lr:2.58)\n",
      "5960: accuracy:0.94 loss: 1.51546 (lr:2.58)\n",
      "5980: accuracy:0.99 loss: 1.46993 (lr:2.58)\n",
      "6000: accuracy:0.99 loss: 1.47251 (lr:2.58)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9752 test loss: 1.48633\n",
      "6020: accuracy:0.99 loss: 1.47288 (lr:2.58)\n",
      "6040: accuracy:1.0 loss: 1.46493 (lr:2.58)\n",
      "6060: accuracy:1.0 loss: 1.4646 (lr:2.58)\n",
      "6080: accuracy:0.97 loss: 1.48832 (lr:2.58)\n",
      "6100: accuracy:0.97 loss: 1.49058 (lr:2.58)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9744 test loss: 1.48735\n",
      "6120: accuracy:0.99 loss: 1.47302 (lr:2.58)\n",
      "6140: accuracy:0.99 loss: 1.46958 (lr:2.58)\n",
      "6160: accuracy:0.99 loss: 1.47765 (lr:2.58)\n",
      "6180: accuracy:0.99 loss: 1.47593 (lr:2.58)\n",
      "6200: accuracy:0.99 loss: 1.47427 (lr:2.58)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9727 test loss: 1.48882\n",
      "6220: accuracy:1.0 loss: 1.46215 (lr:2.58)\n",
      "6240: accuracy:1.0 loss: 1.46492 (lr:2.58)\n",
      "6260: accuracy:1.0 loss: 1.46548 (lr:2.58)\n",
      "6280: accuracy:0.98 loss: 1.48193 (lr:2.58)\n",
      "6300: accuracy:0.97 loss: 1.49286 (lr:2.58)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9744 test loss: 1.48699\n",
      "6320: accuracy:0.99 loss: 1.47049 (lr:2.58)\n",
      "6340: accuracy:0.99 loss: 1.47206 (lr:2.58)\n",
      "6360: accuracy:0.99 loss: 1.47173 (lr:2.58)\n",
      "6380: accuracy:0.99 loss: 1.47216 (lr:2.58)\n",
      "6400: accuracy:0.97 loss: 1.48789 (lr:2.58)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.9749 test loss: 1.48699\n",
      "6420: accuracy:0.99 loss: 1.47393 (lr:2.58)\n",
      "6440: accuracy:1.0 loss: 1.46223 (lr:2.58)\n",
      "6460: accuracy:1.0 loss: 1.46118 (lr:2.58)\n",
      "6480: accuracy:1.0 loss: 1.47019 (lr:2.58)\n",
      "6500: accuracy:0.97 loss: 1.48968 (lr:2.58)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.9766 test loss: 1.48538\n",
      "6520: accuracy:0.99 loss: 1.47305 (lr:2.58)\n",
      "6540: accuracy:0.97 loss: 1.49186 (lr:2.58)\n",
      "6560: accuracy:0.97 loss: 1.48344 (lr:2.58)\n",
      "6580: accuracy:0.97 loss: 1.49101 (lr:2.58)\n",
      "6600: accuracy:0.99 loss: 1.48198 (lr:2.58)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.9756 test loss: 1.48592\n",
      "6620: accuracy:0.98 loss: 1.48423 (lr:2.58)\n",
      "6640: accuracy:0.98 loss: 1.47526 (lr:2.58)\n",
      "6660: accuracy:1.0 loss: 1.46232 (lr:2.58)\n",
      "6680: accuracy:0.97 loss: 1.48926 (lr:2.58)\n",
      "6700: accuracy:0.98 loss: 1.48191 (lr:2.58)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9763 test loss: 1.48561\n",
      "6720: accuracy:0.99 loss: 1.47489 (lr:2.58)\n",
      "6740: accuracy:0.99 loss: 1.47298 (lr:2.58)\n",
      "6760: accuracy:1.0 loss: 1.46241 (lr:2.58)\n",
      "6780: accuracy:0.99 loss: 1.47169 (lr:2.58)\n",
      "6800: accuracy:0.96 loss: 1.50341 (lr:2.58)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9692 test loss: 1.49273\n",
      "6820: accuracy:0.98 loss: 1.4811 (lr:2.58)\n",
      "6840: accuracy:1.0 loss: 1.46158 (lr:2.58)\n",
      "6860: accuracy:0.98 loss: 1.48156 (lr:2.58)\n",
      "6880: accuracy:0.99 loss: 1.47597 (lr:2.58)\n",
      "6900: accuracy:0.99 loss: 1.47221 (lr:2.58)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9762 test loss: 1.48529\n",
      "6920: accuracy:0.99 loss: 1.47507 (lr:2.58)\n",
      "6940: accuracy:0.99 loss: 1.47199 (lr:2.58)\n",
      "6960: accuracy:1.0 loss: 1.46174 (lr:2.58)\n",
      "6980: accuracy:0.97 loss: 1.49259 (lr:2.58)\n",
      "7000: accuracy:0.99 loss: 1.47187 (lr:2.58)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9757 test loss: 1.48596\n",
      "7020: accuracy:0.97 loss: 1.4881 (lr:2.58)\n",
      "7040: accuracy:0.98 loss: 1.48408 (lr:2.58)\n",
      "7060: accuracy:0.99 loss: 1.47125 (lr:2.58)\n",
      "7080: accuracy:0.99 loss: 1.47168 (lr:2.58)\n",
      "7100: accuracy:0.98 loss: 1.48131 (lr:2.58)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9779 test loss: 1.48411\n",
      "7120: accuracy:0.97 loss: 1.48689 (lr:2.58)\n",
      "7140: accuracy:0.96 loss: 1.49968 (lr:2.58)\n",
      "7160: accuracy:1.0 loss: 1.46135 (lr:2.58)\n",
      "7180: accuracy:0.97 loss: 1.4876 (lr:2.58)\n",
      "7200: accuracy:0.96 loss: 1.4999 (lr:2.58)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9769 test loss: 1.48518\n",
      "7220: accuracy:0.95 loss: 1.51404 (lr:2.58)\n",
      "7240: accuracy:0.98 loss: 1.48187 (lr:2.58)\n",
      "7260: accuracy:0.98 loss: 1.47836 (lr:2.58)\n",
      "7280: accuracy:0.99 loss: 1.47561 (lr:2.58)\n",
      "7300: accuracy:0.98 loss: 1.47877 (lr:2.58)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9764 test loss: 1.48565\n",
      "7320: accuracy:1.0 loss: 1.46553 (lr:2.58)\n",
      "7340: accuracy:0.98 loss: 1.47641 (lr:2.58)\n",
      "7360: accuracy:0.99 loss: 1.47117 (lr:2.58)\n",
      "7380: accuracy:1.0 loss: 1.46213 (lr:2.58)\n",
      "7400: accuracy:0.97 loss: 1.49352 (lr:2.58)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9784 test loss: 1.48396\n",
      "7420: accuracy:0.97 loss: 1.48931 (lr:2.58)\n",
      "7440: accuracy:1.0 loss: 1.46143 (lr:2.58)\n",
      "7460: accuracy:0.95 loss: 1.50908 (lr:2.58)\n",
      "7480: accuracy:0.99 loss: 1.47225 (lr:2.58)\n",
      "7500: accuracy:0.99 loss: 1.47141 (lr:2.58)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.977 test loss: 1.48437\n",
      "7520: accuracy:0.99 loss: 1.46769 (lr:2.58)\n",
      "7540: accuracy:0.99 loss: 1.47153 (lr:2.58)\n",
      "7560: accuracy:0.96 loss: 1.49914 (lr:2.58)\n",
      "7580: accuracy:0.99 loss: 1.46937 (lr:2.58)\n",
      "7600: accuracy:0.99 loss: 1.47259 (lr:2.58)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9766 test loss: 1.48476\n",
      "7620: accuracy:0.97 loss: 1.49098 (lr:2.58)\n",
      "7640: accuracy:0.99 loss: 1.47443 (lr:2.58)\n",
      "7660: accuracy:0.99 loss: 1.47197 (lr:2.58)\n",
      "7680: accuracy:0.98 loss: 1.48131 (lr:2.58)\n",
      "7700: accuracy:0.99 loss: 1.47114 (lr:2.58)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9766 test loss: 1.48514\n",
      "7720: accuracy:0.99 loss: 1.47393 (lr:2.58)\n",
      "7740: accuracy:1.0 loss: 1.46438 (lr:2.58)\n",
      "7760: accuracy:1.0 loss: 1.4612 (lr:2.58)\n",
      "7780: accuracy:0.98 loss: 1.48114 (lr:2.58)\n",
      "7800: accuracy:0.99 loss: 1.47066 (lr:2.58)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.977 test loss: 1.48499\n",
      "7820: accuracy:0.97 loss: 1.4913 (lr:2.58)\n",
      "7840: accuracy:0.98 loss: 1.48082 (lr:2.58)\n",
      "7860: accuracy:0.99 loss: 1.46915 (lr:2.58)\n",
      "7880: accuracy:0.98 loss: 1.48567 (lr:2.58)\n",
      "7900: accuracy:0.98 loss: 1.48296 (lr:2.58)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9778 test loss: 1.48403\n",
      "7920: accuracy:1.0 loss: 1.46161 (lr:2.58)\n",
      "7940: accuracy:0.99 loss: 1.47299 (lr:2.58)\n",
      "7960: accuracy:0.97 loss: 1.48896 (lr:2.58)\n",
      "7980: accuracy:0.99 loss: 1.47161 (lr:2.58)\n",
      "8000: accuracy:0.97 loss: 1.48798 (lr:2.58)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9758 test loss: 1.48594\n",
      "8020: accuracy:0.99 loss: 1.47513 (lr:2.58)\n",
      "8040: accuracy:0.98 loss: 1.48533 (lr:2.58)\n",
      "8060: accuracy:0.99 loss: 1.4714 (lr:2.58)\n",
      "8080: accuracy:0.99 loss: 1.46733 (lr:2.58)\n",
      "8100: accuracy:0.98 loss: 1.48391 (lr:2.58)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.972 test loss: 1.4902\n",
      "8120: accuracy:1.0 loss: 1.46134 (lr:2.58)\n",
      "8140: accuracy:1.0 loss: 1.46625 (lr:2.58)\n",
      "8160: accuracy:0.98 loss: 1.48139 (lr:2.58)\n",
      "8180: accuracy:1.0 loss: 1.46573 (lr:2.58)\n",
      "8200: accuracy:1.0 loss: 1.46185 (lr:2.58)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9746 test loss: 1.48779\n",
      "8220: accuracy:1.0 loss: 1.46618 (lr:2.58)\n",
      "8240: accuracy:1.0 loss: 1.46144 (lr:2.58)\n",
      "8260: accuracy:0.99 loss: 1.47546 (lr:2.58)\n",
      "8280: accuracy:0.99 loss: 1.46847 (lr:2.58)\n",
      "8300: accuracy:0.98 loss: 1.48575 (lr:2.58)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9779 test loss: 1.48386\n",
      "8320: accuracy:0.98 loss: 1.48115 (lr:2.58)\n",
      "8340: accuracy:0.99 loss: 1.47735 (lr:2.58)\n",
      "8360: accuracy:0.98 loss: 1.47936 (lr:2.58)\n",
      "8380: accuracy:0.99 loss: 1.46857 (lr:2.58)\n",
      "8400: accuracy:0.96 loss: 1.50296 (lr:2.58)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9763 test loss: 1.48579\n",
      "8420: accuracy:0.97 loss: 1.4924 (lr:2.58)\n",
      "8440: accuracy:0.99 loss: 1.47014 (lr:2.58)\n",
      "8460: accuracy:0.98 loss: 1.48123 (lr:2.58)\n",
      "8480: accuracy:0.99 loss: 1.47085 (lr:2.58)\n",
      "8500: accuracy:1.0 loss: 1.46295 (lr:2.58)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.9795 test loss: 1.48292\n",
      "8520: accuracy:1.0 loss: 1.46294 (lr:2.58)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540: accuracy:1.0 loss: 1.46209 (lr:2.58)\n",
      "8560: accuracy:0.98 loss: 1.47226 (lr:2.58)\n",
      "8580: accuracy:1.0 loss: 1.46185 (lr:2.58)\n",
      "8600: accuracy:0.99 loss: 1.47132 (lr:2.58)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.9782 test loss: 1.48373\n",
      "8620: accuracy:1.0 loss: 1.46192 (lr:2.58)\n",
      "8640: accuracy:0.99 loss: 1.47459 (lr:2.58)\n",
      "8660: accuracy:0.99 loss: 1.47131 (lr:2.58)\n",
      "8680: accuracy:0.98 loss: 1.48155 (lr:2.58)\n",
      "8700: accuracy:0.98 loss: 1.48103 (lr:2.58)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.9762 test loss: 1.48548\n",
      "8720: accuracy:0.98 loss: 1.48114 (lr:2.58)\n",
      "8740: accuracy:1.0 loss: 1.46241 (lr:2.58)\n",
      "8760: accuracy:0.99 loss: 1.47184 (lr:2.58)\n",
      "8780: accuracy:1.0 loss: 1.463 (lr:2.58)\n",
      "8800: accuracy:1.0 loss: 1.46415 (lr:2.58)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9769 test loss: 1.48488\n",
      "8820: accuracy:0.99 loss: 1.4715 (lr:2.58)\n",
      "8840: accuracy:0.98 loss: 1.48135 (lr:2.58)\n",
      "8860: accuracy:1.0 loss: 1.46144 (lr:2.58)\n",
      "8880: accuracy:0.98 loss: 1.48439 (lr:2.58)\n",
      "8900: accuracy:0.99 loss: 1.47129 (lr:2.58)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.977 test loss: 1.4845\n",
      "8920: accuracy:0.99 loss: 1.47117 (lr:2.58)\n",
      "8940: accuracy:1.0 loss: 1.46227 (lr:2.58)\n",
      "8960: accuracy:1.0 loss: 1.4612 (lr:2.58)\n",
      "8980: accuracy:0.99 loss: 1.46853 (lr:2.58)\n",
      "9000: accuracy:0.98 loss: 1.4825 (lr:2.58)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9785 test loss: 1.48266\n",
      "9020: accuracy:0.98 loss: 1.47945 (lr:2.58)\n",
      "9040: accuracy:0.99 loss: 1.47113 (lr:2.58)\n",
      "9060: accuracy:0.98 loss: 1.4824 (lr:2.58)\n",
      "9080: accuracy:0.99 loss: 1.47299 (lr:2.58)\n",
      "9100: accuracy:0.99 loss: 1.47635 (lr:2.58)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.975 test loss: 1.48649\n",
      "9120: accuracy:0.99 loss: 1.47144 (lr:2.58)\n",
      "9140: accuracy:0.96 loss: 1.50114 (lr:2.58)\n",
      "9160: accuracy:0.99 loss: 1.47079 (lr:2.58)\n",
      "9180: accuracy:0.99 loss: 1.47299 (lr:2.58)\n",
      "9200: accuracy:0.95 loss: 1.5063 (lr:2.58)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9773 test loss: 1.48435\n",
      "9220: accuracy:0.96 loss: 1.49978 (lr:2.58)\n",
      "9240: accuracy:0.95 loss: 1.50514 (lr:2.58)\n",
      "9260: accuracy:0.99 loss: 1.47319 (lr:2.58)\n",
      "9280: accuracy:0.99 loss: 1.47899 (lr:2.58)\n",
      "9300: accuracy:0.99 loss: 1.4729 (lr:2.58)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9735 test loss: 1.48806\n",
      "9320: accuracy:1.0 loss: 1.46148 (lr:2.58)\n",
      "9340: accuracy:0.99 loss: 1.47134 (lr:2.58)\n",
      "9360: accuracy:0.99 loss: 1.47303 (lr:2.58)\n",
      "9380: accuracy:0.99 loss: 1.47132 (lr:2.58)\n",
      "9400: accuracy:0.99 loss: 1.47286 (lr:2.58)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9779 test loss: 1.48344\n",
      "9420: accuracy:1.0 loss: 1.46314 (lr:2.58)\n",
      "9440: accuracy:1.0 loss: 1.46126 (lr:2.58)\n",
      "9460: accuracy:0.99 loss: 1.47128 (lr:2.58)\n",
      "9480: accuracy:0.99 loss: 1.47124 (lr:2.58)\n",
      "9500: accuracy:0.99 loss: 1.47023 (lr:2.58)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9784 test loss: 1.48324\n",
      "9520: accuracy:0.97 loss: 1.4893 (lr:2.58)\n",
      "9540: accuracy:0.96 loss: 1.50158 (lr:2.58)\n",
      "9560: accuracy:0.98 loss: 1.48371 (lr:2.58)\n",
      "9580: accuracy:0.98 loss: 1.47966 (lr:2.58)\n",
      "9600: accuracy:0.99 loss: 1.4716 (lr:2.58)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.979 test loss: 1.48232\n",
      "9620: accuracy:1.0 loss: 1.46292 (lr:2.58)\n",
      "9640: accuracy:0.98 loss: 1.47814 (lr:2.58)\n",
      "9660: accuracy:0.99 loss: 1.47489 (lr:2.58)\n",
      "9680: accuracy:0.98 loss: 1.48108 (lr:2.58)\n",
      "9700: accuracy:1.0 loss: 1.46544 (lr:2.58)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9795 test loss: 1.48224\n",
      "9720: accuracy:0.97 loss: 1.49136 (lr:2.58)\n",
      "9740: accuracy:0.99 loss: 1.47197 (lr:2.58)\n",
      "9760: accuracy:0.96 loss: 1.50048 (lr:2.58)\n",
      "9780: accuracy:1.0 loss: 1.46168 (lr:2.58)\n",
      "9800: accuracy:0.99 loss: 1.47013 (lr:2.58)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9774 test loss: 1.48449\n",
      "9820: accuracy:0.98 loss: 1.48191 (lr:2.58)\n",
      "9840: accuracy:0.99 loss: 1.47117 (lr:2.58)\n",
      "9860: accuracy:0.99 loss: 1.47135 (lr:2.58)\n",
      "9880: accuracy:1.0 loss: 1.46121 (lr:2.58)\n",
      "9900: accuracy:0.99 loss: 1.47166 (lr:2.58)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9783 test loss: 1.48321\n",
      "9920: accuracy:1.0 loss: 1.46287 (lr:2.58)\n",
      "9940: accuracy:0.99 loss: 1.47115 (lr:2.58)\n",
      "9960: accuracy:1.0 loss: 1.46264 (lr:2.58)\n",
      "9980: accuracy:1.0 loss: 1.46166 (lr:2.58)\n",
      "10000: accuracy:0.99 loss: 1.47128 (lr:2.58)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9746 test loss: 1.48719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:12<00:00, 39.79s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl81NW9+P/Xe7bMZA8JJIEACTuIiMiiuCEKIqhQ4KqA\neyv1KhartdWqFbW2/q5eW76trZfigooLYBUUFHBBUQRFUWRfAmQhgZBAFrLPnN8fmUmHEDIzySyZ\nzHk+Hnlk8lnOnE8y+bw/ZxelFJqmaZpmCHUGNE3TtPZBBwRN0zQN0AFB0zRNc9IBQdM0TQN0QNA0\nTdOcdEDQNE3TAB0QNE3TNCcdEDRN0zRABwRN0zTNyRTqDPgiJSVFZWZmhjobmqZpYeW77747ppTq\n7Om4sAoImZmZbN68OdTZ0DRNCysicsib43SVkaZpmgbogKBpmqY56YCgaZqmAWHWhqBpmuZJXV0d\neXl5VFdXhzorQWe1WsnIyMBsNrfqfB0QNE3rUPLy8oiLiyMzMxMRCXV2gkYpRXFxMXl5eWRlZbUq\nDV1lpGlah1JdXU1ycnJEBQMAESE5OblNJSMdEDRN63AiLRi4tPW6dUDQNE3TAN2GoAEOh4Oamhpq\namqora2ltraW+vp67HY7SimUUogIIoLBYMBkMmEymbBYLJhMJsxmMxaLBaPRGLFPZprmTkSYNWsW\nr7/+OgD19fWkp6czatQoPvjggxDn7sx0QIhASikqKyspKyujoqKCmpoaDIaGwqLD4fA6HVeQcKUJ\nYDabiYqKwmazYbVaiYqKwmKx6EChRZSYmBi2bdtGVVUVNpuNtWvX0q1bt1BnyyNdZRRBqqurOXz4\nMDt37uTQoUMUFxdTU1MDNAQCX4IBNAQB13mukkRtbS3l5eUcPXqUvLw89u/fz/bt29mzZw+5ubkU\nFxdz8uRJ7HZ7IC5R09qNiRMnsnLlSgDefPNNZsyY0bjv5MmT3H777YwcOZJzzz2X5cuXA3Dw4EEu\nvvhihg0bxrBhw9iwYQMA69atY8yYMUyfPp0BAwYwa9asxocwf9IlhAhw8uRJCgsLqa6uDsiH6Ezc\nA4yrKqqsrAwRweFwYDQasVqtxMTENJYoWtt/WtOac++99/LDDz/4Nc2hQ4fy17/+1eNxN9xwA088\n8QRXX301W7du5fbbb2f9+vUAPPXUU4wdO5aXXnqJEydOMHLkSK644gq6dOnC2rVrsVqt7N27lxkz\nZjTO37Zlyxa2b99O165dufDCC/nqq6+46KKL/HptOiB0YNXV1eTn5wc9ELTEVZIAsNvtnDx5kpMn\nT2IwGBrbKqKiooiOjiY6Ohqr1aqrnLSwNGTIEA4ePMibb77JxIkTT9m3Zs0aVqxYwbPPPgs0/K/m\n5OTQtWtX5syZww8//IDRaGTPnj2N54wcOZKMjAygISgdPHgwNAFBRF4CrgaOKqUGN7NfgPnARKAS\nuFUp9b1z3y3AI85D/6iUWuTcfh7wCmADVgFzVXu5a4U5u91OYWEhJ06caDeBwBNXaUIpRVVVFVVV\nVRw/frwx/xaLBZvN1hgkrFZrY7uHpp2JN0/ygXTttdfym9/8hnXr1lFcXNy4XSnFO++8Q//+/U85\nft68eaSmpvLjjz/icDiwWq2N+6KiohpfG41G6uvr/Z5fb/+jXgEmtLD/KqCv82s28E8AEekEPAaM\nAkYCj4lIkvOcfwJ3uJ3XUvqal06ePMnevXvDKhiciXvbRE1NDSdOnKCgoICDBw+yY8cOdu3axcGD\nBzl69Cjl5eXU1dWF/TVrHcvtt9/OY489xtlnn33K9iuvvJK//e1vjZ/XLVu2AFBaWkp6ejoGg4HX\nXnst6G1tXpUQlFJfiEhmC4dMBl51PuFvFJFEEUkHxgBrlVIlACKyFpggIuuAeKXURuf2V4EpwIet\nvI6Ip5SiqKiIoqKiDn1TdK9yqq+vp6KigoqKisYqJ6Cxl5OrNBEVFaVLE1pIZGRk8Ktf/eq07Y8+\n+ij33nsvQ4YMweFwkJWVxQcffMBdd93FtGnTePXVV5kwYQIxMTFBza94e/NwBoQPzlBl9AHwtFLq\nS+fPnwC/oyEgWJVSf3RufxSoAtY5j7/Cuf1i4HdKqatbysPw4cOVXiDndA6Hg5ycHE6ePNmhg4Gv\nXN1iHQ4HJpMJq9WKzWY7pQFbt010PDt37mTgwIGhzkbINHf9IvKdUmq4p3PbfaOyiMymoRqKHj16\nhDg37U99fT3Z2dm6uqQZ3pQmXG0TriBhtVoxGo2hzLamhYy/AkI+0N3t5wzntnwaSgnu29c5t2c0\nc/xplFILgAXQUELwU347hNraWrKzswPSuNSRuXeHdY3QLi0tPaU7rKvayX2AnS5NaB2dvwLCCmCO\niLxFQwNyqVKqQERWA39ya0geDzyklCoRkTIROR/YBNwM/M1PeYkItbW17N+/Xw/w8pOm3WErKyup\nrKw8ZQS3xWLBarWe0tPJZGr3hWxN85q33U7fpOFJP0VE8mjoOWQGUEq9QEO30YnAPhq6nd7m3Fci\nIk8C3zqTesLVwAzcxX+6nX6IblD2mg4GwdPc4Lry8nJEpHHchKttQo+b0MKdt72MZnjYr4C7z7Dv\nJeClZrZvBk5roNZa5moz0MEgdNxLE655oSorK/W4CS3s6fJuGHE4HBw4cEC3GbRTntomzGYzNpuN\nmJgY3YCttUs6IIQJpRQ5OTnU1taGOiuaD9xLE3V1ddTV1TVWObm6w0ZHR58yn5MuSXQMeXl53H33\n3ezYsQOHw8HVV1/NM888wxtvvMHmzZv5+9//HuosnkZ/8sLE0aNH9TiDDsI1Syw0VAGWlZVRWFjY\nOAJ7z5495OXlcfz48XY1D1WHtXgxZGaCwdDwffHiNieplGLq1KlMmTKFvXv3smfPHioqKnj44Yfb\nnHYg6YAQBsrKyjh27Ji+MXRg7kGitraWEydOcPjwYbKzs9mxYwf79++nsLCQ8vJy3X7kT4sXw+zZ\ncOgQKNXwffbsNgeFTz/9FKvVym233QY0zD30l7/8hZdeeonKykpyc3MZM2YMffv25fHHHwcapp2Z\nNGkS55xzDoMHD+btt99u8+X5SlcZtXO1tbXk5eXpYBCB3KubXBP+lZSUoJTCZDIRExPT+KVHXbfS\nww9DZeWp2yorG7bPmtXqZLdv38555513yrb4+Hh69OhBfX0933zzDdu2bSM6OpoRI0YwadIkDh06\nRNeuXRvXUCgtLW31+7eWLiG0Y0opDh065PPCNVrH5Zrwr66urrEUsXfvXnbt2sWhQ4coKSmhpqZG\nP0B4KyfHt+1+Mm7cOJKTk7HZbEydOpUvv/ySs88+m7Vr1/K73/2O9evXk5CQENA8NEcHhHassLBQ\nNyJrLXKVIux2O+Xl5RQUFLBv3z527tzJwYMHdYDw5EzT4bRxmpxBgwbx3XffnbKtrKyMnJwcTCbT\naaU5EaFfv358//33nH322TzyyCM88cQTbcpDa+iA0E6dPHmysXpA07zlChAOh4OKiorGALFr1y5y\ncnI4fvw4dXV1oc5m+/HUUxAdfeq26OiG7W1w+eWXU1lZyauvvgo0jH6///77ufXWW4mOjmbt2rWU\nlJRQVVXFe++9x4UXXsjhw4eJjo7mxhtv5IEHHuD7779vUx5aQweEdsjhcJCbm6uDgdZm7iWIsrIy\nDh8+zJ49e9i9ezeHDx+mvLw8sqskZ82CBQugZ08Qafi+YEGb2g+g4Yn/3XffZenSpfTt25d+/fph\ntVr505/+BDSsfjZt2jSGDBnCtGnTGD58OD/99BMjR45k6NChPP744zzyyCMe3sX/vJ7+uj2IlOmv\n8/PzQ7bATUlJCYcOHSInJ4eSkhLKy8spKyvDbrdjNpsxmUzExcWRnJxM586d6dq1K5mZmaes5qSF\nD9fMr1arlfj4eOLi4sJ+Ij89/XUHnv460lRWVgY1GBw+fJgvvviCzZs3s3XrVgoKCk7ZbzQaiY2N\nxWQyUV9fT21tLVVVVaccYzAY6NGjB/369WPYsGGce+659OvXT0/8FgZcpYOqqiqqq6s5evQoIkJc\nXBzx8fHExsbq0dQRRP/HtiPBqioqLi7mgw8+4P3332f37t0ApKenc8455zBr1iyysrLo3r07qamp\n2Gy2054W6+rqKC4u5tixY+Tl5bFv3z7279/PTz/9xJo1awCIiYlhxIgRjBkzhksvvZSUlJSAXpPW\ndu7zM5WWllJeXo5SiqioKOLj44mPjw/70oPWMh0Q2pFjx44FdJ6i3bt38+KLL7J27Vrq6+sZMmQI\n999/P5deeilZWVlep2M2m0lLSyMtLY3Bg0+dn7CgoIAtW7bw3XffsX79etatWwfA2WefzVVXXcVV\nV12lg0OYcJUeqqurqampoaioCIPBcErpob1Os+GaiTbStPVhUrchtBO1tbXs3bs3IKWDffv2MX/+\nfNatW0d0dDRTp05l2rRp9OnTx+/v5U4pxd69e/n888/5+OOP2bFjBwaDgfPPP5+rr76ayy+/nOim\nPTy0sOBqe7DZbCQkJBAXF4fFYgl1tgA4cOBAYztXJAUFpRTFxcWUl5ef9oDnbRuCDgjtxMGDB6mo\nqPBrmidOnODvf/87y5YtIzo6mptvvpkZM2aEZMALQHZ2NitXrmTlypXk5+cTFxfH5MmTue6663wq\noWjti+umazKZGquWoqOjQ3YzrqurIy8vj+rq6pC8fyhZrVYyMjIwm82nbNcBIYxUVFRw6NAhv5YO\nPv74Y5588klKS0u57rrr+O///m+SkpI8nxgESim+++47li5dypo1a6ivr2fUqFFcf/31XHbZZbox\nOsy5Sg+xsbGNpQfdMB1aOiCECVe1ir9GJFdUVPDEE0/w4YcfMnDgQJ588kn69+/vl7QD4dixY7z3\n3nssWbKEgoICunXrxs0338yUKVNI/+wzUufPx1xYSF1aGkfmzqV00qRQZ1nzgSs4REVFkZCQ0Ngw\nrQWXDghhoqSkhIKCAr+UDnbv3s39999PXl4ed955Jz//+c9PKzq2V3a7nXXr1vHKK6/www8/8HOb\njefr6ohya2R3WK3kz5ung0KYcq9aSkhIICEhAavVGlH1/KGiA0IYcDgc7Nq1yy8jRT/88EMeffRR\n4uPjeeaZZ06baTGcbNmyhXF33EFaTc1p+2rT09nj7NqqhTeDwYCIkJCQQGJiYrNdnDX/8DYgeNVn\nTEQmiMhuEdknIg82s7+niHwiIltFZJ2IZDi3XyYiP7h9VYvIFOe+V0TkgNu+ob5eZLjzxxoHSikW\nLlzIb3/7W8466yyWLFkS1sEA4NxzzyX1DFVo5sLCIOdGCxSHw4HdbqekpISDBw+ya9cuDh8+TFVV\nlZ62JUQ8BgQRMQLPA1cBg4AZIjKoyWHPAq8qpYYATwB/BlBKfaaUGqqUGgqMBSoB98e7B1z7lVI/\ntP1ywkd9fT1FRUVt+uA7HA7++Mc/Mn/+fK666ir+9a9/+bWPf8LKlfQbP56zhgyh3/jxJDjnaQ+G\nurS0ZrfnKMVDDz1EdnZ20PKiBZ57cMjOzmbPnj0UFRXp9cODzJsSwkhgn1IqWylVC7wFTG5yzCDg\nU+frz5rZDzAd+FApVdnMvohTVFTUpvPtdjuPPvooS5Ys4bbbbuPpp5/2az/whJUr6TZvHpaCAkQp\nLAUFdJs3L2hB4cjcuTis1lO22aOi+Ojii/nkk0+YMmUKDzzwAHv37g1KfrTgca33cPToUXbv3t3Y\nJVuXGgLPm4DQDch1+znPuc3dj8BU5+ufAXEiktzkmBuAN5tse8pZzfQXEWm264GIzBaRzSKyua03\n0ZBpsmZr/auvtmlq6/r6eh5++GFWrFjB3XffzX333ef3EaOp8+djaNKP21BdTer8+X59nzMpnTSJ\n/HnzqE1PR4lQm57O4ccf58J//IOPPvqI22+/nS+++IKpU6dy3333sWvXrqDkSwse10ytFRUV5OTk\nsGfPHo4fPx7Zs7MGmMdGZRGZDkxQSv3C+fNNwCil1By3Y7oCfweygC+AacBgpdQJ5/50YCvQVSlV\n57atELAAC4D9SqkWV4QIy0Zl15qtbsv0OWw28h97rFW9ZZRSPPbYY7z77rvMnTuXX/ziF/7MbaOz\nhgxBmvlsKBG2b90akPf0VWlpKa+99hqLFy+moqKCMWPG8Mtf/vK06TS0jsPVEN25c2c6derUbqfO\naG/82aicD3R3+znDua2RUuqwUmqqUupc4GHnthNuh1wHvOsKBs79BapBDfAyDVVTHU8za7Yaqqpa\n/aT9l7/8hXfffZc777wzYMEAzlyHf6btoZCQkMCcOXNYvXo1d999N99//z0zZszgzjvvPG21Kq1j\ncLU1HDlyhF27dlFcXKyrkvzIm4DwLdBXRLJExEJD1c8K9wNEJEVEXGk9BLzUJI0ZNKkucpYQkIZ+\nZlOAbb5nPwycYW3W1vSWWbRoES+//DLXX389d911V1tz1qLm6vAdVitH5s4N6Pu2Rnx8PHfeeSdr\n1qzh17/+NTt37uTWW2/l1ltvZcOGDfqG0QG5VoUrLCxk9+7dlJeXhzpLHYLHgKCUqgfmAKuBncAS\npdR2EXlCRK51HjYG2C0ie4BUoHH9ORHJpKGE8XmTpBeLyE/AT0AK8Mc2XUl7dYa1WX190v7iiy/4\n3//9X8aNG8dDDz0U8P7azdXht/dBYTExMdx+++189NFHPPjgg+Tm5vLLX/6SWbNmsW7dOh0YOiCl\nFPX19eTk5HDgwAG9Bnkb6YFpgdZcG4KPI26zs7OZOXMmPXr0YNGiRdhstkDltkOpra1l+fLlvPji\ni+Tn59O/f3/uuOMOrrjiCj23TgclIqSmpkbcTKee6JHK7cnixajf/x5yc32ek6e0tJSZM2dSUVHB\n22+/TVo7qsMPF/X19Xz44YcsWLCAgwcPkpmZyU033cQ111yjg2sHJCJYrVa6d+/ebqbkDjUdENqZ\nI0eO+Dwy2eFwcNddd/HNN9/w0ksvMXRoxA3m9iu73c7atWt5+eWX2bFjB4mJiVx//fXccMMNetGe\nDshgMJCRkUF8fHyosxJyfp26Qmsbh8PRqt4Qr7zyCl999RUPPvigDgZ+YDQamTBhAm+99RYvv/wy\nQ4cOZcGCBYwfP55HH32UPXv2hDqLmh+5lqT11+SRkUCXEIKguLiYI0eO+DSgZuvWrdxyyy2MHTuW\nZ599VteHBsjBgwd5/fXXWb58OdXV1QwbNoz/+q//Yty4cXqa5g5CRLDZbPTs2TNi2450lVE7oZRi\n9+7dPs3JUlZWxnXXXYdSiqVLl+oibxCcOHGCd999l6VLl5Kbm0tiYiKTJ09m+vTpZGZmhjp7WhuJ\nCCaTiaysrIhsV9ABoZ0oKysjLy/Pp9LBb3/7W9asWcOiRYs455xzApg7rSmHw8GmTZtYunQpn332\nGfX19QwbNoyrr76a8ePHh2z5Uc0/jEYjWVlZWJuMsenodEBoJ/bt2+fT2q4ff/wxv/71r5kzZw6/\n/OUvA5gzz0QEEWmcU6a5fUCHnVumqKiI5cuXs2LFCg4cOIDZbOaSSy5h0qRJXHzxxRF3U+koDAYD\nmZmZREdHhzorQaMDQjtQVVVFdna21w1aJ06cYPLkyaSmprJ48eKgr3ZmMBhwOBxERUURExODzWYj\nKioKk8mEyWRqDABKKex2O/X19dTW1lJdXc3JkyepqqpCRDpcgFBKsXPnTj744ANWrVpFcXExNpuN\n0aNHM3bsWC655BISExNDnU3NByJCZmYmMTExoc5KUOiA0A7k5ORQVlbm9fEPPfQQH330EW+99VbQ\n1kF23eRjY2NJTEwkNja21Q1vSimqqqooKyvjxIkT2O32Dte7o76+nm+++YZPP/2Uzz77jKNHj2I0\nGhk2bBijR49m1KhRDBo0KGIbL8OJiJCVlRURJQUdEEKsrq6OPXv2eH1D/Pzzz5kzZw533nknd999\nd4Bz11AaMBgMpKSkkJSU5PcbmCs4HDt2rHGemXD6rHlDKcWOHTv45JNP+Pzzzxu7rcbFxTF8+HBG\njRrFOeecQ79+/SKyITMcGAwGsrKyOvwARR0QQqywsNDrsQeVlZVcc801xMfHs2TJkoBWFbl6W6Sl\npREfHx+U7qz19fUcO3ascQ2IcPrM+aK4uJhvvvmGTZs2sXHjRvLzGyYFNpvNDBgwgMGDBzNo0CB6\n9+5Nr169Iqa6or0zGAz07t27Q3cz1gEhhBwOB7t27fK6Lv25557j5Zdf5rXXXgvYADRXI3BaWhpJ\nSUkhGddgt9s5duyYX9aSDgcFBQX89NNPbNu2jW3btrF9+3Yq3ea06tKlC7169SIzM5O0tDRSU1NJ\nS0sjLS2Nzp07h/QG5Wonck037fosWyyWU9qTOgqTyUSfPn0wmUyhzkpA6IAQQidOnODw4cNeBYTs\n7GymTZvGpEmT+OMfAzPhq4iQkJBAenp6u6jbrq+vp7CwkNLS0ogIDC52u52cnByys7M5cOAA2dnZ\nZGdnk5OT0+z0zVarlfj4eOLj40lISCA6OpqoqCgsFkvjd1dp0uFwNJa+XF92u53a2lpqamqora1t\n9nVdXR01NTWNr+vq6hrTOhMRwWKxNOYjNjaWGUrxq8JCutTUcDw2lnXjx3P8qqvo1q0baWlpfiv1\nJqxcSer8+ZgLC32eF8yTqKgoevfu3SEX3dEBIYT27t1LTU2Nx+OUUsyePZvt27fz/vvvk5zcdNXR\nthERDAYD3bt3JzY21q9p+0NVVRV5eXnU1tZGVGBoTmVlJYWFhRQWFnLkyBGKi4spLS095auqquq0\nm3pdXV3j07rr7+0qDRqNxlNu3J5euwKMq33JYDBgNBobvwON7+seVM7bvZu527djc3sAOgncQcMi\nKGazmT59+tC/f38GDBjA2WefzcCBA30OEq51vt2XdvV15uCWiAixsbH06NGjw5WAvA0IHbN8FEKu\nf1pvrF27lo0bN/Lggw8GJBjExsaSkZHRLkoFzbHZbPTp04eSkhIKCwsjOihER0fTq1cvevXqFeqs\n+Kzf+PFYmpSGY4D/69SJAXPncvDgQXbv3s0XX3zBe++9BzRc77nnnsuIESO49NJL6d27t8ebcEvr\nfPsjILjWby4qKqJLly5tTi8c6RKCn+Xm5lJaWurxuJqaGq655hri4uJ4++23/Vp3GY5zwtfW1pKb\nm0t1dXVEB4Zw5O3620opioqK2LJlC99++y2bN29m//79AGRmZnLFFVcwbtw4Bg4c2OznNljrfIsI\nPXr0IC4uzm9phpouIYSA3W73etzBG2+8QUFBAU8++aRfg4HBYKBnz55h14PFYrHQq1evxokAdVAI\nH3VpaVgKCprd7k5E6NKlC1deeSVXXnklAEePHuWzzz5rnJZ84cKF9O/fv7FdzX0eL2/fp62UUuTm\n5tKnT5+I6y7c8VpPQuj48eNeHVdaWsq//vUvLrroIkaNGuW39zeZTPTu3TvsgoGLiJCSkkLv3r2x\nWCxhU7qJdG1Zf7tLly5cf/31LFy4kHXr1vHII49gMBj405/+xBVXXMGf//znxu67wVzn2+FwcOjQ\noYh7MPEqIIjIBBHZLSL7ROTBZvb3FJFPRGSriKwTkQy3fXYR+cH5tcJte5aIbHKm+baIhHUoVkp5\nPe7gX//6FxUVFfz617/2y3u7en306dOnQ/Sltlqt9OnTh8TERB0UwoC/1t92LVi0ZMkS3n77bcaP\nH8+SJUuYNGkSDz74ID+dfXZQ1/mura2lsLAwIGm3Vx7bEETECOwBxgF5wLfADKXUDrdjlgIfKKUW\nichY4Dal1E3OfRVKqdO6uIjIEuDfSqm3ROQF4Eel1D9bykt7bkOorKzkwIEDHgNCfn4+11xzDRMn\nTvRLN1MRISoqiqysrHbbeNwWpaWl5Ofnd7j5kTTvFBYW8vrrr7NkyRLq6uqYNm0ad955Z9BWuBMR\nevbs2S576fnCnyumjQT2KaWylVK1wFvA5CbHDAI+db7+rJn9TTMnwFhgmXPTImCKF3lpt7wdbPW3\nv/0Ng8HAnDlz2vyerrVje/Xq1SGDAUBCQoKuQopgaWlp/OY3v2HVqlVMmzaNZcuWMXHiRBYtWuTT\nGiOt5WpPsNvtAX+v9sCbgNANyHX7Oc+5zd2PwFTn658BcSLi6kdpFZHNIrJRRFw3/WTghFLK9Rdt\nLs2wYbfbmx1Y1NS+fftYtWoVM2fOJK2NDWHuJYOOOJDGXVRUFH369AnaVBta+5OSksIjjzzCe++9\nx/Dhw3n22We54YYb2OrH3kVn4nA4yMvLC/j7tAf+upP8BrhURLYAlwL5gCuk9nQWVWYCfxWR3r4k\nLCKznQFlc1FRkZ+y61/eNib/3//9Hzabjdtuu63N72mxWCIiGLi4FkxPS0vTQSGCZWZm8vzzz/Pc\nc89x/PhxbrzxRp577jnq6uoC9p6u8QnedCcPd97cTfKB7m4/Zzi3NVJKHVZKTVVKnQs87Nx2wvk9\n3/k9G1gHnAsUA4kiYjpTmm5pL1BKDVdKDe/cubO31xU03jYm79u3j9WrVzNz5kySkpLa9J6upQA7\najXRmYgIycnJERUItdOJCOPGjWP58uVMnTqVl19+mVmzZpGdnR2w91RKkZ+fH5RqqlDy5r/qW6Cv\ns1eQBbgBWOF+gIikiIgrrYeAl5zbk0QkynUMcCGwQzXcPT8DpjvPuQVY3taLCYXq6mqvPiQvvPAC\nNpuNW265pU3vZzAY6NWrV4edhMsb0dHR9O3bl6ioKF1aiGCxsbHMmzeP+fPnU1hYyPXXX8+KFSs8\nn9hKDoeDw4cPByz99sBjQHDW888BVgM7gSVKqe0i8oSIXOs8bAywW0T2AKnAU87tA4HNIvIjDQHg\nabfeSb8D7hORfTS0Kbzop2sKKm9KB3v37mXNmjXMmjWrTStruVZ5irTBMs0xm8307t1btytojB07\nln//+98MGTKEhx9+mD//+c8Bq0IqLy/3qr0wXOmpK9rA4XCwc+dOjwHhN7/5DevXr2f16tWtDggi\nQrdu3fRSjU24quz06Gatvr6ev/zlL7z66qucd955PPfcc3Tq1Mnv72M0Gunfv39YVVv6s9updgal\npaUen04Lj4jxAAAgAElEQVQPHDjAmjVrmDlzZpuCQXJysg4GzXCNbu7Zs2dY/YNq/mcymXjggQd4\n+umn2bZtGzfddBO5ubmeT/SRw+HgyJEjfk+3PdD/QW1QXFzsccDUokWLsFgs3Hjjja1+H5vNRmpq\naqvPjwSxsbGNc8/oKqTINmnSJBYuXEhpaSk33ngj27dv92v6SilKSkqobjLzakegA0IruRYVaUlR\nURErVqxgypQprZ7e2mg00rNnT32T84Jr+o7Y2Fj9+4pwQ4cO5bXXXmvs5r1x40a/pu/qddTRqil1\nQGil48ePe/wwLF68GLvd3uqeRa5G5EjrXtoWBoOBHj160LlzZx0UIlxWVhavv/46GRkZzJkzhw0b\nNvg1/erqaq9nNw4XOiC0glLK42C0iooKlixZwrhx4+jevXuLxzbHtaaBzWZrbTYjlmua5R49euh2\nhQiXkpLCiy++SGZmJvfccw9fffWV39JWSnm9VG640P8trXDy5EmPpYNly5ZRXl7e6lHJNpvN76uo\nRZq4uDjdrqCRlJTEwoUL6dWrF/fcc49fSwoOh4P2OoNCa+iA0AqeGpPr6up47bXXGDVqFGeddZbP\n6bvWQdY3sbZztSvExcXp32cES0xMbAwK9957r9/mQFJKcezYsYBOnRFMOiD4yG63U1FR0eIxa9eu\n5ejRo61qOxARMjIyfF6AXDszV4DV8yBFtoSEBF544QWSk5O56667/DbVhVKKgmZWcgtHOiD4yJsJ\nrhYvXkxmZiYXXnihT2mLCHFxcacsG6j5h2ssh2vaDx0YIlNKSgoLFizAZDIxe/Zsvy2AU15eTlVV\nlV/SCiUdEHzkaaqKn376ia1btzJjxgyfGzRFhK5du7Y1i1oLbDYbffv2JSYmRgeFCNW9e3deeOEF\nTp48yZw5c6isrGxzmq4G5nCnA4IPampqqK2tbfGYN954g5iYGCZPbnGNoNO4qooiedK6YHGN7dBV\nSJFrwIABPPPMM+zdu5eHHnrILz2FqqurPVYnt3c6IPjA09iDY8eO8dFHHzFlyhSfF7qPiYnRVUVB\n5KpC0quxRa6LLrqIBx54gE8//ZS//e1vbU7PVUoI58FqOiB4yZuxB0uXLqW+vp4ZM2b4lLZr4jot\n+KxWK3369KFTp046KPiZiJzy1R7NmjWL6dOns3DhQlauXNnm9Orr68N6sJqun/BSZWVli5G/rq6O\nJUuWcNFFF9GzZ0+v03UNQNO9ikLHYDCQnp5OQkICOTk52O32sH7KCyaDwYBSCqUUZrMZi8WC2WzG\nbDZjNBob29GUUjgcDurq6hqrXuvq6k45PxREhN///vdkZ2fz+OOPM2DAAHr39mlRx1M4HA4KCgrC\ndlp2HRC8VFJS0mI946effsqxY8d8Lh2YzWY9AK2diI6Opl+/fhw5coSSkhIdFJphMBhwOBxERUUR\nExNDTEwMVqu1VdVuDoeDmpoaKioqKCsra5wsLti/d7PZzLPPPsv06dO57777ePPNN4mOjm51ena7\nnePHjwdk6u1A01VGXnA4HB6LgUuXLiU9Pd2nrqauqqJwfJLoqFylhd69e+sV2fhPtY/JZKJTp050\n796dQYMG0bdvX7p27UpCQkKrf08GgwGbzUbnzp3p3bs3AwYMoGvXrlit1qD/3jt37sz//M//cPDg\nQR5//PE2BSWlFEeOHAnLKS10QPBCeXl5ix/QnJwcNm3axLRp03yaiC42NtbnxmctOFxtC+np6RgM\nhogKDK4gYLVa6dKlC3369Gm8WcfFxQVsfiij0UhSUhJ9+vShd+/eJCQkBPX3PmrUKO666y5WrVrF\n0qVL25SWw+Hw2ObYHukqIy94mqpi2bJlGI1Gfvazn3mdpoiQnp7uj+xpASIidOrUiYSEBI4ePdqh\nq5FcN96oqCg6depEfHx8SLtAW61WunfvTm1tLUeOHKGsrCwov/s77riD77//nmeeeYYRI0aQlZXV\nqnSUUhw9epSkpKSwmmAxfHIaIvX19S2OQKyrq2P58uVceumldOnSxas0XV0e9drI4cFoNJKenk7f\nvn2D/tQaSO4lgbS0NPr379/Y46q9jIexWCx0796d3r17Y7PZAv67NxgMPPnkk1itVh588ME2zVHk\ncDgoKSnxY+4Cz6uAICITRGS3iOwTkQeb2d9TRD4Rka0isk5EMpzbh4rI1yKy3bnverdzXhGRAyLy\ng/NrqP8uy39OnDjR4v5PPvmEkpISpk+f7nWaIkLnzp3bmjUtyFw3p759+5KYmNiuu1OeiSvPUVFR\npKam0q9fP/r06UNycnK7CQLNsVqt9OrVi65duwb8ibtLly489thj7Nixg3/+85+tTkcpRVFRUVi1\nJXj8BIiIEXgeGAfkAd+KyAql1A63w54FXlVKLRKRscCfgZuASuBmpdReEekKfCciq5VSrrvsA0qp\nZf68IH/zVE2wbNky0tPTGT16tFfpGQwGUlNT9aI3YcxisZCRkUFqairFxcWNT4Ht9R/fvTooMTGR\nhISEsOzmLCIkJSURGxtLbm4uVVVVAatGuuKKK/jZz37Giy++yEUXXcSwYcNalY5r/FK49CT0JtSO\nBPYppbKVUrXAW0DTeRkGAZ86X3/m2q+U2qOU2ut8fRg4CoTNo3FNTU2LRcbc3Fw2bdrE1KlTvb7B\nGwyGsOyOpp3ObDaTlpbW2ODqqtJoD6UGV0N4TEwMXbt2bawOSklJCctg4M5sNpOVlUVqampAf9e/\n+93v6Nq1K48++mirJ65zOBwcPXq03T4sNOVNQOgG5Lr9nOfc5u5HYKrz9c+AOBE5JSSKyEjAAux3\n2/yUsyrpLyIS5VPOg8DTVBUrVqxARJgyZYpX6bkaktvDDUPzH4PBQGJiIr1796Zfv3506dKlsStm\nsBoUXQHAYrGQnJxMjx49GDhwIFlZWSQlJbXr6qDWEBFSUlLo1atXwErbMTExPP744+Tk5PCPf/yj\n1ek4HA6PVc/thb8+rb8BLhWRLcClQD5gd+0UkXTgNeA2pZQrVD4EDABGAJ2A3zWXsIjMFpHNIrI5\nmCsTeZqqwuFw8P777zNq1CjS0tK8StNsNuv5ijo4s9lM586d6du3L/3796dbt24kJiY2PpW3tQur\nK8i4SiKulfUyMjIYMGAA/fr1Iz09ndjY2LDq3dJartlrAzV2YeTIkUyfPp1XX32Vbdu2tSoNV4+j\ncOih5s1jQz7gvihwhnNbI2d10FQAEYkFprnaCUQkHlgJPKyU2uh2jmtFiRoReZmGoHIapdQCYAHA\n8OHDg/Yb9TRVxXfffUd+fj5z5szxKj1dOog8JpOJhIQEEhISgIYRrNXV1dTU1FBTU0N1dTX19fXY\n7XYcDscpUzi4bvgGgwGj0YjJZMJisTSOCo6KisJsNuvPEw2/5169epGfnx+Q7qn33XcfX3zxBX/4\nwx94++23W1XlZrfbKS0tJTEx0a958zdvAsK3QF8RyaIhENwAzHQ/QERSgBLn0/9DwEvO7RbgXRoa\nnJc1OSddKVUgDZ/oKUDrwm+AHD9+vMV6v+XLlxMTE8Pll1/uVXoWi4XY2Fh/ZU8LQ0ajsXG6B82/\nDAYDGRkZHDlyxOOaJb6Ki4vj0Ucf5Z577mHhwoX893//t89puEYvt/duyx7LlEqpemAOsBrYCSxR\nSm0XkSdE5FrnYWOA3SKyB0gFnnJuvw64BLi1me6li0XkJ+AnIAX4o78uqq0cDkeLK6NVVlaydu1a\nxo8fj81m85iewWDQc+9rWoCJCGlpaQFpbB4zZgxXXnklCxcuJDc31/MJzbDb7ZSXl/s1X/7mVUuT\nUmoVsKrJtj+4vV4GnNZ9VCn1OvD6GdIc61NOg6iiogIROeNTxieffEJlZSXXXntts/ubMpvNunSg\naUGSkpKC0Wj0+9oEDzzwAOvXr+fpp5/m73//e6sm8zty5AhxcXHt9uGw47c6tYKnmU2XL19ORkaG\nV32TXeMO2usHQNM6oqSkJLp27erX/7vU1FTuuusuvvjiC9atW9eqNOrq6jh58qTf8uRvOiA0Ybfb\nW/yDFRQU8M0333Dttdd61YvDaDQSFxfnzyxqmuaFQASFmTNn0qdPH55++ulWjU1wlRLaKx0Qmmip\n7QDg/fffRynFNddc4zEtXTrQtNBKSkrya/ud2Wzm97//PYcPH+bFF19sVRrV1dWtHugWaDogNNHS\nVBVKKd5//33OO+88MjIyPKYlIo1dDjVNC43k5GQ6d+7st6AwYsQIJk6cyCuvvEJBQYHnE5pwjUto\nj3RAcFNbW0tNTc0Z9+/evZuDBw8yadIkj2m5JrDTpQNNC70uXbqQlJTkt//He++9F4D58+e36vyK\nigpqa2v9khd/0gHBjafqolWrVmEymRg3bpzHtFxz6Wua1j64RnD7Iyikp6dz8803s3LlSn766Sef\nz2+vpQQdENx4qi766KOPuOCCCzyONnQFg0iYOkDTwoWI0L17d6Ki/DNt2s9//nOSk5N55plnWtW9\ntbS0lPr6er/kxV/0HcvJNY3Amfz4448UFBRw1VVXeZVeuEx3q2mRxGAwkJmZ6ZcJ8WJiYrjnnnvY\nsmULa9eubVUax44da3M+/EkHBCdPM5t++OGHREVFcdlll3lMKy4uLuynGNa0jspkMpGVleWXqqMp\nU6bQr18//vrXv/q8uppSyuOYp2DTAYGGP0xL09Pa7XZWr17NJZdc4nHEsWtaXk3T2i/Xms1tDQpG\no5G5c+eSm5vLe++95/P5nmZVDjYdEPA8s+nmzZspLi5mwoQJHtOyWCxER0f7M3uapgVAfHw8ycnJ\nbQ4KF198MUOHDuWFF16gurrap3Ndy2y2l6mxdUDA81QVH374IdHR0VxyySUtpmMwGPRayZoWRlJT\nU72aoLIlIsKvfvUrjh49yttvv+3z+Q6Hg7KysjblwV8iPiB4+mPU1dWxdu1axo4di9Vq9ZieXgBH\n08KHiNCjR482NzKPGDGC0aNHs3DhQioqKnw617XMZnsQ8QHBNbPpmWzYsIGysjKvehclJSXprqaa\nFmZMJhM9evRoc9XRr371K06cOMFrr73m87m1tbVUVla26f39IeLvXp6qi9asWUNcXBwXXHBBi+no\ngWiaFr5iYmLaPLPAWWedxRVXXMGiRYt8XkO5vQxUi+iA4Glm07q6OtatW8dll13msRup1Wr124AX\nTdOCr3Pnzl5VC7dkzpw5VFZW8sorr/h87smTJ0M+nUVEBwRPDTnffvstZWVlXHHFFS0eZzAYdFdT\nTQtzrvaEtlT79u7dmyuvvJI333zT41Q4Tbl6HIVSRAcET2uvfvzxx9hsNo/VRYBe80DTOgCz2Uy3\nbt3aVHX0i1/8gsrKSt544w2fzz1x4gR2u73V791WXgUEEZkgIrtFZJ+IPNjM/p4i8omIbBWRdSKS\n4bbvFhHZ6/y6xW37eSLykzPN/ydBnha0rq6uxZlN7XY7n376KZdcconHYmRiYqJuTNa0DiIhIaFN\ny1z279+fyy67jNdff93nHkfQ8KAaKh7vYiJiBJ4HrgIGATNEZFCTw54FXlVKDQGeAP7sPLcT8Bgw\nChgJPCYiSc5z/gncAfR1fnke9eVHnhp9fvjhB4qLiz1WF+nGZE3reLp169amh7zZs2dTVlbm87gE\npRTHjh0L2XQW3lzxSGCfUipbKVULvAVMbnLMIOBT5+vP3PZfCaxVSpUopY4Da4EJIpIOxCulNqqG\nOptXgSltvBaftDSzKTRUF1ksFi6++OIW0zGbzW1uiNI0rX0xGo1kZGS0upQwePBgLrzwQl599dVW\nrY7ma/uDv3gTELoBuW4/5zm3ufsRmOp8/TMgTkSSWzi3m/N1S2kGjKeZTZVSfPLJJ4wePZqYmJgz\nHqdLB5rWccXFxREfH9/qoDB79mxKSkpYtmyZT+e5BqqFYjoLf1V8/wa4VES2AJcC+YBfWkZEZLaI\nbBaRzf5qgT9x4kSLv+wdO3ZQUFDgsboI8Lg2gqZp4atr166trjoaNmwYI0aM4OWXX/a5O6mnLvGB\n4s2V5gPd3X7OcG5rpJQ6rJSaqpQ6F3jYue1EC+fmO1+fMU23tBcopYYrpYb7Y54gb2YXXLt2LSaT\niTFjxrR4XHR0NCaTqc150jStfTIajW3qdfSLX/yCoqIiVq5c6dN5oZrOwpuA8C3QV0SyRMQC3ACs\ncD9ARFJExJXWQ8BLztergfEikuRsTB4PrFZKFQBlInK+s3fRzcByP1yPR1VVVS2WDpRSfPzxx4wY\nMYKEhIQzHmcwGHR1kaZFgPj4+BarjltywQUX0K9fPxYtWuRzFVBVVZXPs6e2lceAoJSqB+bQcHPf\nCSxRSm0XkSdE5FrnYWOA3SKyB0gFnnKeWwI8SUNQ+RZ4wrkN4C5gIbAP2A986K+LaomnqSqys7M5\ndOiQx+oipZQee6BpEaK1pQQR4dZbb2X//v18+eWXPp0bioFqXtV3KKVWAauabPuD2+tlQLMtJ0qp\nl/hPicF9+2ZgsC+ZbSullMfRyZ999hmAx+qiuLg4PfZA0yKE2WwmNTWVI0eO+PykP2HCBP7617+y\naNEij70WmyorK6O+vj5oVdMRdUfzZpDI559/zqBBg+jSpcsZj9HVRZoWeZKTk1u1NK7ZbGbWrFls\n2rSJnTt3+nx+MNddjqiA4Km6qLi4mB9//NFj6QBodZ2ipmnhSURaPTZh+vTpREdHs2jRIp/OC/a6\nyxETEOx2u8cSwvr161FKeQwICQkJflmgW9O08BIdHd2qRbDi4+OZNm0aH330EYWFhT6dG8x1lyMm\nIJSXl3u8ia9bt47U1FQGDBhwxmMMBoMee6BpESw9Pb1VD4Q33ngjAK+//rpP5wVz3eWICQieil01\nNTVs2LCBMWPGePxjR0dH+zt7mqaFCZPJRJcuXXwOCl27dmXcuHH8+9//9nl1NIfD0aqJ8nwVMQHB\n03wi33zzDVVVVVx66aUtHqerizRNS05ObtU6zDNnzqS8vNzngWpKKerq6nx+P19FTEDwVNz6/PPP\nsdlsjBw58ozHGAyGFgeraZoWGQwGA127dvX54XDo0KEMGDCAN954IyRzFXkSMQGhJUop1q1bx+jR\noz0ug6l7F2maBg1jkXyd6VhEmDlzJvv27WPz5s0Bylnr6YAA7Nq1iyNHjng1GE1XF2maBg0399aU\nEq666ioSEhJ48803A5Sz1tMBgYbeRSLS4ihC3btI07SmbDYbsbGxPp1jtVqZOnUqn376qc9dUANN\nBwQa2g+GDBlCcnLyGY9RSunqIk3TTtOabqjXX389DoeDJUuWBChXrRPxAaG4uJjt27d7nGMkNjZW\nz12kadppLBaLz7UH3bp149JLL+Wdd97xea2EQIr4O9yGDRsAuOiii854jO5dpGlaS1ozLmHGjBmU\nlJSwevXqAOXKdxEfENavX0+nTp0YOHDgGY/RU11rmtYSs9lMp06dfAoKF1xwAT179vR5ic1AiuiA\nYLfb2bBhAxdddFGL1UFWq7VVg1A0TYscvq7oKCJMmzaN77//nuzs7ADlyjcRHRC2bdtGaWlpi9VF\nIqKrizRN88hkMpGcnOxTKeHaa6/FZDK1m1JCRAeEr776CoPBwAUXXNDica2Z3VDTtMjjaykhOTmZ\nyy+/nBUrVlBTUxOgXHkvogPCl19+yeDBg1vsIWA0GrFYLEHMlaZp4cpoNPpcSpg+fTqlpaV8/PHH\nAcyZdyI2IJSUlLBt27YWq4sAXV2kaZpPUlJSfDp+5MiRZGRktItqI68CgohMEJHdIrJPRB5sZn8P\nEflMRLaIyFYRmejcPktEfnD7cojIUOe+dc40XfvOvGZlAGzYsAGllMfRybp3kaZpvjCZTD71ODIY\nDEybNo3Nmzdz8ODBwGbOU148HSAiRuB54CpgEDBDRAY1OewRYIlS6lzgBuAfAEqpxUqpoUqpocBN\nwAGl1A9u581y7VdKHfXD9Xjtyy+/pFOnTgwa1PRS/kOPTtY0rTV8bUuYMmUKJpOJd955J0A58o43\nJYSRwD6lVLZSqhZ4C5jc5BgFuFpeE4DDzaQzw3luyDkcDjZs2MDo0aNb7G4aHR2tJ7PTNM1nJpPJ\np9HLKSkpXHbZZSxfvjykI5e9CQjdgFy3n/Oc29zNA24UkTxgFXBPM+lcDzSd3u9lZ3XRoxLEO+/2\n7ds5fvy47m6qaVrAdO7c2acHyqlTp3L8+HG++OKLAOaqZf5qVJ4BvKKUygAmAq+JSGPaIjIKqFRK\nbXM7Z5ZS6mzgYufXTc0lLCKzRWSziGwuKiryS2a/+uorRITRo0e3eJyvsxhqmqa5WCwWn9ogL7jg\nArp06cLy5csDmKuWeRMQ8oHubj9nOLe5+zmwBEAp9TVgBdyb2m+gSelAKZXv/F4OvEFD1dRplFIL\nlFLDlVLDfa2XO5Ovv/6agQMHkpSUdMZjTCaT7m6qaVqb+FJKMBqNXH311axfv55jx44FOGfN8yYg\nfAv0FZEsEbHQcHNf0eSYHOByABEZSENAKHL+bACuw639QERMIpLifG0Grga2EQSVlZVs3bqV888/\nv8XjdO8iTdPaymaz+bSq2uTJk7Hb7T6vuewvHgOCUqoemAOsBnbS0Jtou4g8ISLXOg+7H7hDRH6k\noSRwq/rPgqGXALlKKffJOqKA1SKyFfiBhhLHv/xyRR5s3ryZ+vr6Fkcn6+6mmqb5S5cuXbyeOr9X\nr14MGTKE9957LyRrLpu8OUgptYqGxmL3bX9we70DuPAM564Dzm+y7SRwno959Yuvv/6aqKgozj33\n3DMeo7ubaprmL7GxsRiNRhwOh1fHT548mSeffJIdO3Zw1llnBTh3p4q4kcobN25k2LBhREVFnfGY\nqKgovRiOpml+ISJ07tzZ63vKlVdeicViCUnjckTd9Y4ePcq+ffv0ZHaapgWVL2MSEhISGDt2LKtW\nrQr6mISICgibNm0CaLFBWbcfaJrmbwaDocVejU1NnjyZ0tJSPv/88wDm6nQRFRC+/vprkpKS6N+/\n/xmPUUr51CtA0zTNG77MghqqMQkRExCUUmzcuJFRo0bp6So0TQs6i8VCdHS0V8cajUYmTZrEV199\nRUlJSYBz9h8RExD2799PUVFRi+0HIqLbDzRNCxhfGpcnTZpEfX09q1evDnCu/iNiAsLXX38N4LFB\nWXc31TQtUGJiYrwOCP3796dv37588MEHAc7Vf0RMQNi4cSM9e/YkPT39jMeISIvdUTVN09pCRHxq\nS7j66qvZunUrOTk5Ac5Zg4gICLW1tXz77bcep6uIiYnR7QeapgWUL72NJk6ciIgEbSqLiAgImzZt\noqqqymP7ge5uqmlaoJlMJq9nUu7/3Xfkm838v+efJ+Gcc2Dx4oDmLSICwtq1azEYDIwYMaLF43T7\ngaZpwZCSkuKxLSFh5Uq6zZtHem0tBsCYlwezZwc0KEREQFi/fj2DBw9usQeRiOjprjVNC4ro6GiP\nASF1/nwM1dWnbqyshIcfDli+IiIgfPTRRzzzzDMkrFxJv/HjOWvIEPqNH0+CW72cbj/QNC1YRIRO\nnTq1eM8xFxY2vyOADcwRERCioqIYuGUL3ebNw1JQgCiFpaCAbvPmkbBypW4/0DQt6Dw1LtelpTW/\no0ePAOSmQUQEBGi++GWoriZ1/nxAtx9omhZcZrMZm812xv1H5s7F0XQanehoeOqpgOUpYgLCmYpf\n5sJC3X6gaVpIJCcnn7EtoXTSJPLnzaM2PR0lgj0jAxYsgFmzApYfrxbI6Qjq0tKwFBQ0u91ms+n2\nA03Tgi4uLq7FldFKJ02idNIkRIT09HQ6deoU0PxETAmhueKXw2rlyNy5uv1A07SQMBgMJCQkhDob\njSKmhFA6aRLQ0JZgLiykLi2NI3PnUn7NNaTo9gNN00KkU6dOlJWVeb3EZiB5FRBEZAIwHzACC5VS\nTzfZ3wNYBCQ6j3lQKbVKRDKBncBu56EblVJ3Os85D3gFsNGwXvNcFeBVpV3Fr1Pyrtc/0DQthGw2\nGwaDoV0EBI9VRiJiBJ4HrgIGATNEZFCTwx4BliilzgVuAP7htm+/Umqo8+tOt+3/BO4A+jq/JrT+\nMlrParXq9gNN00JGRHxaYjOQvGlDGAnsU0plK6VqgbeAyU2OUYBrGHACcLilBEUkHYhXSm10lgpe\nBab4lHM/8XZOEU3TtEBJSkpqFw+m3gSEbkCu2895zm3u5gE3ikgeDdU/97jtyxKRLSLyuYhc7JZm\nnoc0ARCR2SKyWUQ2FxUVeZFd7xkMBj3+QNO0kIuKisJsNoc6G37rZTQDeEUplQFMBF4TEQNQAPRw\nViXdB7whIj4tSaaUWqCUGq6UGt65c2c/ZbeBw+Hwekk7TdO0QGoPpQRvAkI+0N3t5wznNnc/B5YA\nKKW+BqxAilKqRilV7Nz+HbAf6Oc8P8NDmgFnsVi8Xr1I0zQtkNpD91Nv7obfAn1FJEtELDQ0Gq9o\nckwOcDmAiAykISAUiUhnZ6M0ItKLhsbjbKVUAVAmIudLQ0i8GVjulyvyga4u0jStvbBYLCGfMcFj\nQFBK1QNzgNU0dCFdopTaLiJPiMi1zsPuB+4QkR+BN4FbnY3FlwBbReQHYBlwp1KqxHnOXcBCYB8N\nJYcP/XhdHun2A03T2ptQVxt5NQ5BKbWKhsZi921/cHu9A7iwmfPeAd45Q5qbgcG+ZNaflFK6/UDT\ntHYlISGBI0eOhOz9I7YCXUTaRau+pmmai9lsJioqKmTvH7EBITo6OuQt+pqmaU0lJiaG7N4UkQFB\nRHT7gaZp7VIoexvpgKBpmtaOmM3mkPU2isiA4HA49IR2mqa1W6GqNorIgKAHpGma1p7Fx/s0oYPf\nRORdUVcXaZrWnkVFRWEyBX+5mogLCHpAmqZp4SAUjcsRFxCUUthstlBnQ9M0rUUJCQlBr9qOuIAg\nIiGfL0TTNM2TUCzeFXEBQa+QpmlaOBAR4uLigvqeERcQdPuBpmnhItjVRhEVEAwGg57QTtO0sBET\nE4gYpwkAAAWdSURBVEPDxNHBEVEBweFw6AZlTdPChushNlhBIaICgsFgCEnfXk3TtNYKZvfTiAoI\neroKTdPCjathORilhIgKCLGxsaHOgqZpmk9c67ZUVlYG/L0iqv5Etx9omhaOMjIygrJwjlclBBGZ\nICK7RWSfiDzYzP4eIvKZiGwRka0iMtG5fZyIfCciPzm/j3U7Z50zzR+cX138d1nN0z2MNE0LR4mJ\niUF5oPVYQhARI/A8MA7IA74VkRXOdZRdHgGWKKX+KSKDaFh/ORM4BlyjlDosIoOB1UA3t/NmOddW\nDrjBg0O2fLOmaVpY8KaEMBLYp5TKVkrVAm8Bk5scowDXfK0JwGEApdQWpdRh5/btgE1EQrdgqKZp\nmnZG3gSEbkCu2895nPqUDzAPuFFE8mgoHdzTTDrTgO+VUjVu2152Vhc9Kno+CU3TtJDyVy+jGcAr\nSqkMYCLwmog0pi0iZwH/H/BLt3NmKaXOBi52ft3UXMIiMltENovI5qKiIj9lV9M0TWvKm4CQD3R3\n+znDuc3dz4ElAEqprwErkAIgIhnAu8DNSqn9rhOUUvnO7+XAGzRUTZ1GKbVAKTVcKTW8c+fO3lyT\npmma1greBIRvgb4ikiUiFuAGYEWTY3KAywFEZCANAaFIRBKBlcCDSqmvXAeLiElEXAHDDFwNbGvr\nxWiapmmt5zEgKKXqgTk09BDaSUNvou0i8oSIXOs87H7gDhH5EXgTuFU1DKubA/QB/tCke2kUsFpE\ntgI/0FDi+Je/L07TNE3zngRzJr22Gj58uNq8OSi9VDVN0zoMEflOKTXc03ERNXWFpmmadmZhVUIQ\nkSLgUCtPT6FhoFwk0dccGfQ1R4a2XHNPpZTHXjlhFRDaQkQ2e1Nk6kj0NUcGfc2RIRjXrKuMNE3T\nNEAHBE3TNM0pkgLCglBnIAT0NUcGfc2RIeDXHDFtCJqmaVrLIqmEoGmaprUgIgKCpwV+OhoR6e5c\nsGiHiGwXkbmhzlMwiIjRuUjTB6HOS7CISKKILBORXSKyU0QuCHWeAk1Efu38XG8TkTdFpMMtli4i\nL4nIURHZ5ratk4isFZG9zu9J/n7fDh8Q3Bb4uQoYBMxwLuLTkdUD9yulBgHnA3dHwDUDzKVhepVI\nMh/4SCk1ADiHDn79ItIN+BUwXCk1GDDSML9aR/MKMKHJtgeBT5RSfYFPnD/7VYcPCHi3wE+HopQq\nUEp973xdTsNNoukaFh2Kc1bdScDCUOclWEQkAbgEeBFAKVWrlDoR2lwFhYmGxbZMQDTOBbk6EqXU\nF0BJk82TgUXO14uAKf5+30gICN4s8NNhiUgmcC6wKbQ5Cbi/Ar8FHKHOSBBlAUU0LDS1RUQWikhM\nqDMVSM5p85+lYYblAqBUKbUmtLkKmlSlVIHzdSGQ6u83iISAELFEJBZ4B7hXKVUW6vwEiohcDRxV\nSn0X6rwEmQkYBvxTKXUucJIAVCO0J85688k0BMOuQIyI3BjaXAWfczZpv3cRjYSA4M0CPx2Oc52J\nd4DFSql/hzo/AXYhcK2IHKShSnCsiLwe2iwFRR6Qp5Rylf6W0RAgOrIrgANKqSKlVB3wb2B0iPMU\nLEdEJB3A+f2ov98gEgKCNwv8dCjO9alfBHYqpZ4LdX4CTSn1kFIqQymVScPf91OlVId/alRKFQK5\nItLfuelyYEcIsxQMOcD5IhLt/JxfTgdvSHezArjF+foWYLm/38Dk7wTbG6VUvYi4FvgxAi8ppbaH\nOFuBdiENa1T/JCI/OLf9Xim1KoR50gLjHmCx82EnG7gtxPkJKKXUJhFZBnxPQ2+6LXTAUcsi8iYw\nBkgRkTzgMeBpYImI/JyGWZ+v8/v76pHK2v/fjh3TAAADQQwLf9RdDsDvtUlECkD9sYwAOBAEACpB\nAGAEAYBKEAAYQQCgEgQARhAAqOoBTmDTaliTBBMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fecd0e33a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning rates: [ 0.1   8.    7.14  1.08  9.98  3.64  2.58]\n",
      "Accuracy: [ 0.85659999  0.9562      0.95639998  0.97289997  0.85970002  0.97790003\n",
      "  0.97490001]\n"
     ]
    }
   ],
   "source": [
    "#If sf is used, we have to use opt_hyparams_noise, otherwise use opt_hyparams. \n",
    "n_iter = 5\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    next_candidate = acquisition_fun(l_rates, f, xn, np.array(data['Mean']), np.array(data['StdDev']))\n",
    "    l_rates = np.append(l_rates,next_candidate)\n",
    "    f = np.append(f,nn_train(next_candidate, h_dim))\n",
    "    E, cov = gp_posterior(l_rates, f, xn, m, noise, length, sf)\n",
    "    data = data_posterior(xn, E, cov)\n",
    "\n",
    "    try:\n",
    "        m, noise, length, sf = opt_hyparams(l_rates,f)\n",
    "    except ValueError:\n",
    "        m, noise, length, sf = m, noise, length, sf\n",
    "\n",
    "\n",
    "plt.plot(data['x'],data['Mean'], color = 'black', label = 'Mean')\n",
    "plt.plot(l_rates,f, 'ro', label = 'Obs')\n",
    "plt.fill_between(data['x'], data['Mean']-data['StdDev'], data['Mean']+data['StdDev'],color = 'lightgrey')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print ('Initial Learning rates:', l_rates)\n",
    "print ('Accuracy:', f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we observe how the algorithm explores the domain of the hyperparameter, searching for the most appropiate learning rate. In this examples the initial learning rates are 0.1 and 8. After 5 interations the algorithm recomends a learning rate that allows to increase the accuracy from the region in [0.9, 0.92] to [0.93, 0.95]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accuracy evaluation with decaying learning rate - w/o bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:0.11 loss: 2.30224 (lr:10.0)\n",
      "0: ********* epoch 1 ********* test accuracy:0.1095 test loss: 2.29958\n",
      "20: accuracy:0.31 loss: 2.12496 (lr:9.901493354116765)\n",
      "40: accuracy:0.41 loss: 2.05033 (lr:9.803966865736877)\n",
      "60: accuracy:0.73 loss: 1.7259 (lr:9.70741078213023)\n",
      "80: accuracy:0.69 loss: 1.76293 (lr:9.611815447607999)\n",
      "100: accuracy:0.74 loss: 1.71246 (lr:9.517171302557069)\n",
      "100: ********* epoch 1 ********* test accuracy:0.755 test loss: 1.70362\n",
      "120: accuracy:0.65 loss: 1.81477 (lr:9.423468882484062)\n",
      "140: accuracy:0.9 loss: 1.56727 (lr:9.330698817068887)\n",
      "160: accuracy:0.78 loss: 1.6941 (lr:9.238851829227693)\n",
      "180: accuracy:0.83 loss: 1.63882 (lr:9.14791873418516)\n",
      "200: accuracy:0.89 loss: 1.56942 (lr:9.057890438555999)\n",
      "200: ********* epoch 1 ********* test accuracy:0.899 test loss: 1.5613\n",
      "220: accuracy:0.9 loss: 1.56653 (lr:8.96875793943563)\n",
      "240: accuracy:0.85 loss: 1.60431 (lr:8.88051232349986)\n",
      "260: accuracy:0.82 loss: 1.63697 (lr:8.793144766113556)\n",
      "280: accuracy:0.87 loss: 1.58897 (lr:8.706646530448177)\n",
      "300: accuracy:0.95 loss: 1.51425 (lr:8.621008966608072)\n",
      "300: ********* epoch 1 ********* test accuracy:0.908 test loss: 1.55254\n",
      "320: accuracy:0.91 loss: 1.54984 (lr:8.536223510765492)\n",
      "340: accuracy:0.88 loss: 1.57814 (lr:8.452281684304198)\n",
      "360: accuracy:0.93 loss: 1.53486 (lr:8.369175092971593)\n",
      "380: accuracy:0.9 loss: 1.55484 (lr:8.286895426039287)\n",
      "400: accuracy:0.88 loss: 1.5808 (lr:8.20543445547202)\n",
      "400: ********* epoch 1 ********* test accuracy:0.8874 test loss: 1.57286\n",
      "420: accuracy:0.92 loss: 1.53251 (lr:8.124784035104852)\n",
      "440: accuracy:0.91 loss: 1.54533 (lr:8.044936099828538)\n",
      "460: accuracy:0.96 loss: 1.50625 (lr:7.965882664783007)\n",
      "480: accuracy:0.89 loss: 1.57535 (lr:7.887615824558879)\n",
      "500: accuracy:0.9 loss: 1.55898 (lr:7.810127752406908)\n",
      "500: ********* epoch 1 ********* test accuracy:0.9067 test loss: 1.55448\n",
      "520: accuracy:0.91 loss: 1.55316 (lr:7.733410699455306)\n",
      "540: accuracy:0.93 loss: 1.52031 (lr:7.657456993934846)\n",
      "560: accuracy:0.89 loss: 1.56304 (lr:7.582259040411682)\n",
      "580: accuracy:0.95 loss: 1.51123 (lr:7.507809319027796)\n",
      "600: accuracy:0.92 loss: 1.54022 (lr:7.434100384749007)\n",
      "600: ********* epoch 2 ********* test accuracy:0.9235 test loss: 1.53765\n",
      "620: accuracy:0.93 loss: 1.5309 (lr:7.361124866620464)\n",
      "640: accuracy:0.88 loss: 1.57721 (lr:7.28887546702954)\n",
      "660: accuracy:0.86 loss: 1.60066 (lr:7.217344960976069)\n",
      "680: accuracy:0.9 loss: 1.54879 (lr:7.146526195349836)\n",
      "700: accuracy:0.94 loss: 1.52016 (lr:7.076412088215263)\n",
      "700: ********* epoch 2 ********* test accuracy:0.9319 test loss: 1.52842\n",
      "720: accuracy:0.89 loss: 1.576 (lr:7.006995628103207)\n",
      "740: accuracy:0.93 loss: 1.52889 (lr:6.938269873309811)\n",
      "760: accuracy:0.93 loss: 1.53048 (lr:6.870227951202323)\n",
      "780: accuracy:0.91 loss: 1.55024 (lr:6.80286305753183)\n",
      "800: accuracy:0.96 loss: 1.50398 (lr:6.736168455752829)\n",
      "800: ********* epoch 2 ********* test accuracy:0.9313 test loss: 1.52957\n",
      "820: accuracy:0.89 loss: 1.5669 (lr:6.670137476349562)\n",
      "840: accuracy:0.96 loss: 1.50111 (lr:6.604763516169061)\n",
      "860: accuracy:0.88 loss: 1.57947 (lr:6.540040037760834)\n",
      "880: accuracy:0.94 loss: 1.51585 (lr:6.475960568723099)\n",
      "900: accuracy:0.91 loss: 1.55258 (lr:6.412518701055556)\n",
      "900: ********* epoch 2 ********* test accuracy:0.9302 test loss: 1.53125\n",
      "920: accuracy:0.92 loss: 1.54 (lr:6.3497080905185666)\n",
      "940: accuracy:0.97 loss: 1.49741 (lr:6.2875224559987375)\n",
      "960: accuracy:0.9 loss: 1.55646 (lr:6.225955578880794)\n",
      "980: accuracy:0.95 loss: 1.50028 (lr:6.165001302425719)\n",
      "1000: accuracy:0.94 loss: 1.51974 (lr:6.104653531155071)\n",
      "1000: ********* epoch 2 ********* test accuracy:0.9361 test loss: 1.52533\n",
      "1020: accuracy:0.92 loss: 1.54165 (lr:6.044906230241432)\n",
      "1040: accuracy:0.97 loss: 1.49506 (lr:5.9857534249049245)\n",
      "1060: accuracy:0.97 loss: 1.49883 (lr:5.927189199815716)\n",
      "1080: accuracy:0.93 loss: 1.53108 (lr:5.869207698502497)\n",
      "1100: accuracy:0.97 loss: 1.49391 (lr:5.811803122766817)\n",
      "1100: ********* epoch 2 ********* test accuracy:0.9439 test loss: 1.51753\n",
      "1120: accuracy:0.95 loss: 1.51165 (lr:5.754969732103267)\n",
      "1140: accuracy:0.89 loss: 1.56957 (lr:5.698701843125417)\n",
      "1160: accuracy:0.91 loss: 1.55026 (lr:5.64299382899748)\n",
      "1180: accuracy:0.93 loss: 1.52971 (lr:5.58784011887162)\n",
      "1200: accuracy:0.96 loss: 1.50148 (lr:5.533235197330861)\n",
      "1200: ********* epoch 3 ********* test accuracy:0.9464 test loss: 1.51502\n",
      "1220: accuracy:0.97 loss: 1.49908 (lr:5.479173603837548)\n",
      "1240: accuracy:0.91 loss: 1.54676 (lr:5.4256499321872775)\n",
      "1260: accuracy:0.93 loss: 1.52992 (lr:5.372658829968282)\n",
      "1280: accuracy:0.95 loss: 1.5132 (lr:5.320194998026181)\n",
      "1300: accuracy:0.95 loss: 1.51421 (lr:5.268253189934058)\n",
      "1300: ********* epoch 3 ********* test accuracy:0.9446 test loss: 1.51611\n",
      "1320: accuracy:0.97 loss: 1.48738 (lr:5.2168282114678215)\n",
      "1340: accuracy:0.97 loss: 1.48924 (lr:5.16591492008677)\n",
      "1360: accuracy:0.92 loss: 1.53964 (lr:5.115508224419337)\n",
      "1380: accuracy:0.96 loss: 1.49721 (lr:5.06560308375395)\n",
      "1400: accuracy:0.88 loss: 1.57411 (lr:5.016194507534954)\n",
      "1400: ********* epoch 3 ********* test accuracy:0.9203 test loss: 1.54059\n",
      "1420: accuracy:0.97 loss: 1.49025 (lr:4.967277554863554)\n",
      "1440: accuracy:0.94 loss: 1.52243 (lr:4.918847334003719)\n",
      "1460: accuracy:0.96 loss: 1.49658 (lr:4.870899001893004)\n",
      "1480: accuracy:0.93 loss: 1.53262 (lr:4.82342776365824)\n",
      "1500: accuracy:0.97 loss: 1.49318 (lr:4.776428872136045)\n",
      "1500: ********* epoch 3 ********* test accuracy:0.9453 test loss: 1.51554\n",
      "1520: accuracy:0.93 loss: 1.53089 (lr:4.729897627398101)\n",
      "1540: accuracy:0.95 loss: 1.51092 (lr:4.683829376281158)\n",
      "1560: accuracy:0.97 loss: 1.49054 (lr:4.6382195119217124)\n",
      "1580: accuracy:0.9 loss: 1.55917 (lr:4.593063473295323)\n",
      "1600: accuracy:0.94 loss: 1.5104 (lr:4.548356744760493)\n",
      "1600: ********* epoch 3 ********* test accuracy:0.9479 test loss: 1.51347\n",
      "1620: accuracy:0.97 loss: 1.49217 (lr:4.504094855607117)\n",
      "1640: accuracy:0.94 loss: 1.52489 (lr:4.460273379609393)\n",
      "1660: accuracy:0.95 loss: 1.5171 (lr:4.416887934583202)\n",
      "1680: accuracy:0.95 loss: 1.505 (lr:4.373934181947889)\n",
      "1700: accuracy:0.98 loss: 1.47983 (lr:4.331407826292394)\n",
      "1700: ********* epoch 3 ********* test accuracy:0.9483 test loss: 1.51259\n",
      "1720: accuracy:0.96 loss: 1.50546 (lr:4.2893046149457135)\n",
      "1740: accuracy:1.0 loss: 1.46122 (lr:4.247620337551626)\n",
      "1760: accuracy:0.97 loss: 1.49152 (lr:4.206350825647656)\n",
      "1780: accuracy:0.99 loss: 1.47279 (lr:4.16549195224822)\n",
      "1800: accuracy:0.93 loss: 1.53535 (lr:4.125039631431931)\n",
      "1800: ********* epoch 4 ********* test accuracy:0.9492 test loss: 1.51195\n",
      "1820: accuracy:0.96 loss: 1.50885 (lr:4.084989817932996)\n",
      "1840: accuracy:0.97 loss: 1.49473 (lr:4.04533850673669)\n",
      "1860: accuracy:0.96 loss: 1.50377 (lr:4.006081732678851)\n",
      "1880: accuracy:0.94 loss: 1.51938 (lr:3.9672155700493597)\n",
      "1900: accuracy:0.97 loss: 1.49348 (lr:3.9287361321995626)\n",
      "1900: ********* epoch 4 ********* test accuracy:0.9503 test loss: 1.51071\n",
      "1920: accuracy:0.97 loss: 1.49122 (lr:3.8906395711536095)\n",
      "1940: accuracy:0.98 loss: 1.48424 (lr:3.8529220772236488)\n",
      "1960: accuracy:0.94 loss: 1.52034 (lr:3.815579878628856)\n",
      "1980: accuracy:0.96 loss: 1.50138 (lr:3.778609241118253)\n",
      "2000: accuracy:0.97 loss: 1.492 (lr:3.7420064675972795)\n",
      "2000: ********* epoch 4 ********* test accuracy:0.9507 test loss: 1.511\n",
      "2020: accuracy:0.96 loss: 1.50577 (lr:3.705767897758081)\n",
      "2040: accuracy:0.93 loss: 1.53163 (lr:3.6698899077134755)\n",
      "2060: accuracy:0.95 loss: 1.51422 (lr:3.6343689096345595)\n",
      "2080: accuracy:0.94 loss: 1.52298 (lr:3.599201351391924)\n",
      "2100: accuracy:0.96 loss: 1.50113 (lr:3.564383716200438)\n",
      "2100: ********* epoch 4 ********* test accuracy:0.9538 test loss: 1.50761\n",
      "2120: accuracy:0.96 loss: 1.50117 (lr:3.5299125222675687)\n",
      "2140: accuracy:0.96 loss: 1.49096 (lr:3.495784322445196)\n",
      "2160: accuracy:0.93 loss: 1.52579 (lr:3.4619957038848974)\n",
      "2180: accuracy:0.95 loss: 1.51116 (lr:3.42854328769666)\n",
      "2200: accuracy:0.96 loss: 1.50095 (lr:3.3954237286109876)\n",
      "2200: ********* epoch 4 ********* test accuracy:0.9547 test loss: 1.5059\n",
      "2220: accuracy:0.9 loss: 1.55891 (lr:3.362633714644372)\n",
      "2240: accuracy:0.92 loss: 1.54343 (lr:3.330169966768091)\n",
      "2260: accuracy:0.95 loss: 1.51011 (lr:3.2980292385803045)\n",
      "2280: accuracy:0.97 loss: 1.49668 (lr:3.2662083159814093)\n",
      "2300: accuracy:0.99 loss: 1.47119 (lr:3.2347040168526275)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300: ********* epoch 4 ********* test accuracy:0.9544 test loss: 1.50673\n",
      "2320: accuracy:0.97 loss: 1.49034 (lr:3.2035131907377927)\n",
      "2340: accuracy:0.94 loss: 1.51651 (lr:3.172632718528302)\n",
      "2360: accuracy:0.93 loss: 1.52875 (lr:3.1420595121511994)\n",
      "2380: accuracy:0.94 loss: 1.52137 (lr:3.111790514260371)\n",
      "2400: accuracy:0.99 loss: 1.46876 (lr:3.0818226979308014)\n",
      "2400: ********* epoch 5 ********* test accuracy:0.9583 test loss: 1.50223\n",
      "2420: accuracy:0.98 loss: 1.48381 (lr:3.0521530663558853)\n",
      "2440: accuracy:0.97 loss: 1.49035 (lr:3.022778652547741)\n",
      "2460: accuracy:0.97 loss: 1.49107 (lr:2.9936965190405083)\n",
      "2480: accuracy:0.97 loss: 1.49158 (lr:2.964903757596601)\n",
      "2500: accuracy:0.98 loss: 1.48202 (lr:2.936397488915882)\n",
      "2500: ********* epoch 5 ********* test accuracy:0.9542 test loss: 1.5068\n",
      "2520: accuracy:0.92 loss: 1.54182 (lr:2.9081748623477273)\n",
      "2540: accuracy:0.96 loss: 1.50145 (lr:2.88023305560596)\n",
      "2560: accuracy:0.94 loss: 1.52123 (lr:2.8525692744866222)\n",
      "2580: accuracy:0.96 loss: 1.5011 (lr:2.8251807525885484)\n",
      "2600: accuracy:0.97 loss: 1.49782 (lr:2.798064751036725)\n",
      "2600: ********* epoch 5 ********* test accuracy:0.9556 test loss: 1.50469\n",
      "2620: accuracy:0.94 loss: 1.51976 (lr:2.7712185582083997)\n",
      "2640: accuracy:0.98 loss: 1.48132 (lr:2.7446394894619184)\n",
      "2660: accuracy:0.95 loss: 1.51099 (lr:2.7183248868682575)\n",
      "2680: accuracy:0.96 loss: 1.50196 (lr:2.6922721189452274)\n",
      "2700: accuracy:0.94 loss: 1.51315 (lr:2.666478580394326)\n",
      "2700: ********* epoch 5 ********* test accuracy:0.9557 test loss: 1.50531\n",
      "2720: accuracy:0.95 loss: 1.51451 (lr:2.6409416918402036)\n",
      "2740: accuracy:0.96 loss: 1.50031 (lr:2.615658899572723)\n",
      "2760: accuracy:0.95 loss: 1.50619 (lr:2.5906276752915898)\n",
      "2780: accuracy:0.97 loss: 1.49837 (lr:2.5658455158535154)\n",
      "2800: accuracy:0.97 loss: 1.49091 (lr:2.5413099430219046)\n",
      "2800: ********* epoch 5 ********* test accuracy:0.9584 test loss: 1.50268\n",
      "2820: accuracy:0.94 loss: 1.51635 (lr:2.5170185032190275)\n",
      "2840: accuracy:0.97 loss: 1.49037 (lr:2.492968767280661)\n",
      "2860: accuracy:0.98 loss: 1.48102 (lr:2.4691583302131703)\n",
      "2880: accuracy:0.96 loss: 1.50257 (lr:2.445584810953006)\n",
      "2900: accuracy:0.94 loss: 1.51766 (lr:2.4222458521285968)\n",
      "2900: ********* epoch 5 ********* test accuracy:0.9607 test loss: 1.5004\n",
      "2920: accuracy:0.96 loss: 1.50195 (lr:2.3991391198246124)\n",
      "2940: accuracy:0.98 loss: 1.4848 (lr:2.3762623033485664)\n",
      "2960: accuracy:0.97 loss: 1.48602 (lr:2.3536131149997463)\n",
      "2980: accuracy:0.96 loss: 1.49793 (lr:2.3311892898404434)\n",
      "3000: accuracy:0.97 loss: 1.49277 (lr:2.3089885854694554)\n",
      "3000: ********* epoch 6 ********* test accuracy:0.96 test loss: 1.50143\n",
      "3020: accuracy:0.98 loss: 1.48284 (lr:2.2870087817978444)\n",
      "3040: accuracy:0.96 loss: 1.50044 (lr:2.265247680826926)\n",
      "3060: accuracy:0.95 loss: 1.51235 (lr:2.24370310642847)\n",
      "3080: accuracy:0.96 loss: 1.50109 (lr:2.2223729041270817)\n",
      "3100: accuracy:0.98 loss: 1.48116 (lr:2.2012549408847564)\n",
      "3100: ********* epoch 6 ********* test accuracy:0.9607 test loss: 1.50043\n",
      "3120: accuracy:0.97 loss: 1.49102 (lr:2.180347104887571)\n",
      "3140: accuracy:0.94 loss: 1.52554 (lr:2.1596473053345027)\n",
      "3160: accuracy:0.95 loss: 1.5103 (lr:2.139153472228346)\n",
      "3180: accuracy:0.96 loss: 1.50119 (lr:2.118863556168713)\n",
      "3200: accuracy:0.99 loss: 1.47277 (lr:2.0987755281470886)\n",
      "3200: ********* epoch 6 ********* test accuracy:0.9589 test loss: 1.50238\n",
      "3220: accuracy:0.97 loss: 1.4916 (lr:2.0788873793439304)\n",
      "3240: accuracy:0.98 loss: 1.48207 (lr:2.0591971209277853)\n",
      "3260: accuracy:0.98 loss: 1.48132 (lr:2.0397027838564026)\n",
      "3280: accuracy:0.97 loss: 1.49086 (lr:2.02040241867983)\n",
      "3300: accuracy:0.96 loss: 1.50087 (lr:2.001294095345466)\n",
      "3300: ********* epoch 6 ********* test accuracy:0.9596 test loss: 1.50163\n",
      "3320: accuracy:0.92 loss: 1.5374 (lr:1.9823759030050536)\n",
      "3340: accuracy:0.95 loss: 1.50692 (lr:1.9636459498235934)\n",
      "3360: accuracy:0.98 loss: 1.47634 (lr:1.945102362790159)\n",
      "3380: accuracy:0.96 loss: 1.49862 (lr:1.9267432875305939)\n",
      "3400: accuracy:0.97 loss: 1.49286 (lr:1.9085668881220734)\n",
      "3400: ********* epoch 6 ********* test accuracy:0.9619 test loss: 1.49938\n",
      "3420: accuracy:0.96 loss: 1.50073 (lr:1.890571346909509)\n",
      "3440: accuracy:0.95 loss: 1.51507 (lr:1.872754864323783)\n",
      "3460: accuracy:0.98 loss: 1.48065 (lr:1.8551156587017905)\n",
      "3480: accuracy:0.96 loss: 1.49649 (lr:1.837651966108269)\n",
      "3500: accuracy:0.97 loss: 1.49334 (lr:1.820362040159407)\n",
      "3500: ********* epoch 6 ********* test accuracy:0.9621 test loss: 1.49879\n",
      "3520: accuracy:0.96 loss: 1.49799 (lr:1.8032441518482005)\n",
      "3540: accuracy:0.98 loss: 1.48307 (lr:1.7862965893715534)\n",
      "3560: accuracy:0.96 loss: 1.49675 (lr:1.7695176579590957)\n",
      "3580: accuracy:0.97 loss: 1.4957 (lr:1.752905679703703)\n",
      "3600: accuracy:0.98 loss: 1.48124 (lr:1.736458993393707)\n",
      "3600: ********* epoch 7 ********* test accuracy:0.9614 test loss: 1.49916\n",
      "3620: accuracy:0.93 loss: 1.53154 (lr:1.7201759543467703)\n",
      "3640: accuracy:0.93 loss: 1.53031 (lr:1.7040549342454194)\n",
      "3660: accuracy:0.94 loss: 1.5285 (lr:1.6880943209742103)\n",
      "3680: accuracy:0.98 loss: 1.4783 (lr:1.672292518458515)\n",
      "3700: accuracy:0.98 loss: 1.481 (lr:1.6566479465049135)\n",
      "3700: ********* epoch 7 ********* test accuracy:0.9637 test loss: 1.49777\n",
      "3720: accuracy:0.95 loss: 1.50799 (lr:1.6411590406431735)\n",
      "3740: accuracy:0.96 loss: 1.50089 (lr:1.625824251969801)\n",
      "3760: accuracy:0.96 loss: 1.50214 (lr:1.6106420469931506)\n",
      "3780: accuracy:0.94 loss: 1.51702 (lr:1.5956109074800717)\n",
      "3800: accuracy:0.97 loss: 1.49077 (lr:1.5807293303040872)\n",
      "3800: ********* epoch 7 ********* test accuracy:0.9614 test loss: 1.49964\n",
      "3820: accuracy:0.98 loss: 1.48124 (lr:1.5659958272950785)\n",
      "3840: accuracy:0.98 loss: 1.47939 (lr:1.5514089250904666)\n",
      "3860: accuracy:0.96 loss: 1.5047 (lr:1.5369671649878751)\n",
      "3880: accuracy:0.98 loss: 1.48185 (lr:1.5226691027992592)\n",
      "3900: accuracy:1.0 loss: 1.46293 (lr:1.5085133087064846)\n",
      "3900: ********* epoch 7 ********* test accuracy:0.9637 test loss: 1.49729\n",
      "3920: accuracy:0.94 loss: 1.52116 (lr:1.4944983671183456)\n",
      "3940: accuracy:0.95 loss: 1.51082 (lr:1.4806228765290044)\n",
      "3960: accuracy:0.99 loss: 1.4715 (lr:1.4668854493778392)\n",
      "3980: accuracy:0.98 loss: 1.48653 (lr:1.4532847119106862)\n",
      "4000: accuracy:0.98 loss: 1.47831 (lr:1.4398193040424658)\n",
      "4000: ********* epoch 7 ********* test accuracy:0.964 test loss: 1.49753\n",
      "4020: accuracy:0.98 loss: 1.48201 (lr:1.4264878792211695)\n",
      "4040: accuracy:1.0 loss: 1.46137 (lr:1.4132891042932052)\n",
      "4060: accuracy:0.97 loss: 1.4912 (lr:1.400221659370082)\n",
      "4080: accuracy:0.95 loss: 1.51028 (lr:1.3872842376964165)\n",
      "4100: accuracy:0.96 loss: 1.50601 (lr:1.374475545519262)\n",
      "4100: ********* epoch 7 ********* test accuracy:0.9627 test loss: 1.4979\n",
      "4120: accuracy:0.96 loss: 1.5033 (lr:1.3617943019587255)\n",
      "4140: accuracy:0.97 loss: 1.4912 (lr:1.349239238879884)\n",
      "4160: accuracy:0.95 loss: 1.50969 (lr:1.336809100765966)\n",
      "4180: accuracy:0.99 loss: 1.47118 (lr:1.324502644592803)\n",
      "4200: accuracy:0.98 loss: 1.4818 (lr:1.312318639704521)\n",
      "4200: ********* epoch 8 ********* test accuracy:0.9642 test loss: 1.49726\n",
      "4220: accuracy:0.96 loss: 1.50132 (lr:1.3002558676904787)\n",
      "4240: accuracy:0.96 loss: 1.50543 (lr:1.2883131222634217)\n",
      "4260: accuracy:0.95 loss: 1.51435 (lr:1.2764892091388558)\n",
      "4280: accuracy:0.97 loss: 1.49058 (lr:1.2647829459156141)\n",
      "4300: accuracy:0.98 loss: 1.48377 (lr:1.25319316195762)\n",
      "4300: ********* epoch 8 ********* test accuracy:0.9621 test loss: 1.4988\n",
      "4320: accuracy:0.92 loss: 1.54022 (lr:1.241718698276819)\n",
      "4340: accuracy:0.95 loss: 1.51322 (lr:1.2303584074172815)\n",
      "4360: accuracy:0.98 loss: 1.4793 (lr:1.2191111533404537)\n",
      "4380: accuracy:0.94 loss: 1.52169 (lr:1.207975811311556)\n",
      "4400: accuracy:0.97 loss: 1.49274 (lr:1.1969512677871055)\n",
      "4400: ********* epoch 8 ********* test accuracy:0.9648 test loss: 1.49662\n",
      "4420: accuracy:1.0 loss: 1.46628 (lr:1.1860364203035632)\n",
      "4440: accuracy:0.96 loss: 1.49958 (lr:1.175230177367084)\n",
      "4460: accuracy:0.96 loss: 1.50088 (lr:1.1645314583443702)\n",
      "4480: accuracy:0.96 loss: 1.50449 (lr:1.1539391933546028)\n",
      "4500: accuracy:0.98 loss: 1.48137 (lr:1.143452323162457)\n",
      "4500: ********* epoch 8 ********* test accuracy:0.964 test loss: 1.49713\n",
      "4520: accuracy:0.99 loss: 1.47129 (lr:1.1330697990721739)\n",
      "4540: accuracy:0.95 loss: 1.51072 (lr:1.122790582822692)\n",
      "4560: accuracy:0.97 loss: 1.48838 (lr:1.1126136464838212)\n",
      "4580: accuracy:0.97 loss: 1.48942 (lr:1.1025379723534459)\n",
      "4600: accuracy:0.96 loss: 1.50335 (lr:1.0925625528557572)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600: ********* epoch 8 ********* test accuracy:0.9663 test loss: 1.49562\n",
      "4620: accuracy:0.93 loss: 1.52773 (lr:1.082686390440492)\n",
      "4640: accuracy:0.99 loss: 1.47103 (lr:1.0729084974831793)\n",
      "4660: accuracy:0.96 loss: 1.50053 (lr:1.0632278961863744)\n",
      "4680: accuracy:0.98 loss: 1.47918 (lr:1.0536436184818812)\n",
      "4700: accuracy:0.96 loss: 1.49616 (lr:1.0441547059339413)\n",
      "4700: ********* epoch 8 ********* test accuracy:0.9636 test loss: 1.49711\n",
      "4720: accuracy:0.97 loss: 1.49147 (lr:1.0347602096433932)\n",
      "4740: accuracy:0.99 loss: 1.47083 (lr:1.025459190152779)\n",
      "4760: accuracy:0.98 loss: 1.47738 (lr:1.0162507173523987)\n",
      "4780: accuracy:0.97 loss: 1.49102 (lr:1.0071338703872978)\n",
      "4800: accuracy:0.98 loss: 1.48229 (lr:0.9981077375651839)\n",
      "4800: ********* epoch 9 ********* test accuracy:0.9627 test loss: 1.49805\n",
      "4820: accuracy:0.97 loss: 1.49142 (lr:0.9891714162652515)\n",
      "4840: accuracy:0.97 loss: 1.49427 (lr:0.9803240128479248)\n",
      "4860: accuracy:0.97 loss: 1.49487 (lr:0.9715646425654882)\n",
      "4880: accuracy:0.95 loss: 1.51236 (lr:0.9628924294736149)\n",
      "4900: accuracy:0.96 loss: 1.49781 (lr:0.9543065063437679)\n",
      "4900: ********* epoch 9 ********* test accuracy:0.9654 test loss: 1.49604\n",
      "4920: accuracy:0.95 loss: 1.50951 (lr:0.9458060145764802)\n",
      "4940: accuracy:0.95 loss: 1.51486 (lr:0.9373901041154904)\n",
      "4960: accuracy:0.97 loss: 1.49069 (lr:0.92905793336274)\n",
      "4980: accuracy:1.0 loss: 1.46562 (lr:0.9208086690942093)\n",
      "5000: accuracy:0.96 loss: 1.50116 (lr:0.9126414863765981)\n",
      "5000: ********* epoch 9 ********* test accuracy:0.9637 test loss: 1.49725\n",
      "5020: accuracy:0.98 loss: 1.48123 (lr:0.9045555684848279)\n",
      "5040: accuracy:0.95 loss: 1.51175 (lr:0.8965501068203712)\n",
      "5060: accuracy:0.96 loss: 1.50145 (lr:0.8886243008303906)\n",
      "5080: accuracy:0.99 loss: 1.47127 (lr:0.880777357927682)\n",
      "5100: accuracy:0.97 loss: 1.49164 (lr:0.8730084934114164)\n",
      "5100: ********* epoch 9 ********* test accuracy:0.9648 test loss: 1.49638\n",
      "5120: accuracy:0.96 loss: 1.50176 (lr:0.8653169303886674)\n",
      "5140: accuracy:0.97 loss: 1.49118 (lr:0.857701899696724)\n",
      "5160: accuracy:0.97 loss: 1.49121 (lr:0.8501626398261702)\n",
      "5180: accuracy:0.97 loss: 1.49241 (lr:0.8426983968447371)\n",
      "5200: accuracy:0.93 loss: 1.53074 (lr:0.8353084243219054)\n",
      "5200: ********* epoch 9 ********* test accuracy:0.966 test loss: 1.49564\n",
      "5220: accuracy:0.99 loss: 1.47327 (lr:0.8279919832542653)\n",
      "5240: accuracy:0.97 loss: 1.49262 (lr:0.8207483419916123)\n",
      "5260: accuracy:0.97 loss: 1.4908 (lr:0.8135767761637844)\n",
      "5280: accuracy:0.97 loss: 1.48907 (lr:0.8064765686082219)\n",
      "5300: accuracy:0.98 loss: 1.48244 (lr:0.799447009298253)\n",
      "5300: ********* epoch 9 ********* test accuracy:0.9642 test loss: 1.49664\n",
      "5320: accuracy:0.96 loss: 1.49975 (lr:0.792487395272088)\n",
      "5340: accuracy:0.94 loss: 1.51954 (lr:0.7855970305625254)\n",
      "5360: accuracy:0.98 loss: 1.48159 (lr:0.7787752261273513)\n",
      "5380: accuracy:0.97 loss: 1.48679 (lr:0.7720212997804383)\n",
      "5400: accuracy:0.94 loss: 1.52054 (lr:0.7653345761235226)\n",
      "5400: ********* epoch 10 ********* test accuracy:0.9647 test loss: 1.49621\n",
      "5420: accuracy:0.98 loss: 1.48167 (lr:0.7587143864786668)\n",
      "5440: accuracy:0.97 loss: 1.49291 (lr:0.7521600688213892)\n",
      "5460: accuracy:0.99 loss: 1.46759 (lr:0.7456709677144625)\n",
      "5480: accuracy:0.97 loss: 1.49118 (lr:0.7392464342423679)\n",
      "5500: accuracy:0.96 loss: 1.50175 (lr:0.732885825946405)\n",
      "5500: ********* epoch 10 ********* test accuracy:0.9658 test loss: 1.49547\n",
      "5520: accuracy:0.99 loss: 1.47149 (lr:0.7265885067604433)\n",
      "5540: accuracy:0.96 loss: 1.50074 (lr:0.7203538469473162)\n",
      "5560: accuracy:0.98 loss: 1.48468 (lr:0.7141812230358473)\n",
      "5580: accuracy:0.99 loss: 1.47118 (lr:0.7080700177585012)\n",
      "5600: accuracy:0.97 loss: 1.49343 (lr:0.7020196199896579)\n",
      "5600: ********* epoch 10 ********* test accuracy:0.9657 test loss: 1.49564\n",
      "5620: accuracy:0.98 loss: 1.48049 (lr:0.696029424684498)\n",
      "5640: accuracy:0.96 loss: 1.49884 (lr:0.6900988328184997)\n",
      "5660: accuracy:1.0 loss: 1.46226 (lr:0.6842272513275336)\n",
      "5680: accuracy:0.96 loss: 1.50117 (lr:0.6784140930485582)\n",
      "5700: accuracy:0.98 loss: 1.48141 (lr:0.6726587766609007)\n",
      "5700: ********* epoch 10 ********* test accuracy:0.966 test loss: 1.49542\n",
      "5720: accuracy:0.94 loss: 1.51716 (lr:0.6669607266281269)\n",
      "5740: accuracy:0.96 loss: 1.49694 (lr:0.6613193731404844)\n",
      "5760: accuracy:0.97 loss: 1.49414 (lr:0.6557341520579238)\n",
      "5780: accuracy:0.98 loss: 1.48198 (lr:0.6502045048536823)\n",
      "5800: accuracy:0.98 loss: 1.4805 (lr:0.6447298785584316)\n",
      "5800: ********* epoch 10 ********* test accuracy:0.9666 test loss: 1.49486\n",
      "5820: accuracy:0.97 loss: 1.48996 (lr:0.6393097257049796)\n",
      "5840: accuracy:0.97 loss: 1.49108 (lr:0.6339435042735246)\n",
      "5860: accuracy:0.98 loss: 1.48379 (lr:0.6286306776374512)\n",
      "5880: accuracy:0.96 loss: 1.49534 (lr:0.6233707145096686)\n",
      "5900: accuracy:0.95 loss: 1.5102 (lr:0.6181630888894806)\n",
      "5900: ********* epoch 10 ********* test accuracy:0.9662 test loss: 1.495\n",
      "5920: accuracy:0.99 loss: 1.4715 (lr:0.6130072800099857)\n",
      "5940: accuracy:0.97 loss: 1.48946 (lr:0.6079027722859992)\n",
      "5960: accuracy:0.98 loss: 1.47265 (lr:0.6028490552624952)\n",
      "5980: accuracy:0.97 loss: 1.49494 (lr:0.5978456235635595)\n",
      "6000: accuracy:0.97 loss: 1.49029 (lr:0.592891976841853)\n",
      "6000: ********* epoch 11 ********* test accuracy:0.9659 test loss: 1.49519\n",
      "6020: accuracy:0.97 loss: 1.4911 (lr:0.5879876197285755)\n",
      "6040: accuracy:0.99 loss: 1.47153 (lr:0.5831320617839283)\n",
      "6060: accuracy:0.99 loss: 1.47294 (lr:0.5783248174480712)\n",
      "6080: accuracy:0.96 loss: 1.50124 (lr:0.5735654059925639)\n",
      "6100: accuracy:0.99 loss: 1.47255 (lr:0.5688533514722952)\n",
      "6100: ********* epoch 11 ********* test accuracy:0.9661 test loss: 1.49515\n",
      "6120: accuracy:1.0 loss: 1.46145 (lr:0.5641881826778861)\n",
      "6140: accuracy:0.98 loss: 1.48196 (lr:0.5595694330885697)\n",
      "6160: accuracy:0.98 loss: 1.48586 (lr:0.5549966408255377)\n",
      "6180: accuracy:0.99 loss: 1.47115 (lr:0.5504693486057536)\n",
      "6200: accuracy:0.97 loss: 1.48977 (lr:0.5459871036962223)\n",
      "6200: ********* epoch 11 ********* test accuracy:0.9655 test loss: 1.49524\n",
      "6220: accuracy:0.98 loss: 1.48108 (lr:0.5415494578687179)\n",
      "6240: accuracy:0.93 loss: 1.53158 (lr:0.5371559673549593)\n",
      "6260: accuracy:0.97 loss: 1.49349 (lr:0.5328061928022343)\n",
      "6280: accuracy:0.95 loss: 1.51104 (lr:0.5284996992294624)\n",
      "6300: accuracy:0.99 loss: 1.47484 (lr:0.5242360559836978)\n",
      "6300: ********* epoch 11 ********* test accuracy:0.9658 test loss: 1.49498\n",
      "6320: accuracy:0.94 loss: 1.52043 (lr:0.5200148366970627)\n",
      "6340: accuracy:0.98 loss: 1.48376 (lr:0.515835619244111)\n",
      "6360: accuracy:0.97 loss: 1.49225 (lr:0.5116979856996143)\n",
      "6380: accuracy:0.98 loss: 1.48654 (lr:0.5076015222967707)\n",
      "6400: accuracy:0.97 loss: 1.49065 (lr:0.5035458193858255)\n",
      "6400: ********* epoch 11 ********* test accuracy:0.967 test loss: 1.49468\n",
      "6420: accuracy:0.96 loss: 1.50108 (lr:0.49953047139310836)\n",
      "6440: accuracy:0.99 loss: 1.47151 (lr:0.4955550767804736)\n",
      "6460: accuracy:0.95 loss: 1.51115 (lr:0.4916192380051474)\n",
      "6480: accuracy:0.99 loss: 1.47116 (lr:0.48772256147997195)\n",
      "6500: accuracy:0.96 loss: 1.50113 (lr:0.48386465753404795)\n",
      "6500: ********* epoch 11 ********* test accuracy:0.966 test loss: 1.4949\n",
      "6520: accuracy:0.97 loss: 1.49137 (lr:0.48004514037376556)\n",
      "6540: accuracy:0.98 loss: 1.48118 (lr:0.47626362804422573)\n",
      "6560: accuracy:0.98 loss: 1.481 (lr:0.47251974239104455)\n",
      "6580: accuracy:0.99 loss: 1.47282 (lr:0.4688131090225365)\n",
      "6600: accuracy:0.96 loss: 1.49399 (lr:0.4651433572722762)\n",
      "6600: ********* epoch 12 ********* test accuracy:0.967 test loss: 1.49473\n",
      "6620: accuracy:1.0 loss: 1.46115 (lr:0.46151012016202997)\n",
      "6640: accuracy:0.98 loss: 1.48094 (lr:0.4579130343650596)\n",
      "6660: accuracy:0.98 loss: 1.48124 (lr:0.4543517401697874)\n",
      "6680: accuracy:0.99 loss: 1.47182 (lr:0.4508258814438265)\n",
      "6700: accuracy:0.97 loss: 1.49308 (lr:0.44733510559836576)\n",
      "6700: ********* epoch 12 ********* test accuracy:0.9665 test loss: 1.49467\n",
      "6720: accuracy:0.97 loss: 1.49149 (lr:0.44387906355291185)\n",
      "6740: accuracy:0.96 loss: 1.50181 (lr:0.44045740970037983)\n",
      "6760: accuracy:0.98 loss: 1.48082 (lr:0.4370698018725335)\n",
      "6780: accuracy:0.98 loss: 1.48329 (lr:0.43371590130576676)\n",
      "6800: accuracy:0.99 loss: 1.47135 (lr:0.43039537260722815)\n",
      "6800: ********* epoch 12 ********* test accuracy:0.9666 test loss: 1.49474\n",
      "6820: accuracy:0.98 loss: 1.48136 (lr:0.4271078837212806)\n",
      "6840: accuracy:0.97 loss: 1.49349 (lr:0.4238531058962962)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6860: accuracy:0.97 loss: 1.49252 (lr:0.42063071365177973)\n",
      "6880: accuracy:0.99 loss: 1.4718 (lr:0.4174403847458217)\n",
      "6900: accuracy:0.97 loss: 1.4912 (lr:0.41428180014287264)\n",
      "6900: ********* epoch 12 ********* test accuracy:0.9664 test loss: 1.49477\n",
      "6920: accuracy:0.98 loss: 1.47862 (lr:0.4111546439818403)\n",
      "6940: accuracy:0.94 loss: 1.52104 (lr:0.4080586035445025)\n",
      "6960: accuracy:0.99 loss: 1.47118 (lr:0.4049933692242357)\n",
      "6980: accuracy:0.99 loss: 1.47211 (lr:0.4019586344950531)\n",
      "7000: accuracy:0.98 loss: 1.48142 (lr:0.3989540958809532)\n",
      "7000: ********* epoch 12 ********* test accuracy:0.9667 test loss: 1.49448\n",
      "7020: accuracy:0.97 loss: 1.49181 (lr:0.3959794529255706)\n",
      "7040: accuracy:1.0 loss: 1.46117 (lr:0.39303440816213076)\n",
      "7060: accuracy:0.97 loss: 1.49696 (lr:0.3901186670837036)\n",
      "7080: accuracy:0.96 loss: 1.50272 (lr:0.3872319381137509)\n",
      "7100: accuracy:0.98 loss: 1.48126 (lr:0.3843739325769704)\n",
      "7100: ********* epoch 12 ********* test accuracy:0.9662 test loss: 1.49484\n",
      "7120: accuracy:0.98 loss: 1.48174 (lr:0.38154436467042663)\n",
      "7140: accuracy:0.97 loss: 1.49141 (lr:0.37874295143497105)\n",
      "7160: accuracy:0.98 loss: 1.48117 (lr:0.3759694127269455)\n",
      "7180: accuracy:0.98 loss: 1.48145 (lr:0.37322347119016797)\n",
      "7200: accuracy:0.98 loss: 1.48174 (lr:0.3705048522281964)\n",
      "7200: ********* epoch 13 ********* test accuracy:0.9665 test loss: 1.49501\n",
      "7220: accuracy:0.96 loss: 1.50143 (lr:0.3678132839768692)\n",
      "7240: accuracy:0.98 loss: 1.48095 (lr:0.36514849727711796)\n",
      "7260: accuracy:0.98 loss: 1.48224 (lr:0.36251022564805235)\n",
      "7280: accuracy:1.0 loss: 1.46137 (lr:0.3598982052603108)\n",
      "7300: accuracy:0.97 loss: 1.49149 (lr:0.3573121749096779)\n",
      "7300: ********* epoch 13 ********* test accuracy:0.9663 test loss: 1.49477\n",
      "7320: accuracy:0.98 loss: 1.48116 (lr:0.35475187599096347)\n",
      "7340: accuracy:0.97 loss: 1.48853 (lr:0.3522170524721421)\n",
      "7360: accuracy:0.97 loss: 1.49365 (lr:0.34970745086874944)\n",
      "7380: accuracy:0.96 loss: 1.50639 (lr:0.34722282021853396)\n",
      "7400: accuracy:0.97 loss: 1.49218 (lr:0.34476291205635995)\n",
      "7400: ********* epoch 13 ********* test accuracy:0.9672 test loss: 1.49456\n",
      "7420: accuracy:1.0 loss: 1.46188 (lr:0.34232748038936145)\n",
      "7440: accuracy:0.98 loss: 1.48744 (lr:0.339916281672342)\n",
      "7460: accuracy:0.96 loss: 1.5013 (lr:0.33752907478342087)\n",
      "7480: accuracy:1.0 loss: 1.46125 (lr:0.3351656209999195)\n",
      "7500: accuracy:0.97 loss: 1.49103 (lr:0.3328256839744902)\n",
      "7500: ********* epoch 13 ********* test accuracy:0.9671 test loss: 1.49468\n",
      "7520: accuracy:0.99 loss: 1.47125 (lr:0.33050902971148044)\n",
      "7540: accuracy:1.0 loss: 1.46128 (lr:0.3282154265435332)\n",
      "7560: accuracy:0.96 loss: 1.50119 (lr:0.3259446451084205)\n",
      "7580: accuracy:0.98 loss: 1.48113 (lr:0.3236964583261065)\n",
      "7600: accuracy:0.97 loss: 1.49245 (lr:0.3214706413760395)\n",
      "7600: ********* epoch 13 ********* test accuracy:0.9668 test loss: 1.49464\n",
      "7620: accuracy:0.96 loss: 1.5008 (lr:0.31926697167466944)\n",
      "7640: accuracy:0.99 loss: 1.47156 (lr:0.31708522885319)\n",
      "7660: accuracy:0.96 loss: 1.50237 (lr:0.31492519473550085)\n",
      "7680: accuracy:0.97 loss: 1.4911 (lr:0.3127866533163902)\n",
      "7700: accuracy:0.99 loss: 1.47161 (lr:0.310669390739934)\n",
      "7700: ********* epoch 13 ********* test accuracy:0.9672 test loss: 1.49432\n",
      "7720: accuracy:0.99 loss: 1.4712 (lr:0.3085731952781102)\n",
      "7740: accuracy:1.0 loss: 1.46143 (lr:0.3064978573096257)\n",
      "7760: accuracy:0.94 loss: 1.52172 (lr:0.30444316929895443)\n",
      "7780: accuracy:1.0 loss: 1.46124 (lr:0.30240892577558276)\n",
      "7800: accuracy:0.96 loss: 1.50183 (lr:0.3003949233134635)\n",
      "7800: ********* epoch 14 ********* test accuracy:0.9665 test loss: 1.49444\n",
      "7820: accuracy:0.96 loss: 1.50128 (lr:0.2984009605106718)\n",
      "7840: accuracy:0.96 loss: 1.50153 (lr:0.29642683796926583)\n",
      "7860: accuracy:0.97 loss: 1.49302 (lr:0.2944723582753464)\n",
      "7880: accuracy:0.98 loss: 1.4824 (lr:0.29253732597931537)\n",
      "7900: accuracy:0.99 loss: 1.47254 (lr:0.2906215475763305)\n",
      "7900: ********* epoch 14 ********* test accuracy:0.9667 test loss: 1.49455\n",
      "7920: accuracy:0.96 loss: 1.50206 (lr:0.2887248314869552)\n",
      "7940: accuracy:0.96 loss: 1.5012 (lr:0.2868469880379997)\n",
      "7960: accuracy:0.97 loss: 1.49078 (lr:0.2849878294435545)\n",
      "7980: accuracy:0.98 loss: 1.48208 (lr:0.2831471697862105)\n",
      "8000: accuracy:1.0 loss: 1.46335 (lr:0.2813248249984684)\n",
      "8000: ********* epoch 14 ********* test accuracy:0.9666 test loss: 1.49454\n",
      "8020: accuracy:0.98 loss: 1.4809 (lr:0.27952061284433066)\n",
      "8040: accuracy:0.98 loss: 1.48368 (lr:0.2777343529010784)\n",
      "8060: accuracy:0.98 loss: 1.48098 (lr:0.27596586654122846)\n",
      "8080: accuracy:0.99 loss: 1.47239 (lr:0.27421497691467156)\n",
      "8100: accuracy:0.96 loss: 1.50076 (lr:0.27248150893098577)\n",
      "8100: ********* epoch 14 ********* test accuracy:0.967 test loss: 1.49443\n",
      "8120: accuracy:0.98 loss: 1.48419 (lr:0.27076528924192816)\n",
      "8140: accuracy:0.98 loss: 1.48313 (lr:0.26906614622409947)\n",
      "8160: accuracy:1.0 loss: 1.46318 (lr:0.2673839099617823)\n",
      "8180: accuracy:0.98 loss: 1.48114 (lr:0.2657184122299483)\n",
      "8200: accuracy:1.0 loss: 1.46192 (lr:0.26406948647743644)\n",
      "8200: ********* epoch 14 ********* test accuracy:0.9665 test loss: 1.4945\n",
      "8220: accuracy:0.98 loss: 1.48156 (lr:0.2624369678102972)\n",
      "8240: accuracy:0.97 loss: 1.49229 (lr:0.26082069297530375)\n",
      "8260: accuracy:0.97 loss: 1.49241 (lr:0.25922050034362554)\n",
      "8280: accuracy:0.95 loss: 1.51102 (lr:0.25763622989466584)\n",
      "8300: accuracy:0.96 loss: 1.50078 (lr:0.25606772320005944)\n",
      "8300: ********* epoch 14 ********* test accuracy:0.9669 test loss: 1.49437\n",
      "8320: accuracy:0.96 loss: 1.50153 (lr:0.25451482340783005)\n",
      "8340: accuracy:0.95 loss: 1.51406 (lr:0.2529773752267042)\n",
      "8360: accuracy:0.99 loss: 1.47115 (lr:0.25145522491058264)\n",
      "8380: accuracy:0.99 loss: 1.4663 (lr:0.2499482202431651)\n",
      "8400: accuracy:0.99 loss: 1.47122 (lr:0.24845621052272926)\n",
      "8400: ********* epoch 15 ********* test accuracy:0.9667 test loss: 1.49448\n",
      "8420: accuracy:0.95 loss: 1.51121 (lr:0.24697904654705966)\n",
      "8440: accuracy:0.98 loss: 1.48095 (lr:0.24551658059852768)\n",
      "8460: accuracy:0.99 loss: 1.47373 (lr:0.24406866642931963)\n",
      "8480: accuracy:0.96 loss: 1.50268 (lr:0.24263515924681228)\n",
      "8500: accuracy:0.99 loss: 1.47196 (lr:0.24121591569909265)\n",
      "8500: ********* epoch 15 ********* test accuracy:0.967 test loss: 1.4944\n",
      "8520: accuracy:0.96 loss: 1.50817 (lr:0.23981079386062323)\n",
      "8540: accuracy:0.97 loss: 1.49083 (lr:0.23841965321804925)\n",
      "8560: accuracy:0.97 loss: 1.49136 (lr:0.23704235465614706)\n",
      "8580: accuracy:0.99 loss: 1.47127 (lr:0.23567876044391298)\n",
      "8600: accuracy:0.96 loss: 1.50109 (lr:0.23432873422078926)\n",
      "8600: ********* epoch 15 ********* test accuracy:0.967 test loss: 1.49443\n",
      "8620: accuracy:0.99 loss: 1.47306 (lr:0.23299214098302862)\n",
      "8640: accuracy:0.97 loss: 1.49119 (lr:0.2316688470701933)\n",
      "8660: accuracy:0.97 loss: 1.49134 (lr:0.23035872015178954)\n",
      "8680: accuracy:0.98 loss: 1.48079 (lr:0.22906162921403359)\n",
      "8700: accuracy:0.97 loss: 1.49199 (lr:0.22777744454675075)\n",
      "8700: ********* epoch 15 ********* test accuracy:0.967 test loss: 1.4944\n",
      "8720: accuracy:0.97 loss: 1.49 (lr:0.22650603773040404)\n",
      "8740: accuracy:0.96 loss: 1.50109 (lr:0.22524728162325253)\n",
      "8760: accuracy:0.99 loss: 1.47129 (lr:0.22400105034863643)\n",
      "8780: accuracy:0.97 loss: 1.49053 (lr:0.22276721928238974)\n",
      "8800: accuracy:0.99 loss: 1.47115 (lr:0.22154566504037754)\n",
      "8800: ********* epoch 15 ********* test accuracy:0.9672 test loss: 1.49435\n",
      "8820: accuracy:0.98 loss: 1.48156 (lr:0.22033626546615787)\n",
      "8840: accuracy:0.96 loss: 1.50109 (lr:0.21913889961876537)\n",
      "8860: accuracy:1.0 loss: 1.46187 (lr:0.21795344776061754)\n",
      "8880: accuracy:0.97 loss: 1.49142 (lr:0.21677979134554048)\n",
      "8900: accuracy:1.0 loss: 1.46199 (lr:0.2156178130069149)\n",
      "8900: ********* epoch 15 ********* test accuracy:0.967 test loss: 1.49438\n",
      "8920: accuracy:0.97 loss: 1.49183 (lr:0.21446739654593852)\n",
      "8940: accuracy:0.99 loss: 1.47233 (lr:0.21332842692000653)\n",
      "8960: accuracy:0.95 loss: 1.51287 (lr:0.21220079023120714)\n",
      "8980: accuracy:0.98 loss: 1.4833 (lr:0.21108437371493194)\n",
      "9000: accuracy:0.96 loss: 1.50213 (lr:0.20997906572859884)\n",
      "9000: ********* epoch 16 ********* test accuracy:0.9669 test loss: 1.49442\n",
      "9020: accuracy:0.99 loss: 1.47118 (lr:0.20888475574048815)\n",
      "9040: accuracy:1.0 loss: 1.46125 (lr:0.20780133431868908)\n",
      "9060: accuracy:0.98 loss: 1.47752 (lr:0.20672869312015654)\n",
      "9080: accuracy:0.98 loss: 1.48149 (lr:0.20566672487987697)\n",
      "9100: accuracy:0.99 loss: 1.47117 (lr:0.2046153234001413)\n",
      "9100: ********* epoch 16 ********* test accuracy:0.967 test loss: 1.49443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9120: accuracy:0.98 loss: 1.48136 (lr:0.20357438353992535)\n",
      "9140: accuracy:0.98 loss: 1.48087 (lr:0.2025438012043756)\n",
      "9160: accuracy:0.98 loss: 1.4813 (lr:0.20152347333439985)\n",
      "9180: accuracy:0.97 loss: 1.49147 (lr:0.20051329789636066)\n",
      "9200: accuracy:0.98 loss: 1.48159 (lr:0.1995131738718725)\n",
      "9200: ********* epoch 16 ********* test accuracy:0.9675 test loss: 1.4943\n",
      "9220: accuracy:0.97 loss: 1.49148 (lr:0.19852300124769937)\n",
      "9240: accuracy:0.96 loss: 1.50148 (lr:0.19754268100575384)\n",
      "9260: accuracy:0.99 loss: 1.47117 (lr:0.19657211511319475)\n",
      "9280: accuracy:0.99 loss: 1.47213 (lr:0.19561120651262398)\n",
      "9300: accuracy:0.99 loss: 1.4705 (lr:0.1946598591123807)\n",
      "9300: ********* epoch 16 ********* test accuracy:0.9672 test loss: 1.49428\n",
      "9320: accuracy:0.99 loss: 1.47115 (lr:0.19371797777693223)\n",
      "9340: accuracy:0.95 loss: 1.506 (lr:0.19278546831736)\n",
      "9360: accuracy:0.95 loss: 1.51105 (lr:0.19186223748194098)\n",
      "9380: accuracy:0.94 loss: 1.521 (lr:0.1909481929468222)\n",
      "9400: accuracy:0.97 loss: 1.49402 (lr:0.19004324330678857)\n",
      "9400: ********* epoch 16 ********* test accuracy:0.9671 test loss: 1.49434\n",
      "9420: accuracy:0.96 loss: 1.50093 (lr:0.18914729806612196)\n",
      "9440: accuracy:0.98 loss: 1.48107 (lr:0.1882602676295516)\n",
      "9460: accuracy:0.97 loss: 1.49256 (lr:0.18738206329329454)\n",
      "9480: accuracy:0.99 loss: 1.47184 (lr:0.18651259723618557)\n",
      "9500: accuracy:0.97 loss: 1.4922 (lr:0.1856517825108943)\n",
      "9500: ********* epoch 16 ********* test accuracy:0.9674 test loss: 1.49425\n",
      "9520: accuracy:0.97 loss: 1.49076 (lr:0.1847995330352308)\n",
      "9540: accuracy:0.95 loss: 1.51125 (lr:0.18395576358353738)\n",
      "9560: accuracy:0.99 loss: 1.47236 (lr:0.18312038977816558)\n",
      "9580: accuracy:0.98 loss: 1.4817 (lr:0.18229332808103887)\n",
      "9600: accuracy:0.99 loss: 1.47116 (lr:0.1814744957852983)\n",
      "9600: ********* epoch 17 ********* test accuracy:0.9672 test loss: 1.49415\n",
      "9620: accuracy:0.99 loss: 1.47153 (lr:0.18066381100703188)\n",
      "9640: accuracy:0.98 loss: 1.47975 (lr:0.1798611926770862)\n",
      "9660: accuracy:1.0 loss: 1.4618 (lr:0.1790665605329595)\n",
      "9680: accuracy:0.95 loss: 1.51385 (lr:0.17827983511077508)\n",
      "9700: accuracy:0.98 loss: 1.48124 (lr:0.17750093773733516)\n",
      "9700: ********* epoch 17 ********* test accuracy:0.9674 test loss: 1.49419\n",
      "9720: accuracy:0.98 loss: 1.48519 (lr:0.17672979052225327)\n",
      "9740: accuracy:0.96 loss: 1.50157 (lr:0.17596631635016535)\n",
      "9760: accuracy:0.99 loss: 1.47157 (lr:0.1752104388730179)\n",
      "9780: accuracy:0.93 loss: 1.53168 (lr:0.1744620825024334)\n",
      "9800: accuracy:0.94 loss: 1.52473 (lr:0.17372117240215096)\n",
      "9800: ********* epoch 17 ********* test accuracy:0.9673 test loss: 1.49421\n",
      "9820: accuracy:0.99 loss: 1.4714 (lr:0.17298763448054333)\n",
      "9840: accuracy:0.98 loss: 1.48116 (lr:0.17226139538320698)\n",
      "9860: accuracy:0.97 loss: 1.49185 (lr:0.17154238248562698)\n",
      "9880: accuracy:0.98 loss: 1.48087 (lr:0.17083052388591435)\n",
      "9900: accuracy:0.97 loss: 1.49173 (lr:0.17012574839761596)\n",
      "9900: ********* epoch 17 ********* test accuracy:0.9671 test loss: 1.49441\n",
      "9920: accuracy:0.98 loss: 1.48595 (lr:0.1694279855425957)\n",
      "9940: accuracy:0.96 loss: 1.50124 (lr:0.16873716554398654)\n",
      "9960: accuracy:0.95 loss: 1.51148 (lr:0.16805321931921285)\n",
      "9980: accuracy:0.98 loss: 1.48432 (lr:0.1673760784730824)\n",
      "10000: accuracy:0.99 loss: 1.47131 (lr:0.16670567529094613)\n",
      "10000: ********* epoch 17 ********* test accuracy:0.9671 test loss: 1.49441\n",
      "CPU times: user 1min 12s, sys: 2.11 s, total: 1min 14s\n",
      "Wall time: 35.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96710002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "%time nn_train_var(min_lr=0.1, max_lr=10, h_dim=100, minibatch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
